{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646d10c6",
   "metadata": {},
   "source": [
    "# SPEECH-TO-TEXT WITH VOSK\n",
    "\n",
    "## GLOSSARY\n",
    "\n",
    "- **Speech-to-Text (STT)**: The process of converting spoken language into written text\n",
    "- **Real-time Recognition**: Processing speech as it's being spoken, rather than after a recording is complete\n",
    "- **Partial Results**: Preliminary recognition results that may change as more audio is processed\n",
    "- **Final Result**: The complete recognition result after all audio has been processed\n",
    "- **Grammar**: A set of rules that constrains the words the recognizer will consider\n",
    "- **Word List**: A limited set of words the recognizer should recognize\n",
    "- **Confidence Score**: A measure of how confident the system is in its recognition result\n",
    "- **Alternative Results**: Alternative interpretations of the same speech input\n",
    "- **Speaker Identification**: The process of determining who is speaking in an audio sample\n",
    "- **Word Timestamps**: Time markers indicating when each word was spoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f152c9",
   "metadata": {},
   "source": [
    "## CONCEPT INTERACTIONS\n",
    "\n",
    "- **Building on Vosk Basics**: We'll expand on the Vosk concepts from Module 1, applying them to real-time scenarios\n",
    "- **Building on Audio Preprocessing**: We'll integrate the preprocessing techniques from Module 2 for better recognition\n",
    "- **Looking Forward**: The speech-to-text capabilities we develop here will form the foundation for understanding user intent in the next module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e81aba",
   "metadata": {},
   "source": [
    "## MAIN CONTENT\n",
    "\n",
    "### From Batch to Real-Time Recognition\n",
    "\n",
    "In the first module, we learned how to use Vosk to recognize speech from a pre-recorded audio file. While useful, voice assistants need to process speech in real-time as the user is speaking. In this module, we'll explore how to:\n",
    "\n",
    "1. Capture live audio using PyAudio\n",
    "2. Process this audio in chunks with Vosk\n",
    "3. Handle both partial and final results\n",
    "4. Improve accuracy with preprocessing\n",
    "5. Build a complete real-time speech recognition system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5929da",
   "metadata": {},
   "source": [
    "### Setting Up Real-Time Recognition\n",
    "\n",
    "Real-time recognition follows a different pattern than batch processing:\n",
    "\n",
    "1. Initialize audio capture stream\n",
    "2. Initialize Vosk recognizer\n",
    "3. Continuously:\n",
    "   - Capture audio chunks\n",
    "   - Preprocess the audio (optional)\n",
    "   - Feed chunks to the recognizer\n",
    "   - Handle partial results\n",
    "   - Check for silence/end of speech\n",
    "4. Get final result\n",
    "5. Take action based on the recognized text\n",
    "\n",
    "Let's implement each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a962b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "# For preprocessing (optional)\n",
    "import librosa\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Initialize PyAudio\n",
    "def init_audio(sample_rate=16000, chunk_size=1024):\n",
    "    \"\"\"\n",
    "    Initialize PyAudio for real-time audio capture\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_rate: Audio sample rate (must match model requirements)\n",
    "    - chunk_size: Size of audio chunks to process\n",
    "    \n",
    "    Returns:\n",
    "    - p: PyAudio object\n",
    "    - stream: PyAudio stream\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=sample_rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk_size)\n",
    "    return p, stream\n",
    "\n",
    "# Initialize Vosk recognizer\n",
    "def init_recognizer(model_path, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Initialize Vosk recognizer\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: Path to Vosk model\n",
    "    - sample_rate: Audio sample rate\n",
    "    \n",
    "    Returns:\n",
    "    - recognizer: KaldiRecognizer object\n",
    "    \"\"\"\n",
    "    model = Model(model_path)\n",
    "    recognizer = KaldiRecognizer(model, sample_rate)\n",
    "    return recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610412a6",
   "metadata": {},
   "source": [
    "### Real-Time Audio Processing\n",
    "\n",
    "For real-time recognition, we need to efficiently process audio chunks as they come in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92845a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_chunk(recognizer, audio_data, preprocess=False):\n",
    "    \"\"\"\n",
    "    Process a chunk of audio data\n",
    "    \n",
    "    Parameters:\n",
    "    - recognizer: Vosk recognizer object\n",
    "    - audio_data: Raw audio data (bytes)\n",
    "    - preprocess: Whether to apply preprocessing\n",
    "    \n",
    "    Returns:\n",
    "    - is_final: Whether this is a final result\n",
    "    - text: Recognized text\n",
    "    \"\"\"\n",
    "    if preprocess:\n",
    "        # Convert to numpy array for preprocessing\n",
    "        audio = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32767.0\n",
    "        \n",
    "        # Simple preprocessing (bandpass filter for speech frequencies)\n",
    "        audio = bandpass_filter(audio, 16000, 300, 3000)\n",
    "        \n",
    "        # Convert back to bytes\n",
    "        audio_data = (audio * 32767).astype(np.int16).tobytes()\n",
    "    \n",
    "    # Process the audio chunk\n",
    "    if recognizer.AcceptWaveform(audio_data):\n",
    "        # We have a final result\n",
    "        result = json.loads(recognizer.Result()).get(\"text\", \"\")\n",
    "        return True, result\n",
    "    else:\n",
    "        # We have a partial result\n",
    "        partial = json.loads(recognizer.PartialResult()).get(\"partial\", \"\")\n",
    "        return False, partial\n",
    "\n",
    "\n",
    "def bandpass_filter(audio, sr, lowcut=300, highcut=3000, order=5):\n",
    "    \"\"\"Simple bandpass filter for speech frequencies\"\"\"\n",
    "    nyquist = 0.5 * sr\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9399986",
   "metadata": {},
   "source": [
    "### Continuous Recognition Loop\n",
    "\n",
    "Now, let's create a continuous recognition loop that captures and processes audio in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_recognition(model_path, timeout=5, preprocess=False):\n",
    "    \"\"\"\n",
    "    Perform continuous speech recognition\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: Path to Vosk model\n",
    "    - timeout: Seconds of silence before finalizing\n",
    "    - preprocess: Whether to apply preprocessing\n",
    "    \n",
    "    Returns:\n",
    "    - recognized_text: Final recognized text\n",
    "    \"\"\"\n",
    "    p, stream = init_audio()\n",
    "    recognizer = init_recognizer(model_path)\n",
    "    \n",
    "    print(\"Listening... (Speak now)\")\n",
    "    \n",
    "    # Variables to track silence\n",
    "    last_speech_time = time.time()\n",
    "    speech_detected = False\n",
    "    silence_threshold = 1000  # Adjust based on your microphone\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Read audio chunk\n",
    "            audio_chunk = stream.read(1024, exception_on_overflow=False)\n",
    "            \n",
    "            # Check audio level (simple silence detection)\n",
    "            audio_level = np.max(np.abs(np.frombuffer(audio_chunk, dtype=np.int16)))\n",
    "            \n",
    "            # Update speech detection state\n",
    "            if audio_level > silence_threshold:\n",
    "                if not speech_detected:\n",
    "                    print(\"Speech detected...\")\n",
    "                speech_detected = True\n",
    "                last_speech_time = time.time()\n",
    "            \n",
    "            # Process audio with recognizer\n",
    "            is_final, text = process_audio_chunk(recognizer, audio_chunk, preprocess)\n",
    "            \n",
    "            # Display partial results\n",
    "            if text:\n",
    "                print(f\"\\rPartial: {text}\", end=\"\", flush=True)\n",
    "            \n",
    "            # Check for timeout (silence)\n",
    "            if speech_detected and time.time() - last_speech_time > timeout:\n",
    "                print(\"\\nSilence detected, finalizing...\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopped by user\")\n",
    "    finally:\n",
    "        # Clean up\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "    \n",
    "    # Get final result\n",
    "    final_result = json.loads(recognizer.FinalResult())\n",
    "    final_text = final_result.get(\"text\", \"\")\n",
    "    print(f\"\\nFinal result: \\\"{final_text}\\\"\")\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dcdb4",
   "metadata": {},
   "source": [
    "### Non-Blocking Recognition\n",
    "\n",
    "The above approach blocks the main thread. For a voice assistant, we often want recognition to happen in the background while other operations continue. Here's a non-blocking approach using threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceRecognizer:\n",
    "    \"\"\"Class for non-blocking voice recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, sample_rate=16000, chunk_size=1024, preprocess=False):\n",
    "        \"\"\"Initialize the recognizer\"\"\"\n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_size = chunk_size\n",
    "        self.preprocess = preprocess\n",
    "        self.p = None\n",
    "        self.stream = None\n",
    "        self.recognizer = None\n",
    "        self.model_path = model_path\n",
    "        self.running = False\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.text_queue = queue.Queue()\n",
    "        self.last_partial = \"\"\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start recognition in background thread\"\"\"\n",
    "        if self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk_size,\n",
    "            stream_callback=self._audio_callback\n",
    "        )\n",
    "        \n",
    "        # Initialize Vosk\n",
    "        self.recognizer = init_recognizer(self.model_path, self.sample_rate)\n",
    "        \n",
    "        # Start processing thread\n",
    "        self.process_thread = threading.Thread(target=self._process_audio)\n",
    "        self.process_thread.daemon = True\n",
    "        self.process_thread.start()\n",
    "        \n",
    "        print(\"Voice recognizer started\")\n",
    "    \n",
    "    def _audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"Callback for audio stream\"\"\"\n",
    "        self.audio_queue.put(in_data)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "    \n",
    "    def _process_audio(self):\n",
    "        \"\"\"Process audio chunks from queue\"\"\"\n",
    "        while self.running:\n",
    "            # Get audio chunk from queue\n",
    "            if not self.audio_queue.empty():\n",
    "                audio_data = self.audio_queue.get()\n",
    "                \n",
    "                # Process with recognizer\n",
    "                is_final, text = process_audio_chunk(\n",
    "                    self.recognizer, audio_data, self.preprocess\n",
    "                )\n",
    "                \n",
    "                if is_final and text:\n",
    "                    # Put final result in queue\n",
    "                    self.text_queue.put((\"final\", text))\n",
    "                    self.last_partial = \"\"\n",
    "                elif text and text != self.last_partial:\n",
    "                    # Only queue new partials\n",
    "                    self.text_queue.put((\"partial\", text))\n",
    "                    self.last_partial = text\n",
    "            \n",
    "            # Short sleep to prevent CPU hogging\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    def get_result(self, block=False, timeout=None):\n",
    "        \"\"\"\n",
    "        Get recognition result if available\n",
    "        \n",
    "        Returns tuple of (type, text) where type is 'final' or 'partial'\n",
    "        Returns None if no result available\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.text_queue.get(block=block, timeout=timeout)\n",
    "        except queue.Empty:\n",
    "            return None\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop recognition\"\"\"\n",
    "        if not self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = False\n",
    "        \n",
    "        # Stop and close stream\n",
    "        if self.stream:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "        \n",
    "        # Terminate PyAudio\n",
    "        if self.p:\n",
    "            self.p.terminate()\n",
    "        \n",
    "        # Get final result\n",
    "        final_result = json.loads(self.recognizer.FinalResult())\n",
    "        final_text = final_result.get(\"text\", \"\")\n",
    "        \n",
    "        if final_text:\n",
    "            self.text_queue.put((\"final\", final_text))\n",
    "        \n",
    "        print(\"Voice recognizer stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f339c79",
   "metadata": {},
   "source": [
    "### Example Usage of Non-Blocking Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_non_blocking_recognition():\n",
    "    \"\"\"Demo of non-blocking recognition\"\"\"\n",
    "    recognizer = VoiceRecognizer(\n",
    "        model_path=\"models/vosk-model-small-en-us-0.15\",\n",
    "        preprocess=True\n",
    "    )\n",
    "    \n",
    "    print(\"Starting recognizer...\")\n",
    "    recognizer.start()\n",
    "    \n",
    "    print(\"Speak now! (Press Ctrl+C to stop)\")\n",
    "    \n",
    "    try:\n",
    "        # Main loop - could do other things here\n",
    "        while True:\n",
    "            # Check for results\n",
    "            result = recognizer.get_result(block=False)\n",
    "            if result:\n",
    "                result_type, text = result\n",
    "                if result_type == \"partial\":\n",
    "                    print(f\"\\rPartial: {text}\", end=\"\", flush=True)\n",
    "                else:\n",
    "                    print(f\"\\nFinal: {text}\")\n",
    "            \n",
    "            # Do other things...\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping...\")\n",
    "    finally:\n",
    "        recognizer.stop()\n",
    "\n",
    "# Uncomment to run the demo\n",
    "# demo_non_blocking_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccdd65",
   "metadata": {},
   "source": [
    "### Advanced Features\n",
    "\n",
    "Vosk provides several advanced features that can enhance your speech recognition capabilities:\n",
    "\n",
    "#### 1. Grammars and Word Lists\n",
    "\n",
    "You can constrain recognition to a specific set of words or phrases, which improves accuracy for specific domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc47238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar_recognizer(model_path, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Create a recognizer with a grammar constraint\n",
    "    \"\"\"\n",
    "    model = Model(model_path)\n",
    "    \n",
    "    # Define a simple grammar as JSON\n",
    "    # This example recognizes commands for a smart home\n",
    "    grammar = {\n",
    "        \"grammar\": [\n",
    "            \"turn on the lights\",\n",
    "            \"turn off the lights\",\n",
    "            \"turn on the tv\",\n",
    "            \"turn off the tv\",\n",
    "            \"play music\",\n",
    "            \"stop music\",\n",
    "            \"what time is it\",\n",
    "            \"what's the weather\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create recognizer with grammar\n",
    "    rec = KaldiRecognizer(model, sample_rate, json.dumps(grammar))\n",
    "    return rec\n",
    "\n",
    "# Example usage:\n",
    "# grammar_recognizer = create_grammar_recognizer(\"models/vosk-model-small-en-us-0.15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775dcf12",
   "metadata": {},
   "source": [
    "#### 2. Getting Word Timestamps\n",
    "\n",
    "Vosk can provide timestamps for each recognized word, which is useful for synchronizing with other events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c201a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_timestamps(recognizer, audio_data):\n",
    "    \"\"\"\n",
    "    Process audio and extract word timestamps\n",
    "    \"\"\"\n",
    "    recognizer.AcceptWaveform(audio_data)\n",
    "    result = json.loads(recognizer.Result())\n",
    "    \n",
    "    # Check if we have results with timestamps\n",
    "    if \"result\" in result:\n",
    "        words_with_times = []\n",
    "        for word_data in result[\"result\"]:\n",
    "            words_with_times.append({\n",
    "                \"word\": word_data[\"word\"],\n",
    "                \"start\": word_data[\"start\"],\n",
    "                \"end\": word_data[\"end\"],\n",
    "                \"conf\": word_data.get(\"conf\", 1.0)\n",
    "            })\n",
    "        return words_with_times\n",
    "    \n",
    "    return []\n",
    "\n",
    "# To enable timestamps, set a config parameter\n",
    "# rec = KaldiRecognizer(model, sample_rate, '{\"words\": true}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452f5cf",
   "metadata": {},
   "source": [
    "#### 3. Getting Alternative Results\n",
    "\n",
    "For ambiguous speech, you can get alternative interpretations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1116d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alternatives(recognizer, audio_data, max_alternatives=5):\n",
    "    \"\"\"\n",
    "    Get alternative recognition results\n",
    "    \"\"\"\n",
    "    # Create recognizer with alternatives option\n",
    "    model = Model(\"models/vosk-model-small-en-us-0.15\")\n",
    "    config = json.dumps({\"max_alternatives\": max_alternatives})\n",
    "    alt_recognizer = KaldiRecognizer(model, 16000, config)\n",
    "    \n",
    "    # Process audio\n",
    "    alt_recognizer.AcceptWaveform(audio_data)\n",
    "    result = json.loads(alt_recognizer.Result())\n",
    "    \n",
    "    # Extract alternatives\n",
    "    if \"alternatives\" in result:\n",
    "        return result[\"alternatives\"]\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6bde0",
   "metadata": {},
   "source": [
    "### Integration with Preprocessing\n",
    "\n",
    "For optimal results, we should integrate the audio preprocessing techniques from Module 2 into our real-time recognition system. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c912eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(audio, sr, lowcut=300, highcut=3000, order=5):\n",
    "    \"\"\"\n",
    "    Apply bandpass filter for speech frequencies\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * sr\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, audio)\n",
    "\n",
    "def normalize_audio(audio, target_dB=-3):\n",
    "    \"\"\"\n",
    "    Normalize audio level\n",
    "    \"\"\"\n",
    "    # Find the maximum absolute amplitude\n",
    "    max_amplitude = np.max(np.abs(audio))\n",
    "    \n",
    "    # Calculate current peak in dB\n",
    "    current_dB = 20 * np.log10(max_amplitude) if max_amplitude > 0 else -80\n",
    "    \n",
    "    # Calculate the gain needed to reach target\n",
    "    gain_dB = target_dB - current_dB\n",
    "    gain_linear = 10 ** (gain_dB / 20)\n",
    "    \n",
    "    # Apply gain to normalize\n",
    "    normalized_audio = audio * gain_linear\n",
    "    \n",
    "    # Ensure we don't exceed [-1, 1]\n",
    "    if np.max(np.abs(normalized_audio)) > 1.0:\n",
    "        normalized_audio = normalized_audio / np.max(np.abs(normalized_audio))\n",
    "    \n",
    "    return normalized_audio\n",
    "\n",
    "class EnhancedVoiceRecognizer(VoiceRecognizer):\n",
    "    \"\"\"Voice recognizer with enhanced preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, sample_rate=16000, chunk_size=1024):\n",
    "        super().__init__(model_path, sample_rate, chunk_size, preprocess=False)\n",
    "        self.enable_bandpass = True\n",
    "        self.enable_normalization = True\n",
    "    \n",
    "    def _process_audio(self):\n",
    "        \"\"\"Process audio with enhanced preprocessing\"\"\"\n",
    "        while self.running:\n",
    "            if not self.audio_queue.empty():\n",
    "                audio_data = self.audio_queue.get()\n",
    "                \n",
    "                # Convert to numpy for preprocessing\n",
    "                audio = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32767.0\n",
    "                \n",
    "                # Apply preprocessing\n",
    "                if self.enable_normalization:\n",
    "                    audio = normalize_audio(audio)\n",
    "                \n",
    "                if self.enable_bandpass:\n",
    "                    audio = bandpass_filter(audio, self.sample_rate)\n",
    "                \n",
    "                # Convert back to bytes\n",
    "                processed_audio = (audio * 32767).astype(np.int16).tobytes()\n",
    "                \n",
    "                # Process with recognizer\n",
    "                if self.recognizer.AcceptWaveform(processed_audio):\n",
    "                    result = json.loads(self.recognizer.Result())\n",
    "                    text = result.get(\"text\", \"\")\n",
    "                    if text:\n",
    "                        self.text_queue.put((\"final\", text))\n",
    "                        self.last_partial = \"\"\n",
    "                else:\n",
    "                    partial = json.loads(self.recognizer.PartialResult())\n",
    "                    text = partial.get(\"partial\", \"\")\n",
    "                    if text and text != self.last_partial:\n",
    "                        self.text_queue.put((\"partial\", text))\n",
    "                        self.last_partial = text\n",
    "            \n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc92aa9",
   "metadata": {},
   "source": [
    "### Building a Complete Voice Recognition System\n",
    "\n",
    "Now, let's build a complete system that brings everything together:\n",
    "\n",
    "1. Real-time audio capture\n",
    "2. Advanced preprocessing\n",
    "3. Vosk recognition with alternatives\n",
    "4. Result handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteVoiceRecognizer:\n",
    "    \"\"\"Complete voice recognition system\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, use_grammar=False, grammar_list=None):\n",
    "        self.model_path = model_path\n",
    "        self.sample_rate = 16000\n",
    "        self.chunk_size = 1024\n",
    "        self.use_grammar = use_grammar\n",
    "        self.grammar_list = grammar_list or []\n",
    "        self.p = None\n",
    "        self.stream = None\n",
    "        self.recognizer = None\n",
    "        self.running = False\n",
    "        self.speech_detected = False\n",
    "        self.last_speech_time = 0\n",
    "        self.silence_threshold = 700  # Adjust based on your environment\n",
    "        self.silence_timeout = 2.0  # Seconds of silence to consider speech ended\n",
    "        \n",
    "        # For audio processing\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.result_queue = queue.Queue()\n",
    "        \n",
    "        # Preprocessing settings\n",
    "        self.enable_normalization = True\n",
    "        self.enable_bandpass = True\n",
    "        self.lowcut = 300\n",
    "        self.highcut = 3000\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the recognizer and audio stream\"\"\"\n",
    "        # Load the model\n",
    "        model = Model(self.model_path)\n",
    "        \n",
    "        # Create recognizer with appropriate config\n",
    "        if self.use_grammar and self.grammar_list:\n",
    "            grammar = {\"grammar\": self.grammar_list}\n",
    "            config = json.dumps({\"words\": True, \"grammar\": self.grammar_list})\n",
    "        else:\n",
    "            config = json.dumps({\"words\": True, \"max_alternatives\": 3})\n",
    "        \n",
    "        self.recognizer = KaldiRecognizer(model, self.sample_rate, config)\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            channels=1,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk_size,\n",
    "            stream_callback=self._audio_callback\n",
    "        )\n",
    "    \n",
    "    def _audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"Callback for audio stream\"\"\"\n",
    "        self.audio_queue.put(in_data)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "    \n",
    "    def _preprocess_audio(self, audio_data):\n",
    "        \"\"\"Preprocess audio data\"\"\"\n",
    "        # Convert to numpy array\n",
    "        audio = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32767.0\n",
    "        \n",
    "        # Check audio level for speech detection\n",
    "        audio_level = np.max(np.abs(audio)) * 32767\n",
    "        if audio_level > self.silence_threshold:\n",
    "            self.speech_detected = True\n",
    "            self.last_speech_time = time.time()\n",
    "        elif self.speech_detected and time.time() - self.last_speech_time > self.silence_timeout:\n",
    "            self.speech_detected = False\n",
    "            self.result_queue.put((\"speech_end\", None))\n",
    "        \n",
    "        # Apply preprocessing if enabled\n",
    "        if self.enable_normalization:\n",
    "            audio = normalize_audio(audio)\n",
    "        \n",
    "        if self.enable_bandpass:\n",
    "            audio = bandpass_filter(audio, self.sample_rate, self.lowcut, self.highcut)\n",
    "        \n",
    "        # Convert back to bytes\n",
    "        return (audio * 32767).astype(np.int16).tobytes()\n",
    "    \n",
    "    def _process_thread(self):\n",
    "        \"\"\"Audio processing thread\"\"\"\n",
    "        while self.running:\n",
    "            if not self.audio_queue.empty():\n",
    "                # Get audio data\n",
    "                audio_data = self.audio_queue.get()\n",
    "                \n",
    "                # Preprocess\n",
    "                processed_audio = self._preprocess_audio(audio_data)\n",
    "                \n",
    "                # Process with recognizer\n",
    "                if self.recognizer.AcceptWaveform(processed_audio):\n",
    "                    # We have a final result\n",
    "                    result = json.loads(self.recognizer.Result())\n",
    "                    \n",
    "                    # Extract text and word timestamps\n",
    "                    text = result.get(\"text\", \"\")\n",
    "                    words = result.get(\"result\", [])\n",
    "                    \n",
    "                    if text:\n",
    "                        self.result_queue.put((\"final\", {\n",
    "                            \"text\": text,\n",
    "                            \"words\": words,\n",
    "                            \"alternatives\": result.get(\"alternatives\", [])\n",
    "                        }))\n",
    "                else:\n",
    "                    # We have a partial result\n",
    "                    partial = json.loads(self.recognizer.PartialResult())\n",
    "                    partial_text = partial.get(\"partial\", \"\")\n",
    "                    if partial_text:\n",
    "                        self.result_queue.put((\"partial\", {\"text\": partial_text}))\n",
    "            \n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start recognition\"\"\"\n",
    "        if self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        self.initialize()\n",
    "        \n",
    "        # Start processing thread\n",
    "        self.process_thread = threading.Thread(target=self._process_thread)\n",
    "        self.process_thread.daemon = True\n",
    "        self.process_thread.start()\n",
    "        \n",
    "        # Start audio stream\n",
    "        self.stream.start_stream()\n",
    "        print(\"Voice recognition started\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop recognition\"\"\"\n",
    "        if not self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = False\n",
    "        \n",
    "        # Stop and close the audio stream\n",
    "        if self.stream:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "            self.stream = None\n",
    "        \n",
    "        # Terminate PyAudio\n",
    "        if self.p:\n",
    "            self.p.terminate()\n",
    "            self.p = None\n",
    "        \n",
    "        # Get any final result\n",
    "        final_result = json.loads(self.recognizer.FinalResult())\n",
    "        if final_result.get(\"text\", \"\"):\n",
    "            self.result_queue.put((\"final\", {\n",
    "                \"text\": final_result.get(\"text\", \"\"),\n",
    "                \"words\": final_result.get(\"result\", [])\n",
    "            }))\n",
    "        \n",
    "        print(\"Voice recognition stopped\")\n",
    "    \n",
    "    def get_result(self, block=False, timeout=None):\n",
    "        \"\"\"\n",
    "        Get recognition result\n",
    "        \n",
    "        Returns:\n",
    "        - result_type: 'partial', 'final', or 'speech_end'\n",
    "        - result_data: Result data (text, words, etc.)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.result_queue.get(block=block, timeout=timeout)\n",
    "        except queue.Empty:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f40596",
   "metadata": {},
   "source": [
    "### Demo of the Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_complete_system(model_path=\"models/vosk-model-small-en-us-0.15\", \n",
    "                         use_grammar=False, run_time=30):\n",
    "    \"\"\"Demo of the complete voice recognition system\"\"\"\n",
    "    # Define grammar if used\n",
    "    grammar_list = [\n",
    "        \"what time is it\",\n",
    "        \"what's the weather\",\n",
    "        \"play music\",\n",
    "        \"stop music\",\n",
    "        \"turn on the lights\",\n",
    "        \"turn off the lights\",\n",
    "        \"open the door\",\n",
    "        \"close the door\"\n",
    "    ] if use_grammar else None\n",
    "    \n",
    "    # Create recognizer\n",
    "    recognizer = CompleteVoiceRecognizer(\n",
    "        model_path=model_path,\n",
    "        use_grammar=use_grammar,\n",
    "        grammar_list=grammar_list\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting voice recognition{'with grammar' if use_grammar else ''}...\")\n",
    "    recognizer.start()\n",
    "    \n",
    "    print(\"Speak now! (Press Ctrl+C to stop)\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < run_time:  # Run for specified seconds\n",
    "            # Get results\n",
    "            result = recognizer.get_result(block=False)\n",
    "            if result:\n",
    "                result_type, result_data = result\n",
    "                \n",
    "                if result_type == \"partial\":\n",
    "                    print(f\"\\rPartial: {result_data['text']}\", end=\"\", flush=True)\n",
    "                elif result_type == \"final\":\n",
    "                    print(f\"\\nFinal: {result_data['text']}\")\n",
    "                    \n",
    "                    # Print word timestamps if available\n",
    "                    if result_data.get(\"words\"):\n",
    "                        print(\"Word timestamps:\")\n",
    "                        for word in result_data[\"words\"]:\n",
    "                            print(f\"  {word['word']}: {word['start']:.2f}s - {word['end']:.2f}s\")\n",
    "                    \n",
    "                    # Print alternatives if available\n",
    "                    if result_data.get(\"alternatives\"):\n",
    "                        print(\"Alternatives:\")\n",
    "                        for alt in result_data[\"alternatives\"]:\n",
    "                            print(f\"  {alt['text']}\")\n",
    "                \n",
    "                elif result_type == \"speech_end\":\n",
    "                    print(\"\\n--- End of speech detected ---\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping...\")\n",
    "    finally:\n",
    "        recognizer.stop()\n",
    "\n",
    "# Uncomment to run demo\n",
    "# demo_complete_system(run_time=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29fef4",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "When implementing real-time speech recognition with Vosk, you might encounter some challenges:\n",
    "\n",
    "1. **High CPU Usage**: Processing audio in real-time can be CPU-intensive. Solutions:\n",
    "   - Reduce the sample rate if possible\n",
    "   - Process audio in a separate thread\n",
    "   - Skip some preprocessing steps for faster processing\n",
    "\n",
    "2. **Recognition Latency**: There might be a delay between speaking and recognition. Solutions:\n",
    "   - Use smaller audio chunks\n",
    "   - Use a smaller model\n",
    "   - Run on more powerful hardware\n",
    "\n",
    "3. **False Detections in Silence**: The recognizer might detect words in silence. Solutions:\n",
    "   - Implement proper silence detection\n",
    "   - Use a higher silence threshold\n",
    "   - Add a minimum audio level check before processing\n",
    "\n",
    "4. **Accuracy Issues**: Recognition might not be accurate enough. Solutions:\n",
    "   - Use a larger, more accurate model\n",
    "   - Apply preprocessing techniques (normalization, filtering)\n",
    "   - Implement a domain-specific grammar for constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda54eac",
   "metadata": {},
   "source": [
    "## Performance Optimizations\n",
    "\n",
    "For voice assistants, recognition performance is crucial. Here are some optimizations:\n",
    "\n",
    "1. **Buffer Management**: Only keep recent audio in memory\n",
    "2. **Adaptive Processing**: Apply heavier preprocessing only when needed\n",
    "3. **Model Selection**: Choose the right balance between size and accuracy\n",
    "4. **Grammar Constraints**: Use grammar when the possible responses are limited\n",
    "5. **Hardware Acceleration**: Some Vosk models support GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1653981",
   "metadata": {},
   "source": [
    "## BRIDGE TO PRACTICE\n",
    "\n",
    "Now that you understand the concepts of real-time speech recognition with Vosk, the practice guide will walk you through building a complete speech recognition system. You'll implement real-time audio capture, preprocessing, and recognition, and test it in various scenarios. By the end of the practice, you'll have a robust speech-to-text system that forms the foundation of your voice assistant."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
