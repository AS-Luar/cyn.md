{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df960eb",
   "metadata": {},
   "source": [
    "# VOSK INTRODUCTION\n",
    "\n",
    "## GLOSSARY\n",
    "- **Vosk**: An offline speech recognition toolkit that enables speech-to-text functionality without internet connectivity\n",
    "- **Speech Recognition**: The ability of a computer program to identify words spoken aloud and convert them to text\n",
    "- **ASR (Automatic Speech Recognition)**: Technology that converts spoken language into written text\n",
    "- **Acoustic Model**: A statistical representation of sounds that make up each word in a language\n",
    "- **Language Model**: A statistical model that determines the probability of a sequence of words\n",
    "- **Offline Processing**: Processing data without requiring an internet connection\n",
    "- **API (Application Programming Interface)**: A set of rules allowing different software applications to communicate with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa93b44",
   "metadata": {},
   "source": [
    "## CONCEPT INTERACTIONS\n",
    "- **Building on PyAudio**: Vosk works with audio data we capture using PyAudio concepts learned earlier\n",
    "- **Looking Forward**: After understanding Vosk basics, we'll learn how to preprocess audio to improve recognition accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098ee42",
   "metadata": {},
   "source": [
    "## MAIN CONTENT\n",
    "\n",
    "### Introduction to Vosk\n",
    "\n",
    "Vosk is an offline speech recognition toolkit designed to provide fast and accurate speech-to-text conversion without requiring an internet connection. Unlike cloud-based services like Google Speech API or Amazon Transcribe, Vosk runs entirely on your device, making it ideal for applications requiring privacy, reliability regardless of internet connectivity, or deployment on edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb66da6",
   "metadata": {},
   "source": [
    "### Key Features of Vosk\n",
    "\n",
    "1. **Offline Operation**: Works without internet connectivity\n",
    "2. **Multiple Language Support**: Provides models for many languages and dialects\n",
    "3. **Lightweight**: Can run on resource-constrained devices like Raspberry Pi\n",
    "4. **Open Source**: Free to use and modify\n",
    "5. **Cross-Platform**: Works on Windows, macOS, Linux, Android, and iOS\n",
    "6. **Python Integration**: Easy to use with Python applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0695244",
   "metadata": {},
   "source": [
    "### Installing Vosk\n",
    "\n",
    "Before we can use Vosk, we need to install both the Python package and a language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Vosk package\n",
    "#!pip install vosk\n",
    "\n",
    "# We'll also need SoundFile for handling audio files\n",
    "#!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08459724",
   "metadata": {},
   "source": [
    "After installation, we need to download a model for our target language. Vosk provides models of different sizes:\n",
    "- Small models (50-100MB): Fast but less accurate\n",
    "- Large models (1-2GB): More accurate but require more computational resources\n",
    "\n",
    "Models can be downloaded from: https://alphacephei.com/vosk/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da811a",
   "metadata": {},
   "source": [
    "### Basic Vosk Architecture\n",
    "\n",
    "Vosk operates using the following components:\n",
    "\n",
    "1. **Model**: Contains acoustic and language models for a specific language\n",
    "2. **Recognizer**: Processes audio data and extracts text\n",
    "3. **Result Handler**: Manages the recognition results\n",
    "\n",
    "Here's a simple diagram of how these components work together:\n",
    "\n",
    "Audio Input → Audio Preprocessing → Vosk Recognizer → Text Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c87a36",
   "metadata": {},
   "source": [
    "### Your First Vosk Program\n",
    "\n",
    "Here's a basic example that shows how to use Vosk with pre-recorded audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import wave\n",
    "import json\n",
    "\n",
    "# Path to the model - adjust this to your actual model path\n",
    "model_path = \"/home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15\"\n",
    "\n",
    "# Load the model\n",
    "model = Model(model_path)\n",
    "\n",
    "# Open the audio file - adjust this to an actual audio file in your system\n",
    "# You can use a recording from the PyAudio exercises if you have one\n",
    "audio_path = \"path/to/audio.wav\"  # Replace with your actual audio file path\n",
    "\n",
    "wf = wave.open(audio_path, \"rb\")\n",
    "\n",
    "# Create a recognizer\n",
    "recognizer = KaldiRecognizer(model, wf.getframerate())\n",
    "\n",
    "# Process the audio\n",
    "text = \"\"\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if recognizer.AcceptWaveform(data):\n",
    "        result = json.loads(recognizer.Result())\n",
    "        text += result.get(\"text\", \"\") + \" \"\n",
    "\n",
    "# Get the final results\n",
    "final_result = json.loads(recognizer.FinalResult())\n",
    "text += final_result.get(\"text\", \"\")\n",
    "\n",
    "print(\"Recognized text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f7a1d",
   "metadata": {},
   "source": [
    "### Understanding the Code\n",
    "\n",
    "Let's break down each element:\n",
    "\n",
    "1. **Model Initialization**:\n",
    "   ```python\n",
    "   model = Model(model_path)\n",
    "   ```\n",
    "   This loads the language model from disk. The model contains all the information needed to recognize speech in a specific language.\n",
    "\n",
    "2. **Recognizer Setup**:\n",
    "   ```python\n",
    "   recognizer = KaldiRecognizer(model, wf.getframerate())\n",
    "   ```\n",
    "   The KaldiRecognizer combines the model with the audio properties (specifically the sample rate) to create a recognition engine.\n",
    "\n",
    "3. **Processing Audio Data**:\n",
    "   ```python\n",
    "   while 1==1:\n",
    "       data = wf.readframes(4000)\n",
    "       if len(data) == 0:\n",
    "           break\n",
    "       if recognizer.AcceptWaveform(data):\n",
    "           result = json.loads(recognizer.Result())\n",
    "           text += result.get(\"text\", \"\") + \" \"\n",
    "   ```\n",
    "   This loop reads chunks of audio data and feeds them to the recognizer. When enough data is processed to recognize words, it returns a JSON result containing the recognized text.\n",
    "\n",
    "4. **Final Result**:\n",
    "   ```python\n",
    "   final_result = json.loads(recognizer.FinalResult())\n",
    "   ```\n",
    "   After all audio is processed, FinalResult() returns any remaining recognized text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489ea8b",
   "metadata": {},
   "source": [
    "#### Detailed Explanation: Processing Audio Data\n",
    "\n",
    "Let's look at the audio processing loop line by line:\n",
    "\n",
    "- `while True:`  \n",
    "  This starts an infinite loop to process the audio file in chunks.\n",
    "\n",
    "- `data = wf.readframes(4000)`  \n",
    "  Reads 4000 audio frames from the WAV file. This gives a small chunk of audio data to process at a time.\n",
    "\n",
    "- `if len(data) == 0:`  \n",
    "  Checks if there is no more audio data left to read (end of file).\n",
    "\n",
    "- `break`  \n",
    "  If there is no more data, exit the loop.\n",
    "\n",
    "- `if recognizer.AcceptWaveform(data):`  \n",
    "  Feeds the chunk of audio data to the recognizer. If the recognizer has enough data to recognize a phrase or sentence, it returns `True`.\n",
    "\n",
    "- `result = json.loads(recognizer.Result())`  \n",
    "  Gets the recognition result as a JSON string and parses it into a Python dictionary.\n",
    "\n",
    "- `text += result.get(\"text\", \"\") + \" \"`  \n",
    "  Extracts the recognized text from the result and adds it to the overall text string.\n",
    "\n",
    "This loop continues until all audio data is processed. Each chunk may or may not produce recognized text, depending on whether the recognizer has enough information to form words or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46163e02",
   "metadata": {},
   "source": [
    "### Try It Yourself!\n",
    "\n",
    "Now you can experiment by modifying the code above. Here are some suggestions:\n",
    "\n",
    "1. Use a different audio file\n",
    "2. Print intermediate results as they are recognized\n",
    "3. Measure the time it takes to process your audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e4a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15/graph/HCLr.fst /home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 1.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. Import any required modules (e.g., time, vosk, wave, json)\n",
    "# 2. Start timing the processing\n",
    "# 3. Set up the model path and load the Vosk model\n",
    "# 4. Open your audio file (WAV format)\n",
    "# 5. Create a recognizer using the model and audio sample rate\n",
    "\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "import vosk\n",
    "import os\n",
    "\n",
    "vmod = vosk.Model\n",
    "vrec = vosk.KaldiRecognizer\n",
    "texr = \"\"\n",
    "model = vmod('/home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15') \n",
    "wavfile = wave.open('/home/luar/AI/voice_assistant/Branch/recordings/test.wav', 'rb')\n",
    "recognizer = vrec(model, wavfile.getframerate())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 6. Process the audio in chunks and collect recognized text\n",
    "\n",
    "while 1:\n",
    "    binary = wavfile.readframes(4000)\n",
    "    if len(binary) == 0:\n",
    "        break\n",
    "    if recognizer.AcceptWaveform(binary):\n",
    "        result = json.loads(recognizer.Result())\n",
    "        print(result)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 7. Get the final recognition result and append to text\n",
    "\n",
    "# 8. Print the recognized text\n",
    "\n",
    "# 9. Stop timing and print the total processing time\n",
    "end_time = time.time()\n",
    "print(f\"Processing time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d840a26f",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "1. **Model Not Found**: Ensure you've downloaded the model and specified the correct path\n",
    "2. **Audio Format Issues**: Vosk works best with 16kHz, 16-bit mono audio\n",
    "3. **No Text Recognized**: Check if your audio file is clear, at a good volume, and in a supported language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e695c5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics of Vosk, you're ready to set up your first Vosk project. You'll need to:\n",
    "1. Install Vosk\n",
    "2. Download a model\n",
    "3. Create a simple script that recognizes speech from a WAV file\n",
    "\n",
    "The practice guide will walk you through each step in detail, helping you build a foundation for your voice assistant project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
