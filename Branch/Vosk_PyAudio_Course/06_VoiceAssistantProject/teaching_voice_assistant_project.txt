# VOICE ASSISTANT PROJECT: PUTTING IT ALL TOGETHER

## GLOSSARY

- **Voice Assistant**: A software application that uses voice recognition, natural language processing, and speech synthesis to provide services to users
- **Project Architecture**: The high-level structure that defines a system's components and their interactions
- **Feature Set**: A collection of capabilities or functions that a product offers to its users
- **User Experience (UX)**: The overall experience a user has when using a product, especially in terms of how easy or pleasing it is to use
- **System Integration**: The process of bringing together different components to form a cohesive, functioning whole
- **Continuous Operation**: A system that runs without interruption, handling errors gracefully
- **Configuration Management**: The process of handling settings that control a system's behavior
- **Dialog Management**: The component that controls conversation flow in a voice assistant
- **Action Handling**: The process of executing the appropriate function in response to a recognized intent
- **Error Recovery**: The ability of a system to detect, respond to, and recover from unexpected conditions

## CONCEPT INTERACTIONS

- **Building on Integration**: We'll use the integrated system from Module 5 as our foundation
- **Building on Speech Recognition**: We'll enhance the speech recognition component with wake word detection
- **Building on Speech Understanding**: We'll expand the intent recognition capabilities with more sophisticated patterns
- **New Concepts**: We'll add speech synthesis, configuration management, and dialog management

## MAIN CONTENT

### Project Overview

In this final module, we'll create a complete voice assistant that can:

1. Listen continuously for a wake word
2. Process speech commands using Vosk
3. Understand user intents and execute appropriate actions
4. Respond with synthesized speech
5. Maintain context across conversation turns
6. Handle errors gracefully

Our voice assistant will support multiple domains of functionality:

1. **Information queries**: Time, date, weather, general knowledge
2. **Media control**: Play/pause/stop music or videos
3. **Task management**: Set timers, reminders, or create to-do items
4. **System control**: Adjust volume, brightness, or other system settings

### System Architecture

Our voice assistant will use a layered architecture:

```
┌────────────────────┐
│  User Interface    │ ← User speech and system responses
├────────────────────┤
│  Dialog Manager    │ ← Manages conversation flow
├────────────────────┤
│  Intent Processor  │ ← Identifies user intentions
├────────────────────┤
│  Speech Services   │ ← Recognition and synthesis
├────────────────────┤
│  Action Handlers   │ ← Executes tasks based on intents
├────────────────────┤
│  Resource Access   │ ← APIs, databases, system resources
└────────────────────┘
```

### Wake Word Detection

Most voice assistants use a wake word (like "Hey Siri" or "Alexa") to:
1. Conserve resources by only processing speech when needed
2. Avoid responding to background conversations
3. Give users control over when the system is listening

We'll integrate a wake word detection system using:

1. **Porcupine**: A lightweight wake word detection library
2. **Custom activation**: Only process full speech recognition after wake word detection

Example code for wake word detection:

```python
import pvporcupine
import pyaudio
import struct

def detect_wake_word(wake_word="computer"):
    # Initialize Porcupine with the desired wake word
    porcupine = pvporcupine.create(keywords=[wake_word])
    
    # Set up audio input
    pa = pyaudio.PyAudio()
    audio_stream = pa.open(
        rate=porcupine.sample_rate,
        channels=1,
        format=pyaudio.paInt16,
        input=True,
        frames_per_buffer=porcupine.frame_length
    )
    
    print(f"Listening for wake word: '{wake_word}'")
    
    try:
        while True:
            # Read audio frame
            pcm = audio_stream.read(porcupine.frame_length)
            pcm = struct.unpack_from("h" * porcupine.frame_length, pcm)
            
            # Process with Porcupine
            keyword_index = porcupine.process(pcm)
            
            # Check if wake word was detected
            if keyword_index >= 0:
                print("Wake word detected!")
                return True
                
    finally:
        # Clean up
        if audio_stream:
            audio_stream.close()
        if pa:
            pa.terminate()
        if porcupine:
            porcupine.delete()
```

### Dialog Management

A voice assistant needs to maintain context across multiple conversation turns. Our dialog manager will:

1. Track the current conversation state
2. Maintain a history of recent interactions
3. Handle follow-up questions or ambiguous commands
4. Manage multi-turn interactions for complex tasks

Example dialog management code:

```python
class DialogManager:
    def __init__(self):
        self.conversation_state = "idle"  # Current state
        self.conversation_history = []    # History of turns
        self.current_context = {}         # Active entities and values
        self.pending_actions = []         # Actions waiting for more info
    
    def process_turn(self, intent, entities):
        # Add to history
        self.conversation_history.append({
            "intent": intent,
            "entities": entities,
            "timestamp": time.time()
        })
        
        # Update context with new entities
        if entities:
            self.current_context.update(entities)
        
        # Handle based on current state
        if self.conversation_state == "idle":
            return self._handle_new_request(intent, entities)
        
        elif self.conversation_state == "awaiting_confirmation":
            return self._handle_confirmation(intent)
            
        elif self.conversation_state == "awaiting_more_info":
            return self._handle_more_info(intent, entities)
    
    def _handle_new_request(self, intent, entities):
        # Process a new request from idle state
        if intent == "weather" and "location" not in entities:
            # Missing information, ask for it
            self.conversation_state = "awaiting_more_info"
            return "What city would you like the weather for?"
        
        # More intent handling...
```

### Speech Synthesis

To create a truly interactive experience, our assistant needs to respond verbally using speech synthesis. We'll use pyttsx3, a text-to-speech conversion library:

```python
import pyttsx3

class SpeechSynthesizer:
    def __init__(self):
        # Initialize the TTS engine
        self.engine = pyttsx3.init()
        
        # Configure properties
        self.engine.setProperty('rate', 150)    # Speed of speech
        self.engine.setProperty('volume', 0.9)  # Volume (0.0 to 1.0)
        
        # Get available voices
        voices = self.engine.getProperty('voices')
        if voices:
            self.engine.setProperty('voice', voices[0].id)  # Default voice
    
    def speak(self, text):
        """Convert text to speech and play it"""
        print(f"Assistant: {text}")  # Also print the response
        self.engine.say(text)
        self.engine.runAndWait()
    
    def change_voice(self, voice_index):
        """Change the voice used for speech"""
        voices = self.engine.getProperty('voices')
        if 0 <= voice_index < len(voices):
            self.engine.setProperty('voice', voices[voice_index].id)
            return True
        return False
    
    def adjust_rate(self, rate):
        """Adjust the speaking rate (words per minute)"""
        if 50 <= rate <= 300:
            self.engine.setProperty('rate', rate)
            return True
        return False
```

### Action Handlers

The action handlers execute the actual functionality requested by the user. We'll organize these by domain:

```python
class ActionHandlers:
    def __init__(self, speech_synthesizer):
        self.synthesizer = speech_synthesizer
        
        # Register handlers for different intents
        self.handlers = {
            "greeting": self.handle_greeting,
            "weather": self.handle_weather,
            "time": self.handle_time,
            "timer": self.handle_timer,
            "play_music": self.handle_play_music,
            "stop_music": self.handle_stop_music,
            "general_query": self.handle_general_query
        }
    
    def handle_intent(self, intent, entities):
        """Route the intent to the appropriate handler"""
        if intent in self.handlers:
            return self.handlers[intent](entities)
        else:
            return "I'm not sure how to help with that."
    
    # Handler implementations
    
    def handle_greeting(self, entities):
        import random
        responses = [
            "Hello! How can I help you today?",
            "Hi there! What can I do for you?",
            "Greetings! How may I assist you?",
            "Hello! What can I help you with?"
        ]
        response = random.choice(responses)
        self.synthesizer.speak(response)
        return response
    
    def handle_time(self, entities):
        from datetime import datetime
        current_time = datetime.now().strftime('%I:%M %p')
        response = f"The current time is {current_time}."
        self.synthesizer.speak(response)
        return response
    
    def handle_weather(self, entities):
        location = entities.get('location', 'your area')
        # In a real implementation, this would call a weather API
        response = f"The weather in {location} is currently sunny with a temperature of 72 degrees."
        self.synthesizer.speak(response)
        return response
    
    def handle_timer(self, entities):
        duration = entities.get('duration', 0)
        if duration > 0:
            # In a real implementation, this would start a timer
            response = f"Timer set for {duration} seconds."
            self.synthesizer.speak(response)
            return response
        else:
            response = "For how long should I set the timer?"
            self.synthesizer.speak(response)
            return response
    
    def handle_play_music(self, entities):
        song = entities.get('song', '')
        artist = entities.get('artist', '')
        
        if song and artist:
            response = f"Playing {song} by {artist}."
        elif song:
            response = f"Playing {song}."
        elif artist:
            response = f"Playing music by {artist}."
        else:
            response = "Playing some music for you."
            
        self.synthesizer.speak(response)
        # In a real implementation, this would integrate with a music service
        return response
    
    def handle_stop_music(self, entities):
        response = "Stopping the music."
        self.synthesizer.speak(response)
        # In a real implementation, this would stop the music service
        return response
    
    def handle_general_query(self, entities):
        query = entities.get('query', '')
        if not query:
            response = "What would you like to know about?"
        else:
            # In a real implementation, this would call a knowledge base or search API
            response = f"I found some information about {query}, but I'm still learning to answer general questions."
            
        self.synthesizer.speak(response)
        return response
```

### Configuration Management

Our assistant will be configurable to adapt to user preferences:

```python
import json
import os

class ConfigManager:
    def __init__(self, config_file="assistant_config.json"):
        self.config_file = config_file
        self.default_config = {
            "assistant_name": "Assistant",
            "wake_word": "computer",
            "voice_index": 0,
            "speech_rate": 150,
            "volume": 0.8,
            "language": "en-US",
            "log_level": "info",
            "user_name": "User"
        }
        
        # Load or create configuration
        self.config = self.load_config()
    
    def load_config(self):
        """Load configuration from file or create default"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r') as file:
                    config = json.load(file)
                    # Merge with defaults for any missing values
                    return {**self.default_config, **config}
            else:
                # Create default config file
                self.save_config(self.default_config)
                return self.default_config
                
        except Exception as e:
            print(f"Error loading configuration: {e}")
            return self.default_config
    
    def save_config(self, config):
        """Save configuration to file"""
        try:
            with open(self.config_file, 'w') as file:
                json.dump(config, file, indent=2)
            return True
        except Exception as e:
            print(f"Error saving configuration: {e}")
            return False
    
    def get(self, key, default=None):
        """Get a configuration value"""
        return self.config.get(key, default)
    
    def set(self, key, value):
        """Set a configuration value and save"""
        self.config[key] = value
        return self.save_config(self.config)
    
    def update(self, updates):
        """Update multiple configuration values and save"""
        self.config.update(updates)
        return self.save_config(self.config)
```

### Error Handling and Logging

Robust error handling is essential for a voice assistant that runs continuously:

```python
import logging
import traceback
import sys

def setup_logger(name, level=logging.INFO):
    """Configure logging for the assistant"""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    
    # File handler
    file_handler = logging.FileHandler(f"{name}.log")
    file_handler.setLevel(level)
    
    # Format
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)
    
    # Add handlers
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

class ErrorHandler:
    def __init__(self, logger):
        self.logger = logger
        self.error_counts = {}
    
    def handle_error(self, component, error, fatal=False):
        """Handle an error in a component"""
        error_type = type(error).__name__
        
        # Log the error
        self.logger.error(f"Error in {component}: {error_type} - {error}")
        self.logger.debug(traceback.format_exc())
        
        # Track error counts
        key = f"{component}:{error_type}"
        self.error_counts[key] = self.error_counts.get(key, 0) + 1
        
        # Check if this is a recurring error
        if self.error_counts[key] > 3:
            self.logger.warning(f"Recurring error in {component}: {error_type}")
            recovery_action = self.suggest_recovery(component, error_type)
            self.logger.info(f"Suggested recovery: {recovery_action}")
            
        # If fatal, raise the error to be handled at a higher level
        if fatal:
            raise error
            
        return self.get_error_response(component, error_type)
    
    def suggest_recovery(self, component, error_type):
        """Suggest a recovery action for a specific error"""
        # Components and their common errors
        strategies = {
            "audio_processor": {
                "IOError": "Restart audio device",
                "OSError": "Check audio permissions",
                "default": "Restart audio processing"
            },
            "speech_recognition": {
                "FileNotFoundError": "Check model path",
                "RuntimeError": "Reinitialize recognition model",
                "default": "Restart speech recognition"
            },
            "intent_processor": {
                "KeyError": "Check intent patterns",
                "ValueError": "Review intent handling logic",
                "default": "Reset context and continue"
            },
            "default": {
                "default": "Restart the affected component"
            }
        }
        
        # Get component strategies or default
        component_strategies = strategies.get(component, strategies["default"])
        
        # Get specific error strategy or component default
        return component_strategies.get(error_type, component_strategies["default"])
    
    def get_error_response(self, component, error_type):
        """Generate a user-friendly response for an error"""
        responses = {
            "audio_processor": "I'm having trouble hearing you. Please try again.",
            "speech_recognition": "I couldn't understand that. Could you repeat it?",
            "intent_processor": "I'm not sure what you want me to do.",
            "action_handler": "I can't do that right now.",
            "default": "Something went wrong. Let me try again."
        }
        
        return responses.get(component, responses["default"])
```

### Putting It All Together

Finally, we'll integrate all components into a complete voice assistant:

```python
class VoiceAssistant:
    def __init__(self, config_file="assistant_config.json"):
        # Set up logging
        self.logger = setup_logger("voice_assistant")
        self.logger.info("Initializing voice assistant")
        
        # Set up error handling
        self.error_handler = ErrorHandler(self.logger)
        
        # Load configuration
        self.config_manager = ConfigManager(config_file)
        self.config = self.config_manager.config
        
        # Initialize components
        try:
            self.init_components()
        except Exception as e:
            self.logger.critical(f"Failed to initialize assistant: {e}")
            raise
    
    def init_components(self):
        """Initialize all assistant components"""
        # Speech components
        self.speech_synthesizer = SpeechSynthesizer()
        
        # Adjust voice settings from config
        voice_index = self.config.get("voice_index", 0)
        speech_rate = self.config.get("speech_rate", 150)
        self.speech_synthesizer.change_voice(voice_index)
        self.speech_synthesizer.adjust_rate(speech_rate)
        
        # Set up audio processing and recognition
        model_path = self.config.get("model_path", "path/to/vosk/model")
        self.audio_processor = ThreadedAudioProcessor()
        self.recognition = ThreadedRecognition(
            self.audio_processor.get_audio_queue(), 
            model_path
        )
        
        # Set up understanding and action components
        self.intent_processor = IntentProcessor(self.recognition.get_text_queue())
        self.dialog_manager = DialogManager()
        self.action_handlers = ActionHandlers(self.speech_synthesizer)
        
        self.logger.info("All components initialized")
    
    def start(self):
        """Start the voice assistant"""
        self.logger.info("Starting voice assistant")
        
        try:
            # Start components
            self.audio_processor.start_processing()
            self.recognition.start_recognition()
            self.intent_processor.start_processing()
            
            # Initial greeting
            assistant_name = self.config.get("assistant_name", "Assistant")
            user_name = self.config.get("user_name", "User")
            greeting = f"Hello {user_name}, I'm {assistant_name}. How can I help you today?"
            self.speech_synthesizer.speak(greeting)
            
            # Main processing loop
            self.main_loop()
            
        except Exception as e:
            self.error_handler.handle_error("assistant_core", e, fatal=True)
        
        finally:
            self.stop()
    
    def main_loop(self):
        """Main assistant processing loop"""
        self.running = True
        
        while self.running:
            try:
                # Process intents
                result = self.intent_processor.get_results_queue().get(timeout=0.5)
                
                # Get dialog response
                dialog_response = self.dialog_manager.process_turn(
                    result["intent"], 
                    result["entities"]
                )
                
                if dialog_response:
                    # Use dialog response if available
                    self.speech_synthesizer.speak(dialog_response)
                else:
                    # Otherwise handle the intent directly
                    self.action_handlers.handle_intent(
                        result["intent"],
                        result["entities"]
                    )
                
                # Check for stop intent
                if result["intent"] == "stop":
                    self.logger.info("Stop intent detected")
                    self.running = False
                    
            except queue.Empty:
                continue  # No results, continue waiting
                
            except Exception as e:
                response = self.error_handler.handle_error("main_loop", e)
                self.speech_synthesizer.speak(response)
    
    def stop(self):
        """Stop the voice assistant"""
        self.logger.info("Stopping voice assistant")
        
        # Stop all components
        self.intent_processor.stop_processing()
        self.recognition.stop_recognition()
        self.audio_processor.stop_processing()
        
        self.logger.info("Voice assistant stopped")
```

### Running the Voice Assistant

```python
def main():
    """Main entry point for the voice assistant"""
    try:
        # Create and start the assistant
        assistant = VoiceAssistant()
        assistant.start()
        
    except KeyboardInterrupt:
        print("\nExiting voice assistant...")
        
    except Exception as e:
        print(f"Critical error: {e}")
        return 1
        
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

### Extending the Voice Assistant

The modular design of our voice assistant makes it easy to extend with new features:

1. **New Intent Handlers**: Add new intent patterns and corresponding handler functions
2. **API Integrations**: Connect to weather services, calendar APIs, music streaming services, etc.
3. **Custom Actions**: Add domain-specific functionality for your needs
4. **Improved NLP**: Replace the rule-based intent recognition with machine learning models
5. **Multilingual Support**: Add additional language models and translations
6. **Accessibility Features**: Add options for different speech rates, voices, interaction modes

## PRACTICAL IMPLEMENTATION

In the practice guide, you'll build the complete voice assistant by:

1. Creating a configuration system
2. Integrating wake word detection
3. Building a dialog management system
4. Adding speech synthesis for responses
5. Implementing domain-specific action handlers
6. Creating a robust error handling system

The result will be a functional voice assistant that you can extend with your own features and customizations.

## FURTHER LEARNING

1. **Deep Learning for NLP**: Explore transformer-based models for more sophisticated language understanding
2. **Voice Cloning**: Learn about custom voice synthesis to personalize your assistant's voice
3. **Multimodal Interfaces**: Combine voice with visual interfaces for a richer experience
4. **Edge Deployment**: Learn how to optimize speech processing for edge devices
5. **Privacy-Preserving Voice AI**: Explore techniques for processing voice commands locally without sending data to the cloud
