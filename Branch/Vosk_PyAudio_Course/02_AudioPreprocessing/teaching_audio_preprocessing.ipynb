{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b15160f",
   "metadata": {},
   "source": [
    "# AUDIO PREPROCESSING FOR SPEECH RECOGNITION\n",
    "\n",
    "## GLOSSARY\n",
    "\n",
    "- **Preprocessing**: Manipulating raw audio data before feeding it to a speech recognition system\n",
    "- **Resampling**: Converting audio from one sample rate to another\n",
    "- **Normalization**: Adjusting the volume of audio to a standard level\n",
    "- **Noise Reduction**: Removing unwanted background noise from audio\n",
    "- **VAD (Voice Activity Detection)**: Detecting which parts of an audio stream contain speech\n",
    "- **SNR (Signal-to-Noise Ratio)**: The ratio of speech signal power to background noise power\n",
    "- **Filtering**: Removing specific frequency components from audio\n",
    "- **Gain**: The amount by which audio is amplified\n",
    "- **dB (Decibel)**: A unit used to measure sound level and signal strength\n",
    "- **FFT (Fast Fourier Transform)**: An algorithm that converts time-domain signals to frequency-domain representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddfb0c8",
   "metadata": {},
   "source": [
    "## CONCEPT INTERACTIONS\n",
    "\n",
    "- **Building on PyAudio**: We'll use PyAudio to capture raw audio, then preprocess it before passing to Vosk\n",
    "- **Building on Vosk Basics**: The preprocessing techniques will improve the accuracy of Vosk's speech recognition\n",
    "- **Looking Forward**: After learning preprocessing, we'll integrate these techniques with Vosk for better speech-to-text conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b022b3f",
   "metadata": {},
   "source": [
    "## MAIN CONTENT\n",
    "\n",
    "### Why Preprocess Audio?\n",
    "\n",
    "Speech recognition systems like Vosk perform best with clean, well-formatted audio. In real-world environments, however, audio recordings often contain:\n",
    "\n",
    "1. **Background noise**: air conditioners, traffic, other people talking\n",
    "2. **Volume issues**: too quiet or too loud\n",
    "3. **Varying sample rates**: different recording devices use different rates\n",
    "4. **Irrelevant audio**: silence or non-speech segments\n",
    "\n",
    "Preprocessing addresses these issues, improving recognition accuracy by 20-40% in noisy environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab21754",
   "metadata": {},
   "source": [
    "### Key Preprocessing Techniques\n",
    "\n",
    "In this module, we'll focus on five essential preprocessing techniques:\n",
    "\n",
    "1. **Resampling**: Converting audio to a standard sample rate\n",
    "2. **Normalization**: Adjusting volume to a consistent level\n",
    "3. **Noise reduction**: Removing background noise\n",
    "4. **Voice Activity Detection (VAD)**: Identifying speech segments\n",
    "5. **Filtering**: Enhancing speech frequencies\n",
    "\n",
    "Let's explore each technique in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09336f69",
   "metadata": {},
   "source": [
    "### 1. Resampling\n",
    "\n",
    "**What it does**: Converts audio from one sample rate (e.g., 44.1kHz) to another (e.g., 16kHz).\n",
    "\n",
    "**Why it matters**: Vosk and most speech recognition models expect a specific sample rate, usually 16kHz. Using a different rate can dramatically reduce accuracy.\n",
    "\n",
    "Here's how to resample audio using librosa:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b9584",
   "metadata": {},
   "source": [
    "**Flowchart: Resampling Audio**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Load audio file and get original sample rate]\n",
    "   |\n",
    "   v\n",
    "[Is original sample rate == target?] --Yes--> [Return audio as is]\n",
    "   |\n",
    "  No\n",
    "   |\n",
    "   v\n",
    "[Resample audio to target sample rate]\n",
    "   |\n",
    "   v\n",
    "[Return resampled audio]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cee5fa",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `librosa.load`: Loads an audio file as a floating point time series and returns both the audio data and its sample rate.\n",
    "- `librosa.resample`: Changes the sample rate of audio data to a new target sample rate.\n",
    "- `soundfile.write` (`sf.write`): Saves audio data to a file in various formats (e.g., WAV).\n",
    "- `numpy.max`, `numpy.abs`: Used to find the maximum amplitude in the audio data.\n",
    "- `matplotlib.pyplot`: Used for plotting audio waveforms (not directly used in this code block, but imported for visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ca0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for audio processing and visualization\n",
    "import librosa              # For loading and resampling audio\n",
    "import soundfile as sf      # For saving audio files\n",
    "import numpy as np          # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting waveforms\n",
    "\n",
    "def resample_audio(audio_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Resample audio to target sample rate.\n",
    "    Parameters:\n",
    "        audio_path: Path to audio file\n",
    "        target_sr: Target sample rate (default: 16kHz)\n",
    "    Returns:\n",
    "        resampled_audio: Audio data at new sample rate\n",
    "        target_sr: Target sample rate\n",
    "    \"\"\"\n",
    "    # Load audio file with its original sample rate (sr=None disables automatic resampling)\n",
    "    audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "    print(f\"Loaded audio with original sample rate: {original_sr} Hz\")\n",
    "\n",
    "    # If the original sample rate is different from the target, resample\n",
    "    if original_sr != target_sr:\n",
    "        # Resample audio to the target sample rate\n",
    "        resampled_audio = librosa.resample(audio, orig_sr=original_sr, target_sr=target_sr)\n",
    "        print(f\"Resampled from {original_sr} Hz to {target_sr} Hz\")\n",
    "    else:\n",
    "        # If already at target sample rate, no need to resample\n",
    "        resampled_audio = audio\n",
    "        print(f\"Audio is already at {target_sr} Hz\")\n",
    "\n",
    "    return resampled_audio, target_sr\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# audio, sr = resample_audio(\"path_to_audio.wav\")\n",
    "# sf.write(\"resampled_audio.wav\", audio, sr)  # Save the resampled audio to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e211347",
   "metadata": {},
   "source": [
    "#### Breakdown: Resampling Audio\n",
    "\n",
    "- The function checks if the audio needs resampling and only processes it if necessary.\n",
    "- Using `librosa.load` with `sr=None` ensures you get the file's true sample rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f0af6",
   "metadata": {},
   "source": [
    "**Amplitude, Sample Rate, and Their Role in Resampling**\n",
    "\n",
    "- **Amplitude**: Represents the loudness of the audio at each sample point. Resampling does not change amplitude values, but changes how often they are measured per second.\n",
    "- **Sample Rate**: The number of samples per second (Hz). Higher sample rates capture more detail, but may not be supported by all models.\n",
    "- **Why Resample?**: Speech recognition models are trained on a specific sample rate (often 16kHz). Mismatched rates can cause the model to misinterpret the timing and frequency content of speech.\n",
    "- **Relation**: Resampling changes the number of data points per second, but preserves the overall shape and amplitude of the waveform as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ee75b",
   "metadata": {},
   "source": [
    "### 2. Normalization\n",
    "\n",
    "**What it does**: Adjusts the volume of the audio to a standard level.\n",
    "\n",
    "**Why it matters**: Audio that's too quiet may not be recognized, while audio that's too loud may be distorted.\n",
    "\n",
    "There are several normalization techniques:\n",
    "\n",
    "1. **Peak normalization**: Scales audio so the loudest peak reaches a target level\n",
    "2. **RMS normalization**: Scales audio to reach a target average loudness\n",
    "3. **Dynamic range compression**: Reduces the difference between loud and quiet parts\n",
    "\n",
    "Let's implement peak normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c288df",
   "metadata": {},
   "source": [
    "**Flowchart: Normalizing Audio**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Find maximum amplitude of audio]\n",
    "   |\n",
    "   v\n",
    "[Calculate current dB level]\n",
    "   |\n",
    "   v\n",
    "[Calculate gain needed to reach target dB]\n",
    "   |\n",
    "   v\n",
    "[Apply gain to audio]\n",
    "   |\n",
    "   v\n",
    "[Check if audio exceeds [-1, 1]]\n",
    "   |\n",
    "   v\n",
    "[If so, scale down to fit range]\n",
    "   |\n",
    "   v\n",
    "[Return normalized audio]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1a385",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `numpy.max`, `numpy.abs`: Used to find the maximum absolute amplitude in the audio array.\n",
    "- `numpy.log10`: Used to convert amplitude to decibels (dB).\n",
    "- Standard Python arithmetic for gain calculation and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(numpaudio, target_dB=-3):\n",
    "    \"\"\"\n",
    "    Normalize audio using peak normalization.\n",
    "    Parameters:\n",
    "        audio: Audio data (numpy array)\n",
    "        target_dB: Target peak level in dB (0 = max possible level)\n",
    "    Returns:\n",
    "        normalized_audio: Normalized audio data\n",
    "    \"\"\"\n",
    "    \n",
    "    shh = -80\n",
    "    good = -3\n",
    "    \n",
    "    \n",
    "    log = 20*np.log10\n",
    "    gainform = 10 ** (gain_dB / 20)\n",
    "    \n",
    "    # Use numpy's log10 for logarithmic calculations\n",
    "    # Find the maximum absolute amplitude in the audio (peak value)\n",
    "    # splits audio amplitude and gets the max value\n",
    "    max_amplitude = np.max(np.abs(numpaudio))\n",
    "    print(f\"Maximum amplitude before normalization: {max_amplitude}\")\n",
    "\n",
    "    # Convert the peak amplitude to decibels (dB)\n",
    "    current_dB = log(max_amplitude) if max_amplitude > 0 else shh\n",
    "    \n",
    "    print(f\"Current peak dB: {current_dB:.2f}\")\n",
    "\n",
    "    # Calculate the gain needed to reach the target dB\n",
    "    \n",
    "    gain_dB = target_dB - current_dB\n",
    "    gain_amp = gainform(gain_dB) \n",
    "    \n",
    "    \n",
    "    print(f\"Applying gain (linear): {gain_amp}\")\n",
    "\n",
    "    # Apply the gain to the audio\n",
    "    normalized_audio = numpaudio * gain_amp\n",
    "\n",
    "    # Ensure the audio does not exceed the range [-1, 1] (to avoid distortion)\n",
    "    \n",
    "    if np.max(np.abs(normalized_audio)) > 1.0:\n",
    "        normalized_audio = normalized_audio / np.max(np.abs(normalized_audio))\n",
    "        print(\"Audio was clipped; scaled down to fit [-1, 1]\")\n",
    "\n",
    "    print(f\"Normalized audio from {current_dB:.2f}dB to {target_dB:.2f}dB\")\n",
    "    return normalized_audio\n",
    "\n",
    "# Example usage:\n",
    "# normalized = normalize_audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e0b10",
   "metadata": {},
   "source": [
    "#### Breakdown: Normalizing Audio\n",
    "\n",
    "- Decibels (dB) are a logarithmic way to express amplitude; this function ensures the loudest part of the audio matches the target dB.\n",
    "- **Amplitude** is a measure of how \"strong\" or \"loud\" a signal is at any point in time. In digital audio, amplitude values are typically floating-point numbers between -1.0 and 1.0, where 0 is silence, positive values are above the center line, and negative values are below.\n",
    "- **Absolute amplitude** means we ignore the sign (positive or negative) and just look at the size of the value. For example, both -0.8 and 0.8 have an absolute amplitude of 0.8. This is important because loudness is about how far the signal moves from zero, not the direction.\n",
    "- **Maximum absolute amplitude** is the largest distance from zero in the entire audio signal. This is used for \"peak normalization\" because it tells us the loudest moment in the audio.\n",
    "- **Decibels (dB)** are a logarithmic way to express amplitude. The formula `20 * log10(amplitude)` converts a linear amplitude (like 0.5) to dB. This is useful because our ears perceive loudness logarithmically, and dB makes it easier to compare very large and very small values.\n",
    "- **Relation**: The higher the amplitude, the higher the dB value. If amplitude is 1.0, that's 0 dB (the maximum possible in digital audio). Lower amplitudes have negative dB values (e.g., 0.5 amplitude â‰ˆ -6 dB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a244e7",
   "metadata": {},
   "source": [
    "### 3. Noise Reduction\n",
    "\n",
    "**What it does**: Reduces background noise while preserving speech content.\n",
    "\n",
    "**Why it matters**: Background noise can significantly interfere with speech recognition.\n",
    "\n",
    "We'll use the `noisereduce` library, which employs spectral gating - a technique that identifies and removes frequencies that likely contain noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a229b",
   "metadata": {},
   "source": [
    "**What is a Noise Clip? What is a Noise Reference?**\n",
    "\n",
    "- **Noise Clip**: A short segment of audio that contains only background noise, with no speech. This can be recorded before or after the main speech, or extracted from a silent part of the recording. It serves as an example of what the \"noise\" sounds like.\n",
    "- **Noise Reference**: The actual data (from a noise clip) used by noise reduction algorithms to learn the characteristics of the noise. By providing a noise reference, the algorithm can more accurately remove similar noise from the rest of the audio.\n",
    "- **Why Use Them?**: If you provide a noise clip as a reference, noise reduction works better because it knows exactly what to filter out. If you don't provide one, the algorithm tries to estimate noise from the audio itself, which may be less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053699ce",
   "metadata": {},
   "source": [
    "**Flowchart: Noise Reduction**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Is a noise clip provided?] --Yes--> [Use noise clip as reference]\n",
    "   |                                   |\n",
    "  No                                   v\n",
    "   |                             [Reduce noise using reference]\n",
    "   v                                   |\n",
    "[Estimate noise from audio itself]     |\n",
    "   |                                   v\n",
    "   +------------------------------> [Return cleaned audio]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff623d",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `noisereduce.reduce_noise(y, sr, y_noise=None, stationary=True)`: Reduces noise in the signal `y` at sample rate `sr`.  \n",
    "  - `y`: The noisy audio (numpy array).  \n",
    "  - `sr`: Sample rate (int).  \n",
    "  - `y_noise`: Optional noise reference (numpy array).  \n",
    "  - `stationary`: If `True`, assumes noise is constant; if `False`, adapts to changing noise.\n",
    "- `matplotlib.pyplot.plot(x, y)`: Plots `y` (amplitude) versus `x` (time or sample index).\n",
    "  - `x`: X-axis data (e.g., time array).\n",
    "  - `y`: Y-axis data (audio amplitude).\n",
    "- `numpy.linspace(start, stop, num)`: Returns `num` evenly spaced values from `start` to `stop`.\n",
    "  - `start`: Start value.\n",
    "  - `stop`: End value.\n",
    "  - `num`: Number of samples to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454120ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import noisereduce as nr\n",
    "\n",
    "def reduce_noise(numpaudio, sr, noise_clip=None, stationary=True):\n",
    "    \"\"\"\n",
    "    Reduce noise in audio. \n",
    "    Parameters:\n",
    "        numpaudio: Audio data (numpy array)\n",
    "        sr: Sample rate\n",
    "        noise_clip: Reference noise clip (if available)\n",
    "        stationary: Whether noise is consistent (True) or changes over time (False)\n",
    "    Returns:\n",
    "        cleaned_audio: Audio with reduced noise\n",
    "    \"\"\"\n",
    "    if noise_clip is not None:\n",
    "        # Use the provided noise clip as a reference for what is \"noise\"\n",
    "        reduced = nr.reduce_noise(y=numpaudio, sr=sr, y_noise=noise_clip)\n",
    "        print(\"Noise reduction applied using reference noise sample\")\n",
    "    else:\n",
    "        # Estimate noise from the audio itself (good for stationary noise)\n",
    "        reduced = nr.reduce_noise(y=numpaudio, sr=sr, stationary=stationary)\n",
    "        print(\"Noise reduction applied using estimated noise profile\")\n",
    "    return reduced\n",
    "\n",
    "# Example usage:\n",
    "# reduced_audio = reduce_noise(audio, sr)\n",
    "\n",
    "def plot_waveforms(original, processed, sr, title=\"Noise Reduction\"):\n",
    "    \"\"\"\n",
    "    Plot original and processed audio waveforms for comparison.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plot original audio\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(np.linspace(0, len(original)/sr, len(original)), original)\n",
    "    plt.title(\"Original Audio\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    # Plot processed audio\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(np.linspace(0, len(processed)/sr, len(processed)), processed, color='orange')\n",
    "    plt.title(\"Processed Audio\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Example visualization:\n",
    "# plot_waveforms(audio, reduced_audio, sr, \"Before vs. After Noise Reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e9741",
   "metadata": {},
   "source": [
    "#### Breakdown: Noise Reduction\n",
    "\n",
    "- Spectral gating works by identifying frequency regions that are likely noise and reducing their volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b8f03b",
   "metadata": {},
   "source": [
    "**Amplitude, Noise, and Their Role in Noise Reduction**\n",
    "\n",
    "- **Amplitude**: Represents both speech and noise in the audio signal. Noise reduction aims to lower the amplitude of unwanted noise without affecting speech.\n",
    "- **Noise**: Unwanted, often random, background sounds that overlap with speech. Noise can be stationary (constant, like a fan) or non-stationary (changing, like traffic).\n",
    "- **Spectral Gating**: Analyzes the frequency content of the audio and reduces the amplitude of frequencies identified as noise.\n",
    "- **Relation**: By reducing the amplitude of noise frequencies, the speech signal stands out more clearly, improving recognition accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e1d89",
   "metadata": {},
   "source": [
    "### 4. Voice Activity Detection (VAD)\n",
    "\n",
    "**What it does**: Identifies segments of audio that contain speech (vs. silence or noise).\n",
    "\n",
    "**Why it matters**: Processing only speech segments improves recognition speed and accuracy.\n",
    "\n",
    "We'll use the `webrtcvad` library, which implements Google's WebRTC Voice Activity Detector:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6277742",
   "metadata": {},
   "source": [
    "**Flowchart: Voice Activity Detection**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Check sample rate is valid]\n",
    "   |\n",
    "   v\n",
    "[Normalize audio to [-1, 1]]\n",
    "   |\n",
    "   v\n",
    "[Convert audio to int16]\n",
    "   |\n",
    "   v\n",
    "[Split audio into frames]\n",
    "   |\n",
    "   v\n",
    "[For each frame:]\n",
    "   |\n",
    "   v\n",
    "[Check if frame contains speech]\n",
    "   |\n",
    "   v\n",
    "[Collect speech frames as True/False]\n",
    "   |\n",
    "   v\n",
    "[Return speech frames list]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8196fd",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `webrtcvad.Vad`: Creates a Voice Activity Detector object.\n",
    "- `Vad.is_speech`: Checks if a given audio frame contains speech.\n",
    "- `struct.pack`: Converts a list of int16 samples into bytes for VAD processing.\n",
    "- `numpy.max`, `numpy.abs`, `numpy.zeros_like`: Used for normalization and creating silent arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def detect_speech(audio, sr, frame_duration=30, aggressiveness=3):\n",
    "    \"\"\"\n",
    "    Detect speech segments using WebRTC VAD.\n",
    "    Parameters:\n",
    "        audio: Audio data (mono, numpy array)\n",
    "        sr: Sample rate (must be 8000, 16000, 32000, or 48000 Hz)\n",
    "        frame_duration: Frame duration in ms (10, 20, or 30)\n",
    "        aggressiveness: VAD aggressiveness (0-3, higher = more aggressive)\n",
    "    Returns:\n",
    "        speech_frames: List of booleans (True = speech detected)\n",
    "    \"\"\"\n",
    "    vad = webrtcvad.Vad(aggressiveness)  # Create VAD object\n",
    "\n",
    "    # Check if sample rate is valid for VAD\n",
    "    valid_rates = (8000, 16000, 32000, 48000)\n",
    "    if sr not in valid_rates:\n",
    "        raise ValueError(f\"Sample rate must be one of {valid_rates}\")\n",
    "\n",
    "    # Normalize audio to [-1, 1] if needed\n",
    "    if np.max(np.abs(audio)) > 1.0:\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    # Convert audio from float [-1, 1] to int16 (required by VAD)\n",
    "    audio_int16 = (audio * 32767).astype(np.int16)\n",
    "\n",
    "    # Calculate frame size in samples\n",
    "    frame_size = int(sr * frame_duration / 1000)\n",
    "\n",
    "    # Analyze each frame for speech\n",
    "    speech_frames = []\n",
    "    for i in range(0, len(audio_int16) - frame_size, frame_size):\n",
    "        frame = audio_int16[i:i + frame_size]\n",
    "        frame_bytes = struct.pack(\"h\" * len(frame), *frame)  # Convert to bytes\n",
    "        is_speech = vad.is_speech(frame_bytes, sr)           # VAD decision\n",
    "        speech_frames.append(is_speech)\n",
    "\n",
    "    speech_percentage = sum(speech_frames) / len(speech_frames) * 100\n",
    "    print(f\"Detected speech in {speech_percentage:.1f}% of frames\")\n",
    "    return speech_frames\n",
    "\n",
    "# Example usage:\n",
    "# speech_frames = detect_speech(audio, sr)\n",
    "\n",
    "def keep_speech_segments(audio, sr, speech_frames, frame_duration=30):\n",
    "    \"\"\"\n",
    "    Keep only speech segments from audio.\n",
    "    Parameters:\n",
    "        audio: Audio data\n",
    "        sr: Sample rate\n",
    "        speech_frames: List of booleans from detect_speech()\n",
    "        frame_duration: Frame duration in ms\n",
    "    Returns:\n",
    "        speech_only: Audio with non-speech segments replaced by silence\n",
    "    \"\"\"\n",
    "    frame_size = int(sr * frame_duration / 1000)\n",
    "    speech_only = np.zeros_like(audio)  # Start with silence\n",
    "\n",
    "    for i, is_speech in enumerate(speech_frames):\n",
    "        if is_speech:\n",
    "            start = i * frame_size\n",
    "            end = min(start + frame_size, len(audio))\n",
    "            speech_only[start:end] = audio[start:end]  # Copy speech frames\n",
    "\n",
    "    return speech_only\n",
    "\n",
    "# Example usage:\n",
    "# speech_only_audio = keep_speech_segments(audio, sr, speech_frames)\n",
    "# plot_waveforms(audio, speech_only_audio, sr, \"Original vs. Speech Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c5752",
   "metadata": {},
   "source": [
    "#### Breakdown: Voice Activity Detection (VAD)\n",
    "\n",
    "- Aggressiveness controls how strict the VAD is: higher values remove more non-speech but may cut off quiet speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9d18f",
   "metadata": {},
   "source": [
    "**Amplitude, Speech Detection, and Their Role in VAD**\n",
    "\n",
    "- **Amplitude**: Speech tends to have higher and more variable amplitude than silence or background noise.\n",
    "- **VAD**: Uses both amplitude and frequency patterns to decide if a frame contains speech.\n",
    "- **Frames**: Audio is split into short segments (frames) for analysis; each is checked for speech presence.\n",
    "- **Relation**: VAD helps isolate the parts of the audio where speech is present, reducing the amount of irrelevant data passed to the recognizer and improving both speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c4b2f",
   "metadata": {},
   "source": [
    "### 5. Filtering\n",
    "\n",
    "**What it does**: Enhances frequencies important for speech while reducing others.\n",
    "\n",
    "**Why it matters**: Human speech typically occurs in the 300-3000Hz range. Filtering can emphasize these frequencies.\n",
    "\n",
    "Let's implement a band-pass filter to focus on speech frequencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a8ec3",
   "metadata": {},
   "source": [
    "**Flowchart: Bandpass Filtering**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Calculate Nyquist frequency]\n",
    "   |\n",
    "   v\n",
    "[Normalize cutoff frequencies]\n",
    "   |\n",
    "   v\n",
    "[Design bandpass filter]\n",
    "   |\n",
    "   v\n",
    "[Apply filter to audio]\n",
    "   |\n",
    "   v\n",
    "[Return filtered audio]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa10b8",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `scipy.signal.butter`: Designs a Butterworth filter (returns filter coefficients).\n",
    "- `scipy.signal.lfilter`: Applies the designed filter to the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1304956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "def bandpass_filter(audio, sr, lowcut=300, highcut=3000, order=5):\n",
    "    \"\"\"\n",
    "    Apply bandpass filter to audio.\n",
    "    Parameters:\n",
    "        audio: Audio data\n",
    "        sr: Sample rate\n",
    "        lowcut: Low frequency cutoff (Hz)\n",
    "        highcut: High frequency cutoff (Hz)\n",
    "        order: Filter order (higher = sharper cutoff)\n",
    "    Returns:\n",
    "        filtered_audio: Audio with frequencies outside range attenuated\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * sr  # Nyquist frequency is half the sample rate\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "\n",
    "    # Design a Butterworth bandpass filter\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "\n",
    "    # Apply the filter to the audio\n",
    "    filtered_audio = lfilter(b, a, audio)\n",
    "\n",
    "    print(f\"Applied bandpass filter: {lowcut}-{highcut}Hz\")\n",
    "    return filtered_audio\n",
    "\n",
    "# Example usage:\n",
    "# filtered_audio = bandpass_filter(audio, sr)\n",
    "# plot_waveforms(audio, filtered_audio, sr, \"Original vs. Filtered (300-3000Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52e99a",
   "metadata": {},
   "source": [
    "#### Breakdown: Bandpass Filtering\n",
    "\n",
    "- The Butterworth filter is chosen for its flat frequency response in the passband."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d744fdf",
   "metadata": {},
   "source": [
    "**Amplitude, Frequency, and Their Role in Filtering**\n",
    "\n",
    "- **Amplitude**: Filtering changes the amplitude of different frequency components in the audio.\n",
    "- **Frequency**: Human speech is concentrated between 300Hz and 3000Hz. Frequencies outside this range are often noise or irrelevant.\n",
    "- **Bandpass Filter**: Allows frequencies within a certain range to pass through, while attenuating (reducing) others.\n",
    "- **Relation**: By filtering out frequencies not used in speech, we reduce noise and make the speech signal clearer for recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63106d6a",
   "metadata": {},
   "source": [
    "### Building a Complete Preprocessing Pipeline\n",
    "\n",
    "Now, let's combine all these techniques into a complete audio preprocessing pipeline for speech recognition:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e91f2f",
   "metadata": {},
   "source": [
    "**Flowchart: Preprocessing Pipeline**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Resample audio]\n",
    "   |\n",
    "   v\n",
    "[Normalize audio]\n",
    "   |\n",
    "   v\n",
    "[If noise reduction enabled, reduce noise]\n",
    "   |\n",
    "   v\n",
    "[If VAD enabled, keep only speech segments]\n",
    "   |\n",
    "   v\n",
    "[If filtering enabled, apply bandpass filter]\n",
    "   |\n",
    "   v\n",
    "[Return processed audio]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55384df5",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `resample_audio`: Resamples audio to the target sample rate.\n",
    "- `normalize_audio`: Normalizes audio volume.\n",
    "- `reduce_noise`: Reduces background noise.\n",
    "- `detect_speech`: Detects speech frames using VAD.\n",
    "- `keep_speech_segments`: Keeps only speech segments in the audio.\n",
    "- `bandpass_filter`: Applies a bandpass filter to the audio.\n",
    "- All these are user-defined functions from previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, target_sr=16000, normalize_target_dB=-3, \n",
    "                     reduce_noise=True, vad_enabled=True, apply_filter=True):\n",
    "    \"\"\"\n",
    "    Complete audio preprocessing pipeline.\n",
    "    Parameters:\n",
    "        audio_path: Path to audio file\n",
    "        target_sr: Target sample rate (default: 16kHz)\n",
    "        normalize_target_dB: Target normalization level (dB)\n",
    "        reduce_noise: Whether to apply noise reduction\n",
    "        vad_enabled: Whether to apply voice activity detection\n",
    "        apply_filter: Whether to apply bandpass filtering\n",
    "    Returns:\n",
    "        processed_audio: Fully processed audio\n",
    "        sr: Sample rate of processed audio\n",
    "    \"\"\"\n",
    "    print(f\"===== Processing {audio_path} =====\")\n",
    "\n",
    "    # Step 1: Resample to target sample rate\n",
    "    audio, sr = resample_audio(audio_path, target_sr)\n",
    "\n",
    "    # Step 2: Normalize audio volume\n",
    "    audio = normalize_audio(audio, normalize_target_dB)\n",
    "\n",
    "    # Step 3: Optionally reduce noise\n",
    "    if reduce_noise:\n",
    "        audio = reduce_noise(audio, sr)\n",
    "\n",
    "    # Step 4: Optionally apply VAD to keep only speech\n",
    "    if vad_enabled:\n",
    "        speech_frames = detect_speech(audio, sr)\n",
    "        audio = keep_speech_segments(audio, sr, speech_frames)\n",
    "\n",
    "    # Step 5: Optionally apply bandpass filter\n",
    "    if apply_filter:\n",
    "        audio = bandpass_filter(audio, sr)\n",
    "\n",
    "    print(\"===== Processing complete =====\")\n",
    "    return audio, sr\n",
    "\n",
    "# Example usage:\n",
    "# processed, sr = preprocess_audio(\"path_to_audio.wav\")\n",
    "# sf.write(\"processed_audio.wav\", processed, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23161f",
   "metadata": {},
   "source": [
    "#### Breakdown: Complete Preprocessing Pipeline\n",
    "\n",
    "- Each step is modular and can be enabled or disabled as needed for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52caf74d",
   "metadata": {},
   "source": [
    "**How Each Step Contributes to Better Recognition**\n",
    "\n",
    "- **Resampling**: Ensures compatibility with the speech recognition model.\n",
    "- **Normalization**: Guarantees consistent loudness, preventing missed or distorted words.\n",
    "- **Noise Reduction**: Removes background noise, making speech clearer.\n",
    "- **VAD**: Focuses processing on speech, ignoring silence and irrelevant sounds.\n",
    "- **Filtering**: Emphasizes the frequency range of human speech, further reducing noise and improving clarity.\n",
    "- **Combined Effect**: Each step addresses a specific problem in raw audio, and together they maximize the quality and recognizability of speech for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab2a86",
   "metadata": {},
   "source": [
    "### Testing Preprocessing with Vosk\n",
    "\n",
    "To see the impact of preprocessing, we can test it with Vosk:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8b2dd",
   "metadata": {},
   "source": [
    "**Flowchart: Speech Recognition with Vosk**\n",
    "\n",
    "```\n",
    "[Start]\n",
    "   |\n",
    "   v\n",
    "[Check if model exists]\n",
    "   |\n",
    "   v\n",
    "[Save audio data to temporary WAV file]\n",
    "   |\n",
    "   v\n",
    "[Open WAV file for reading]\n",
    "   |\n",
    "   v\n",
    "[Load Vosk model and create recognizer]\n",
    "   |\n",
    "   v\n",
    "[Read audio in chunks and recognize speech]\n",
    "   |\n",
    "   v\n",
    "[Collect recognized text]\n",
    "   |\n",
    "   v\n",
    "[Delete temporary file]\n",
    "   |\n",
    "   v\n",
    "[Return recognized text]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5dd1f2",
   "metadata": {},
   "source": [
    "**Key Functions Used in This Section**\n",
    "\n",
    "- `vosk.Model`: Loads a Vosk speech recognition model.\n",
    "- `vosk.KaldiRecognizer`: Creates a recognizer object for processing audio.\n",
    "- `soundfile.write` (`sf.write`): Saves numpy audio data to a WAV file.\n",
    "- `wave.open`: Opens a WAV file for reading.\n",
    "- `KaldiRecognizer.AcceptWaveform`: Processes audio frames and returns recognition results.\n",
    "- `json.loads`: Parses JSON output from the recognizer.\n",
    "- `os.remove`: Deletes the temporary WAV file after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc498e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import wave\n",
    "import json\n",
    "import os\n",
    "\n",
    "def recognize_speech(audio_data, sample_rate, model_path):\n",
    "    \"\"\"\n",
    "    Recognize speech in audio data using Vosk.\n",
    "    Parameters:\n",
    "        audio_data: Numpy array of audio data\n",
    "        sample_rate: Sample rate of the audio\n",
    "        model_path: Path to Vosk model\n",
    "    Returns:\n",
    "        text: Recognized text\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model not found at {model_path}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Save audio to a temporary WAV file for Vosk to read\n",
    "    temp_wav = \"temp_audio.wav\"\n",
    "    sf.write(temp_wav, audio_data, sample_rate)\n",
    "\n",
    "    # Open the WAV file for reading\n",
    "    wf = wave.open(temp_wav, \"rb\")\n",
    "\n",
    "    # Load the Vosk model and create a recognizer\n",
    "    model = Model(model_path)\n",
    "    recognizer = KaldiRecognizer(model, wf.getframerate())\n",
    "\n",
    "    # Read and process audio in chunks\n",
    "    text = \"\"\n",
    "    while True:\n",
    "        data = wf.readframes(4000)  # Read 4000 frames at a time\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = json.loads(recognizer.Result())\n",
    "            text += result.get(\"text\", \"\") + \" \"\n",
    "\n",
    "    # Get the final recognition result\n",
    "    final_result = json.loads(recognizer.FinalResult())\n",
    "    text += final_result.get(\"text\", \"\")\n",
    "\n",
    "    # Clean up: remove the temporary file\n",
    "    os.remove(temp_wav)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage:\n",
    "# model_path = \"/path/to/vosk-model\"\n",
    "# original_audio, sr = librosa.load(\"audio.wav\", sr=16000)\n",
    "# processed_audio, sr = preprocess_audio(\"audio.wav\")\n",
    "# print(\"Recognition without preprocessing:\")\n",
    "# print(recognize_speech(original_audio, sr, model_path))\n",
    "# print(\"\\nRecognition with preprocessing:\")\n",
    "# print(recognize_speech(processed_audio, sr, model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa6e02",
   "metadata": {},
   "source": [
    "#### Breakdown: Speech Recognition with Vosk\n",
    "\n",
    "- Vosk expects audio in WAV format, so we save the numpy array to a temporary file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13417b3",
   "metadata": {},
   "source": [
    "**Why Preprocessing Improves Recognition Results**\n",
    "\n",
    "- **Raw audio** may contain noise, silence, or irrelevant frequencies that confuse the recognizer.\n",
    "- **Preprocessed audio** is cleaner, with only the important speech content at the right loudness and frequency range.\n",
    "- **Recognition accuracy**: Preprocessing can significantly increase the percentage of correctly recognized words, especially in challenging environments.\n",
    "- **Practical tip**: Always compare recognition results before and after preprocessing to see the improvement for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9394eed",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "1. **Out of memory errors**: When processing long audio files, try processing in chunks\n",
    "2. **Distortion after preprocessing**: Ensure normalization doesn't clip audio and filter settings are appropriate\n",
    "3. **VAD removing speech**: Try lower aggressiveness values (0-1 instead of 2-3)\n",
    "4. **Noise reduction artifacts**: Use non-stationary mode for varying noise conditions\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **For real-time applications**: Skip intensive steps like noise reduction\n",
    "2. **For batch processing**: Use all preprocessing steps for maximum accuracy\n",
    "3. **For VAD**: Frame duration of 10-30ms works best for typical speech\n",
    "4. **For noise reduction**: Providing a separate noise profile when possible improves results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31e40f",
   "metadata": {},
   "source": [
    "## BRIDGE TO PRACTICE\n",
    "\n",
    "In the practice guide, you'll build a complete preprocessing pipeline and test it with real audio. You'll compare recognition results before and after preprocessing to see how each technique improves accuracy. The final exercise will integrate everything into a reusable audio processing module that you can use in your voice assistant project."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
