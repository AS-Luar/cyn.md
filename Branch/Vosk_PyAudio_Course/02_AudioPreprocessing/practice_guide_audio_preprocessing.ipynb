{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e991414",
   "metadata": {},
   "source": [
    "# AUDIO PREPROCESSING PRACTICE GUIDE\n",
    "\n",
    "## EXERCISE GOAL\n",
    "In this practice exercise, you'll build a complete audio preprocessing pipeline for speech recognition and test how it improves Vosk's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d652d38",
   "metadata": {},
   "source": [
    "## STEP-BY-STEP INSTRUCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc692171",
   "metadata": {},
   "source": [
    "### Step 1: Setup Project and Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3478016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install vosk librosa scipy numpy matplotlib noisereduce webrtcvad soundfile\n",
    "\n",
    "# Verify all libraries installed correctly\n",
    "import vosk, librosa, scipy, numpy, matplotlib, noisereduce\n",
    "try:\n",
    "    import webrtcvad, soundfile\n",
    "    print('All libraries installed successfully!')\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef5ab4f",
   "metadata": {},
   "source": [
    "### Step 2: Create a Sample Audio Recording\n",
    "\n",
    "We'll need an audio file to test our preprocessing. You can either:\n",
    "1. Record your own audio\n",
    "2. Use the provided functions to record audio\n",
    "3. Use an existing audio file\n",
    "\n",
    "Let's set up a function to record some test audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfac84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import os\n",
    "\n",
    "def record_test_audio(filename=\"test_audio.wav\", duration=5, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Record audio for testing preprocessing\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Output filename\n",
    "    - duration: Recording duration in seconds\n",
    "    - sample_rate: Sample rate (16000 recommended for Vosk)\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=sample_rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=1024)\n",
    "    \n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(0, int(sample_rate / 1024 * duration)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "        if i % 10 == 0:\n",
    "            # Print a dot every 10 frames to show progress\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            \n",
    "    print(\"\\nFinished recording!\")\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(sample_rate)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    \n",
    "    print(f\"Audio saved to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Record a test audio file (uncomment if you want to record)\n",
    "# test_file = record_test_audio(duration=5)\n",
    "\n",
    "# If you already have an audio file, set the path here:\n",
    "# test_file = \"path/to/your/audio.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b89d39",
   "metadata": {},
   "source": [
    "### Step 3: Create the Audio Preprocessing Module\n",
    "\n",
    "Now, let's build a comprehensive audio preprocessing module. We'll create a class that implements all the preprocessing techniques we learned about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "import webrtcvad\n",
    "import struct\n",
    "import os\n",
    "import json\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"A complete audio preprocessing pipeline for speech recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_sr=16000, normalize_level=-3, \n",
    "                 vad_aggressiveness=3, lowcut=300, highcut=3000):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor\n",
    "        \n",
    "        Parameters:\n",
    "        - target_sr: Target sample rate in Hz\n",
    "        - normalize_level: Target peak normalization level in dB\n",
    "        - vad_aggressiveness: VAD aggressiveness (0-3)\n",
    "        - lowcut: Low cutoff frequency for bandpass filter\n",
    "        - highcut: High cutoff frequency for bandpass filter\n",
    "        \"\"\"\n",
    "        self.target_sr = target_sr\n",
    "        self.normalize_level = normalize_level\n",
    "        self.vad_aggressiveness = vad_aggressiveness\n",
    "        self.lowcut = lowcut\n",
    "        self.highcut = highcut\n",
    "        \n",
    "        # Check if VAD sample rate is valid\n",
    "        if self.target_sr not in (8000, 16000, 32000, 48000):\n",
    "            print(f\"Warning: VAD requires sample rate of 8000, 16000, 32000, or 48000 Hz\")\n",
    "            print(f\"VAD will be disabled if sample rate is not compatible\")\n",
    "    \n",
    "    def resample(self, audio, original_sr):\n",
    "        \"\"\"Resample audio to target sample rate\"\"\"\n",
    "        if original_sr == self.target_sr:\n",
    "            return audio\n",
    "        return librosa.resample(audio, orig_sr=original_sr, target_sr=self.target_sr)\n",
    "    \n",
    "    def normalize(self, audio):\n",
    "        \"\"\"Normalize audio to target level\"\"\"\n",
    "        # Find the maximum absolute amplitude\n",
    "        max_amplitude = np.max(np.abs(audio))\n",
    "        \n",
    "        # Calculate current peak in dB\n",
    "        current_dB = 20 * np.log10(max_amplitude) if max_amplitude > 0 else -80\n",
    "        \n",
    "        # Calculate the gain needed\n",
    "        gain_dB = self.normalize_level - current_dB\n",
    "        gain_linear = 10 ** (gain_dB / 20)\n",
    "        \n",
    "        # Apply gain\n",
    "        normalized = audio * gain_linear\n",
    "        \n",
    "        # Ensure no clipping\n",
    "        if np.max(np.abs(normalized)) > 1.0:\n",
    "            normalized = normalized / np.max(np.abs(normalized))\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def reduce_noise(self, audio, stationary=True):\n",
    "        \"\"\"Apply noise reduction\"\"\"\n",
    "        return nr.reduce_noise(y=audio, sr=self.target_sr, stationary=stationary)\n",
    "    \n",
    "    def detect_speech(self, audio, frame_duration=30):\n",
    "        \"\"\"\n",
    "        Detect speech segments using WebRTC VAD\n",
    "        Returns list of booleans (True = speech)\n",
    "        \"\"\"\n",
    "        # Initialize VAD\n",
    "        vad = webrtcvad.Vad(self.vad_aggressiveness)\n",
    "        \n",
    "        # WebRTC VAD requires specific sample rates\n",
    "        if self.target_sr not in (8000, 16000, 32000, 48000):\n",
    "            return [True] * int(len(audio) / (self.target_sr * frame_duration / 1000))\n",
    "        \n",
    "        # Scale to int16 range\n",
    "        audio_int16 = (audio * 32767).astype(np.int16)\n",
    "        \n",
    "        # Calculate frame size\n",
    "        frame_size = int(self.target_sr * frame_duration / 1000)\n",
    "        \n",
    "        # Process frames\n",
    "        speech_frames = []\n",
    "        for i in range(0, len(audio_int16) - frame_size, frame_size):\n",
    "            frame = audio_int16[i:i + frame_size]\n",
    "            frame_bytes = struct.pack(\"h\" * len(frame), *frame)\n",
    "            try:\n",
    "                is_speech = vad.is_speech(frame_bytes, self.target_sr)\n",
    "                speech_frames.append(is_speech)\n",
    "            except Exception as e:\n",
    "                print(f\"VAD error: {e}\")\n",
    "                speech_frames.append(True)  # Default to keeping the frame\n",
    "        \n",
    "        return speech_frames\n",
    "    \n",
    "    def apply_vad(self, audio, speech_frames, frame_duration=30):\n",
    "        \"\"\"Keep only detected speech segments\"\"\"\n",
    "        frame_size = int(self.target_sr * frame_duration / 1000)\n",
    "        speech_only = np.zeros_like(audio)\n",
    "        \n",
    "        for i, is_speech in enumerate(speech_frames):\n",
    "            if is_speech:\n",
    "                start = i * frame_size\n",
    "                end = min(start + frame_size, len(audio))\n",
    "                speech_only[start:end] = audio[start:end]\n",
    "        \n",
    "        return speech_only\n",
    "    \n",
    "    def apply_bandpass(self, audio, order=5):\n",
    "        \"\"\"Apply bandpass filter to focus on speech frequencies\"\"\"\n",
    "        nyquist = 0.5 * self.target_sr\n",
    "        low = self.lowcut / nyquist\n",
    "        high = self.highcut / nyquist\n",
    "        \n",
    "        # Design filter\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        \n",
    "        # Apply filter\n",
    "        return lfilter(b, a, audio)\n",
    "    \n",
    "    def process(self, audio_path, apply_noise_reduction=True, \n",
    "                apply_vad=True, apply_bandpass=True):\n",
    "        \"\"\"\n",
    "        Process audio file with complete pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        - audio_path: Path to audio file\n",
    "        - apply_noise_reduction: Whether to apply noise reduction\n",
    "        - apply_vad: Whether to apply voice activity detection\n",
    "        - apply_bandpass: Whether to apply bandpass filtering\n",
    "        \n",
    "        Returns:\n",
    "        - processed_audio: Processed audio data\n",
    "        - settings: Dictionary of settings used\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing: {audio_path}\")\n",
    "        \n",
    "        # Load audio\n",
    "        audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "        print(f\"Loaded audio: {len(audio)/original_sr:.1f}s, {original_sr}Hz\")\n",
    "        \n",
    "        # Track original audio for comparison\n",
    "        original_audio = audio.copy()\n",
    "        \n",
    "        # Step 1: Resample\n",
    "        if original_sr != self.target_sr:\n",
    "            audio = self.resample(audio, original_sr)\n",
    "            print(f\"Resampled from {original_sr}Hz to {self.target_sr}Hz\")\n",
    "        \n",
    "        # Step 2: Normalize\n",
    "        audio = self.normalize(audio)\n",
    "        print(f\"Normalized audio to {self.normalize_level}dB\")\n",
    "        \n",
    "        # Step 3: Noise reduction\n",
    "        if apply_noise_reduction:\n",
    "            audio = self.reduce_noise(audio)\n",
    "            print(\"Applied noise reduction\")\n",
    "        \n",
    "        # Step 4: Voice activity detection\n",
    "        if apply_vad:\n",
    "            speech_frames = self.detect_speech(audio)\n",
    "            speech_percentage = sum(speech_frames) / len(speech_frames) * 100\n",
    "            print(f\"VAD: Speech detected in {speech_percentage:.1f}% of frames\")\n",
    "            audio = self.apply_vad(audio, speech_frames)\n",
    "        \n",
    "        # Step 5: Bandpass filter\n",
    "        if apply_bandpass:\n",
    "            audio = self.apply_bandpass(audio)\n",
    "            print(f\"Applied bandpass filter: {self.lowcut}-{self.highcut}Hz\")\n",
    "        \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "        # Return both processed audio and settings used\n",
    "        settings = {\n",
    "            \"sample_rate\": self.target_sr,\n",
    "            \"normalize_level\": self.normalize_level,\n",
    "            \"vad_aggressiveness\": self.vad_aggressiveness,\n",
    "            \"bandpass_range\": [self.lowcut, self.highcut],\n",
    "            \"steps_applied\": {\n",
    "                \"resampling\": True,\n",
    "                \"normalization\": True,\n",
    "                \"noise_reduction\": apply_noise_reduction,\n",
    "                \"vad\": apply_vad,\n",
    "                \"bandpass\": apply_bandpass\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return audio, original_audio, settings\n",
    "    \n",
    "    def plot_comparison(self, original, processed, title=\"Audio Preprocessing Comparison\"):\n",
    "        \"\"\"Plot original vs processed audio waveforms\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Time axis\n",
    "        time_original = np.linspace(0, len(original)/self.target_sr, len(original))\n",
    "        time_processed = np.linspace(0, len(processed)/self.target_sr, len(processed))\n",
    "        \n",
    "        # Plot original audio\n",
    "        ax1.plot(time_original, original)\n",
    "        ax1.set_title(\"Original Audio\")\n",
    "        ax1.set_ylabel(\"Amplitude\")\n",
    "        ax1.set_xlim(0, len(original)/self.target_sr)\n",
    "        \n",
    "        # Plot processed audio\n",
    "        ax2.plot(time_processed, processed, color='orange')\n",
    "        ax2.set_title(\"Processed Audio\")\n",
    "        ax2.set_xlabel(\"Time (seconds)\")\n",
    "        ax2.set_ylabel(\"Amplitude\")\n",
    "        ax2.set_xlim(0, len(processed)/self.target_sr)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "        \n",
    "    def save_audio(self, audio, output_path):\n",
    "        \"\"\"Save processed audio to file\"\"\"\n",
    "        sf.write(output_path, audio, self.target_sr)\n",
    "        print(f\"Audio saved to {output_path}\")\n",
    "\n",
    "# Create an instance of the preprocessor\n",
    "preprocessor = AudioPreprocessor(\n",
    "    target_sr=16000,         # 16kHz is optimal for Vosk\n",
    "    normalize_level=-3,      # -3dB peak level\n",
    "    vad_aggressiveness=2,    # Medium aggressiveness\n",
    "    lowcut=300,              # Low cutoff frequency for speech\n",
    "    highcut=3000             # High cutoff frequency for speech\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81441ae",
   "metadata": {},
   "source": [
    "### Step 4: Test the Preprocessing Pipeline\n",
    "\n",
    "Now that we have our processor, let's test it with an audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d81fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your test audio file\n",
    "test_file = \"test_audio.wav\"  # Change this if you're using a different file\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(test_file):\n",
    "    print(f\"File {test_file} not found. Please record an audio file first.\")\n",
    "else:\n",
    "    # Process with all techniques\n",
    "    processed_audio, original_audio, settings = preprocessor.process(\n",
    "        test_file,\n",
    "        apply_noise_reduction=True,\n",
    "        apply_vad=True,\n",
    "        apply_bandpass=True\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    preprocessor.plot_comparison(original_audio, processed_audio, \"Full Preprocessing Pipeline\")\n",
    "    \n",
    "    # Save processed audio\n",
    "    processed_file = \"processed_\" + os.path.basename(test_file)\n",
    "    preprocessor.save_audio(processed_audio, processed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b263f",
   "metadata": {},
   "source": [
    "### Step 5: Testing with Vosk\n",
    "\n",
    "Now, let's test how our preprocessing affects speech recognition accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f833b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import wave\n",
    "import json\n",
    "\n",
    "def recognize_with_vosk(audio_file, model_path=\"models/vosk-model-small-en-us-0.15\"):\n",
    "    \"\"\"\n",
    "    Recognize speech in audio file using Vosk\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_file: Path to audio file\n",
    "    - model_path: Path to Vosk model\n",
    "    \n",
    "    Returns:\n",
    "    - text: Recognized text\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model not found at {model_path}. Please download a Vosk model first.\")\n",
    "        return \"\"\n",
    "        \n",
    "    if not os.path.exists(audio_file):\n",
    "        print(f\"Audio file not found: {audio_file}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Open the audio file\n",
    "    wf = wave.open(audio_file, \"rb\")\n",
    "    \n",
    "    # Check format\n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2:\n",
    "        print(\"Audio file must be mono PCM.\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = Model(model_path)\n",
    "    recognizer = KaldiRecognizer(model, wf.getframerate())\n",
    "    \n",
    "    # Process the audio\n",
    "    text = \"\"\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = json.loads(recognizer.Result())\n",
    "            text += result.get(\"text\", \"\") + \" \"\n",
    "    \n",
    "    # Get final result\n",
    "    final_result = json.loads(recognizer.FinalResult())\n",
    "    text += final_result.get(\"text\", \"\")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Check if we have a Vosk model\n",
    "model_path = \"models/vosk-model-small-en-us-0.15\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model not found at {model_path}. Please download a Vosk model first.\")\n",
    "    print(\"You can download models from: https://alphacephei.com/vosk/models\")\n",
    "else:\n",
    "    # Recognize original audio\n",
    "    print(\"\\n--- Recognition Results ---\")\n",
    "    print(\"\\nOriginal Audio:\")\n",
    "    original_text = recognize_with_vosk(test_file, model_path)\n",
    "    print(f'\"{original_text}\"')\n",
    "    \n",
    "    # Recognize processed audio\n",
    "    print(\"\\nProcessed Audio:\")\n",
    "    processed_text = recognize_with_vosk(processed_file, model_path)\n",
    "    print(f'\"{processed_text}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e1c3b",
   "metadata": {},
   "source": [
    "### Step 6: Experimentation\n",
    "\n",
    "Now, let's experiment with different preprocessing settings to see how they affect recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_settings(audio_file, model_path, output_file=\"experiment_results.txt\"):\n",
    "    \"\"\"Run experiments with different preprocessing settings\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Original audio (baseline)\n",
    "    original_text = recognize_with_vosk(audio_file, model_path)\n",
    "    results.append({\n",
    "        \"experiment\": \"Baseline (No preprocessing)\",\n",
    "        \"settings\": {},\n",
    "        \"recognized_text\": original_text\n",
    "    })\n",
    "    \n",
    "    # Experiment 1: Only normalization\n",
    "    print(\"\\nExperiment 1: Only normalization\")\n",
    "    audio1, _, settings1 = preprocessor.process(\n",
    "        audio_file, \n",
    "        apply_noise_reduction=False,\n",
    "        apply_vad=False, \n",
    "        apply_bandpass=False\n",
    "    )\n",
    "    output1 = \"exp1_norm_only.wav\"\n",
    "    preprocessor.save_audio(audio1, output1)\n",
    "    text1 = recognize_with_vosk(output1, model_path)\n",
    "    results.append({\n",
    "        \"experiment\": \"Experiment 1: Only normalization\",\n",
    "        \"settings\": settings1,\n",
    "        \"recognized_text\": text1\n",
    "    })\n",
    "    \n",
    "    # Experiment 2: Normalization + Noise Reduction\n",
    "    print(\"\\nExperiment 2: Normalization + Noise Reduction\")\n",
    "    audio2, _, settings2 = preprocessor.process(\n",
    "        audio_file, \n",
    "        apply_noise_reduction=True,\n",
    "        apply_vad=False, \n",
    "        apply_bandpass=False\n",
    "    )\n",
    "    output2 = \"exp2_norm_noise.wav\"\n",
    "    preprocessor.save_audio(audio2, output2)\n",
    "    text2 = recognize_with_vosk(output2, model_path)\n",
    "    results.append({\n",
    "        \"experiment\": \"Experiment 2: Normalization + Noise Reduction\",\n",
    "        \"settings\": settings2,\n",
    "        \"recognized_text\": text2\n",
    "    })\n",
    "    \n",
    "    # Experiment 3: Full Pipeline\n",
    "    print(\"\\nExperiment 3: Full Pipeline\")\n",
    "    audio3, _, settings3 = preprocessor.process(\n",
    "        audio_file, \n",
    "        apply_noise_reduction=True,\n",
    "        apply_vad=True, \n",
    "        apply_bandpass=True\n",
    "    )\n",
    "    output3 = \"exp3_full.wav\"\n",
    "    preprocessor.save_audio(audio3, output3)\n",
    "    text3 = recognize_with_vosk(output3, model_path)\n",
    "    results.append({\n",
    "        \"experiment\": \"Experiment 3: Full Pipeline\",\n",
    "        \"settings\": settings3,\n",
    "        \"recognized_text\": text3\n",
    "    })\n",
    "    \n",
    "    # Custom experiment: Noise reduction + bandpass (no VAD)\n",
    "    print(\"\\nExperiment 4: Noise reduction + Bandpass\")\n",
    "    audio4, _, settings4 = preprocessor.process(\n",
    "        audio_file, \n",
    "        apply_noise_reduction=True,\n",
    "        apply_vad=False, \n",
    "        apply_bandpass=True\n",
    "    )\n",
    "    output4 = \"exp4_noise_bandpass.wav\"\n",
    "    preprocessor.save_audio(audio4, output4)\n",
    "    text4 = recognize_with_vosk(output4, model_path)\n",
    "    results.append({\n",
    "        \"experiment\": \"Experiment 4: Noise reduction + Bandpass\",\n",
    "        \"settings\": settings4,\n",
    "        \"recognized_text\": text4\n",
    "    })\n",
    "    \n",
    "    # Save experiment results\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for res in results:\n",
    "            f.write(f\"=== {res['experiment']} ===\\n\")\n",
    "            f.write(f\"Recognized text: \\\"{res['recognized_text']}\\\"\\n\\n\")\n",
    "    \n",
    "    print(f\"\\nExperiment results saved to {output_file}\")\n",
    "    return results\n",
    "\n",
    "# Run experiments if Vosk model exists\n",
    "if os.path.exists(model_path) and os.path.exists(test_file):\n",
    "    experiment_results = experiment_with_settings(test_file, model_path)\n",
    "    \n",
    "    # Display results comparison\n",
    "    print(\"\\n=== EXPERIMENT RESULTS COMPARISON ===\")\n",
    "    for res in experiment_results:\n",
    "        print(f\"\\n{res['experiment']}:\")\n",
    "        print(f'\"{res[\"recognized_text\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637da67",
   "metadata": {},
   "source": [
    "### Step 7: Create a Reusable Module\n",
    "\n",
    "Now, let's create a reusable module that you can import into your voice assistant project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the AudioPreprocessor class as a Python module\n",
    "module_code = '''\"\"\"\n",
    "audio_preprocessor.py - A comprehensive audio preprocessing module for speech recognition\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "import webrtcvad\n",
    "import struct\n",
    "import os\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"A complete audio preprocessing pipeline for speech recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_sr=16000, normalize_level=-3, \n",
    "                 vad_aggressiveness=3, lowcut=300, highcut=3000):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor\n",
    "        \n",
    "        Parameters:\n",
    "        - target_sr: Target sample rate in Hz\n",
    "        - normalize_level: Target peak normalization level in dB\n",
    "        - vad_aggressiveness: VAD aggressiveness (0-3)\n",
    "        - lowcut: Low cutoff frequency for bandpass filter\n",
    "        - highcut: High cutoff frequency for bandpass filter\n",
    "        \"\"\"\n",
    "        self.target_sr = target_sr\n",
    "        self.normalize_level = normalize_level\n",
    "        self.vad_aggressiveness = vad_aggressiveness\n",
    "        self.lowcut = lowcut\n",
    "        self.highcut = highcut\n",
    "        \n",
    "        # Check if VAD sample rate is valid\n",
    "        if self.target_sr not in (8000, 16000, 32000, 48000):\n",
    "            print(f\"Warning: VAD requires sample rate of 8000, 16000, 32000, or 48000 Hz\")\n",
    "            print(f\"VAD will be disabled if sample rate is not compatible\")\n",
    "    \n",
    "    def resample(self, audio, original_sr):\n",
    "        \"\"\"Resample audio to target sample rate\"\"\"\n",
    "        if original_sr == self.target_sr:\n",
    "            return audio\n",
    "        return librosa.resample(audio, orig_sr=original_sr, target_sr=self.target_sr)\n",
    "    \n",
    "    def normalize(self, audio):\n",
    "        \"\"\"Normalize audio to target level\"\"\"\n",
    "        # Find the maximum absolute amplitude\n",
    "        max_amplitude = np.max(np.abs(audio))\n",
    "        \n",
    "        # Calculate current peak in dB\n",
    "        current_dB = 20 * np.log10(max_amplitude) if max_amplitude > 0 else -80\n",
    "        \n",
    "        # Calculate the gain needed\n",
    "        gain_dB = self.normalize_level - current_dB\n",
    "        gain_linear = 10 ** (gain_dB / 20)\n",
    "        \n",
    "        # Apply gain\n",
    "        normalized = audio * gain_linear\n",
    "        \n",
    "        # Ensure no clipping\n",
    "        if np.max(np.abs(normalized)) > 1.0:\n",
    "            normalized = normalized / np.max(np.abs(normalized))\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def reduce_noise(self, audio, stationary=True):\n",
    "        \"\"\"Apply noise reduction\"\"\"\n",
    "        return nr.reduce_noise(y=audio, sr=self.target_sr, stationary=stationary)\n",
    "    \n",
    "    def detect_speech(self, audio, frame_duration=30):\n",
    "        \"\"\"\n",
    "        Detect speech segments using WebRTC VAD\n",
    "        Returns list of booleans (True = speech)\n",
    "        \"\"\"\n",
    "        # Initialize VAD\n",
    "        vad = webrtcvad.Vad(self.vad_aggressiveness)\n",
    "        \n",
    "        # WebRTC VAD requires specific sample rates\n",
    "        if self.target_sr not in (8000, 16000, 32000, 48000):\n",
    "            return [True] * int(len(audio) / (self.target_sr * frame_duration / 1000))\n",
    "        \n",
    "        # Scale to int16 range\n",
    "        audio_int16 = (audio * 32767).astype(np.int16)\n",
    "        \n",
    "        # Calculate frame size\n",
    "        frame_size = int(self.target_sr * frame_duration / 1000)\n",
    "        \n",
    "        # Process frames\n",
    "        speech_frames = []\n",
    "        for i in range(0, len(audio_int16) - frame_size, frame_size):\n",
    "            frame = audio_int16[i:i + frame_size]\n",
    "            frame_bytes = struct.pack(\"h\" * len(frame), *frame)\n",
    "            try:\n",
    "                is_speech = vad.is_speech(frame_bytes, self.target_sr)\n",
    "                speech_frames.append(is_speech)\n",
    "            except Exception as e:\n",
    "                print(f\"VAD error: {e}\")\n",
    "                speech_frames.append(True)  # Default to keeping the frame\n",
    "        \n",
    "        return speech_frames\n",
    "    \n",
    "    def apply_vad(self, audio, speech_frames, frame_duration=30):\n",
    "        \"\"\"Keep only detected speech segments\"\"\"\n",
    "        frame_size = int(self.target_sr * frame_duration / 1000)\n",
    "        speech_only = np.zeros_like(audio)\n",
    "        \n",
    "        for i, is_speech in enumerate(speech_frames):\n",
    "            if is_speech:\n",
    "                start = i * frame_size\n",
    "                end = min(start + frame_size, len(audio))\n",
    "                speech_only[start:end] = audio[start:end]\n",
    "        \n",
    "        return speech_only\n",
    "    \n",
    "    def apply_bandpass(self, audio, order=5):\n",
    "        \"\"\"Apply bandpass filter to focus on speech frequencies\"\"\"\n",
    "        nyquist = 0.5 * self.target_sr\n",
    "        low = self.lowcut / nyquist\n",
    "        high = self.highcut / nyquist\n",
    "        \n",
    "        # Design filter\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        \n",
    "        # Apply filter\n",
    "        return lfilter(b, a, audio)\n",
    "    \n",
    "    def process(self, audio_path, apply_noise_reduction=True, \n",
    "                apply_vad=True, apply_bandpass=True):\n",
    "        \"\"\"\n",
    "        Process audio file with complete pipeline\n",
    "        \n",
    "        Parameters:\n",
    "        - audio_path: Path to audio file\n",
    "        - apply_noise_reduction: Whether to apply noise reduction\n",
    "        - apply_vad: Whether to apply voice activity detection\n",
    "        - apply_bandpass: Whether to apply bandpass filtering\n",
    "        \n",
    "        Returns:\n",
    "        - processed_audio: Processed audio data\n",
    "        - original_audio: Original audio data\n",
    "        - settings: Dictionary of settings used\n",
    "        \"\"\"\n",
    "        print(f\"Processing: {audio_path}\")\n",
    "        \n",
    "        # Load audio\n",
    "        audio, original_sr = librosa.load(audio_path, sr=None)\n",
    "        print(f\"Loaded audio: {len(audio)/original_sr:.1f}s, {original_sr}Hz\")\n",
    "        \n",
    "        # Track original audio for comparison\n",
    "        original_audio = audio.copy()\n",
    "        \n",
    "        # Step 1: Resample\n",
    "        if original_sr != self.target_sr:\n",
    "            audio = self.resample(audio, original_sr)\n",
    "            print(f\"Resampled from {original_sr}Hz to {self.target_sr}Hz\")\n",
    "        \n",
    "        # Step 2: Normalize\n",
    "        audio = self.normalize(audio)\n",
    "        print(f\"Normalized audio to {self.normalize_level}dB\")\n",
    "        \n",
    "        # Step 3: Noise reduction\n",
    "        if apply_noise_reduction:\n",
    "            audio = self.reduce_noise(audio)\n",
    "            print(\"Applied noise reduction\")\n",
    "        \n",
    "        # Step 4: Voice activity detection\n",
    "        if apply_vad:\n",
    "            speech_frames = self.detect_speech(audio)\n",
    "            speech_percentage = sum(speech_frames) / len(speech_frames) * 100\n",
    "            print(f\"VAD: Speech detected in {speech_percentage:.1f}% of frames\")\n",
    "            audio = self.apply_vad(audio, speech_frames)\n",
    "        \n",
    "        # Step 5: Bandpass filter\n",
    "        if apply_bandpass:\n",
    "            audio = self.apply_bandpass(audio)\n",
    "            print(f\"Applied bandpass filter: {self.lowcut}-{self.highcut}Hz\")\n",
    "        \n",
    "        print(\"Processing complete!\")\n",
    "        \n",
    "        # Return both processed audio and settings used\n",
    "        settings = {\n",
    "            \"sample_rate\": self.target_sr,\n",
    "            \"normalize_level\": self.normalize_level,\n",
    "            \"vad_aggressiveness\": self.vad_aggressiveness,\n",
    "            \"bandpass_range\": [self.lowcut, self.highcut],\n",
    "            \"steps_applied\": {\n",
    "                \"resampling\": True,\n",
    "                \"normalization\": True,\n",
    "                \"noise_reduction\": apply_noise_reduction,\n",
    "                \"vad\": apply_vad,\n",
    "                \"bandpass\": apply_bandpass\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return audio, original_audio, settings\n",
    "    \n",
    "    def process_stream(self, audio_data, sr, apply_noise_reduction=True, \n",
    "                       apply_vad=False, apply_bandpass=True):\n",
    "        \"\"\"\n",
    "        Process audio data from a stream (e.g., microphone)\n",
    "        \n",
    "        Parameters:\n",
    "        - audio_data: Numpy array of audio data\n",
    "        - sr: Sample rate of the audio data\n",
    "        - apply_noise_reduction: Whether to apply noise reduction\n",
    "        - apply_vad: Whether to apply voice activity detection\n",
    "        - apply_bandpass: Whether to apply bandpass filtering\n",
    "        \n",
    "        Returns:\n",
    "        - processed_audio: Processed audio data\n",
    "        \"\"\"\n",
    "        # Copy original audio\n",
    "        audio = audio_data.copy()\n",
    "        \n",
    "        # Step 1: Resample if needed\n",
    "        if sr != self.target_sr:\n",
    "            audio = self.resample(audio, sr)\n",
    "        \n",
    "        # Step 2: Normalize\n",
    "        audio = self.normalize(audio)\n",
    "        \n",
    "        # Step 3: Noise reduction\n",
    "        if apply_noise_reduction:\n",
    "            audio = self.reduce_noise(audio)\n",
    "        \n",
    "        # Step 4: Voice activity detection\n",
    "        if apply_vad:\n",
    "            speech_frames = self.detect_speech(audio)\n",
    "            audio = self.apply_vad(audio, speech_frames)\n",
    "        \n",
    "        # Step 5: Bandpass filter\n",
    "        if apply_bandpass:\n",
    "            audio = self.apply_bandpass(audio)\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def plot_comparison(self, original, processed, title=\"Audio Preprocessing Comparison\"):\n",
    "        \"\"\"Plot original vs processed audio waveforms\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Time axis\n",
    "        time_original = np.linspace(0, len(original)/self.target_sr, len(original))\n",
    "        time_processed = np.linspace(0, len(processed)/self.target_sr, len(processed))\n",
    "        \n",
    "        # Plot original audio\n",
    "        ax1.plot(time_original, original)\n",
    "        ax1.set_title(\"Original Audio\")\n",
    "        ax1.set_ylabel(\"Amplitude\")\n",
    "        ax1.set_xlim(0, len(original)/self.target_sr)\n",
    "        \n",
    "        # Plot processed audio\n",
    "        ax2.plot(time_processed, processed, color='orange')\n",
    "        ax2.set_title(\"Processed Audio\")\n",
    "        ax2.set_xlabel(\"Time (seconds)\")\n",
    "        ax2.set_ylabel(\"Amplitude\")\n",
    "        ax2.set_xlim(0, len(processed)/self.target_sr)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "        \n",
    "    def save_audio(self, audio, output_path):\n",
    "        \"\"\"Save processed audio to file\"\"\"\n",
    "        sf.write(output_path, audio, self.target_sr)\n",
    "        print(f\"Audio saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create preprocessor\n",
    "    preprocessor = AudioPreprocessor(\n",
    "        target_sr=16000,\n",
    "        normalize_level=-3,\n",
    "        vad_aggressiveness=2,\n",
    "        lowcut=300,\n",
    "        highcut=3000\n",
    "    )\n",
    "    \n",
    "    # Process a file if provided as an argument\n",
    "    import sys\n",
    "    if len(sys.argv) > 1:\n",
    "        input_file = sys.argv[1]\n",
    "        output_file = \"processed_\" + os.path.basename(input_file)\n",
    "        \n",
    "        processed, original, _ = preprocessor.process(input_file)\n",
    "        preprocessor.save_audio(processed, output_file)\n",
    "        \n",
    "        # If matplotlib is available, show a comparison\n",
    "        try:\n",
    "            preprocessor.plot_comparison(original, processed)\n",
    "        except:\n",
    "            pass\n",
    "'''\n",
    "\n",
    "# Write the module to a file\n",
    "with open('audio_preprocessor.py', 'w') as f:\n",
    "    f.write(module_code)\n",
    "\n",
    "print(\"Created audio_preprocessor.py module!\")\n",
    "print(\"You can now import this module in your projects:\")\n",
    "print(\"from audio_preprocessor import AudioPreprocessor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb48ff",
   "metadata": {},
   "source": [
    "### Step 8: Integration with Real-time Audio\n",
    "\n",
    "In a real voice assistant application, you'll often need to process audio in real-time. Let's create a simple example of real-time audio preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ad784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realtime_example():\n",
    "    \"\"\"Create a real-time audio processing example file\"\"\"\n",
    "    \n",
    "    code = '''\"\"\"\n",
    "real_time_example.py - Example of real-time audio preprocessing for speech recognition\n",
    "\"\"\"\n",
    "\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import time\n",
    "from audio_preprocessor import AudioPreprocessor\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = AudioPreprocessor(\n",
    "    target_sr=16000,\n",
    "    normalize_level=-3,\n",
    "    vad_aggressiveness=1,  # Lower for real-time\n",
    "    lowcut=300,\n",
    "    highcut=3000\n",
    ")\n",
    "\n",
    "# Initialize Vosk recognizer\n",
    "def init_recognizer(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model not found at {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    model = Model(model_path)\n",
    "    recognizer = KaldiRecognizer(model, RATE)\n",
    "    return recognizer\n",
    "\n",
    "def process_audio_stream():\n",
    "    # Initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "    \n",
    "    # Set up the audio stream\n",
    "    stream = p.open(format=FORMAT,\n",
    "                   channels=CHANNELS,\n",
    "                   rate=RATE,\n",
    "                   input=True,\n",
    "                   frames_per_buffer=CHUNK)\n",
    "    \n",
    "    # Initialize Vosk\n",
    "    model_path = \"models/vosk-model-small-en-us-0.15\"  # Change this if needed\n",
    "    recognizer = init_recognizer(model_path)\n",
    "    \n",
    "    if not recognizer:\n",
    "        print(\"Failed to initialize speech recognition. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Ready to process audio! Say something...\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Collect audio for processing\n",
    "            frames = []\n",
    "            for _ in range(0, int(RATE / CHUNK * 2)):  # 2 seconds of audio\n",
    "                data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "                frames.append(data)\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "            audio_data = audio_data.astype(np.float32) / 32767.0  # Convert to float\n",
    "            \n",
    "            print(\"\\\\nProcessing audio chunk...\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            start_time = time.time()\n",
    "            processed_audio = preprocessor.process_stream(\n",
    "                audio_data, \n",
    "                RATE, \n",
    "                apply_noise_reduction=True,\n",
    "                apply_vad=False,  # VAD can be tricky in real-time\n",
    "                apply_bandpass=True\n",
    "            )\n",
    "            \n",
    "            # Convert back to int16\n",
    "            processed_int16 = (processed_audio * 32767).astype(np.int16)\n",
    "            processed_bytes = processed_int16.tobytes()\n",
    "            \n",
    "            # Feed to recognizer\n",
    "            if recognizer.AcceptWaveform(processed_bytes):\n",
    "                result = json.loads(recognizer.Result())\n",
    "                text = result.get(\"text\", \"\")\n",
    "                if text:\n",
    "                    print(f\"Recognized: {text}\")\n",
    "            else:\n",
    "                partial = json.loads(recognizer.PartialResult())\n",
    "                partial_text = partial.get(\"partial\", \"\")\n",
    "                if partial_text:\n",
    "                    print(f\"Partial: {partial_text}\")\n",
    "            \n",
    "            proc_time = time.time() - start_time\n",
    "            print(f\"Processing time: {proc_time:.3f}s\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\\\nStopping...\")\n",
    "    finally:\n",
    "        # Clean up\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        print(\"Audio processing stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_audio_stream()\n",
    "'''\n",
    "    \n",
    "    with open('real_time_example.py', 'w') as f:\n",
    "        f.write(code)\n",
    "    \n",
    "    print(\"Created real_time_example.py!\")\n",
    "    print(\"You can run this example to test real-time audio preprocessing.\")\n",
    "\n",
    "create_realtime_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b4208",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this practice exercise, you've built and tested a complete audio preprocessing pipeline for speech recognition. You've learned how to:\n",
    "\n",
    "1. **Resample audio** to the proper sample rate for Vosk\n",
    "2. **Normalize audio** to consistent volume levels\n",
    "3. **Reduce background noise** in recordings\n",
    "4. **Apply Voice Activity Detection** to identify speech segments\n",
    "5. **Filter audio** to focus on speech frequencies\n",
    "6. **Integrate preprocessing** with Vosk for improved recognition\n",
    "\n",
    "You now have a reusable `AudioPreprocessor` class that you can integrate into your voice assistant project. In the next module, we'll build on these skills to create a more robust speech-to-text system that can handle continuous real-time audio.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To further improve your preprocessing skills:\n",
    "\n",
    "1. Experiment with different VAD aggressiveness settings\n",
    "2. Try different filter bandwidths\n",
    "3. Test with various noise environments\n",
    "4. Compare stationary vs. non-stationary noise reduction\n",
    "5. Integrate with your own voice assistant project"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
