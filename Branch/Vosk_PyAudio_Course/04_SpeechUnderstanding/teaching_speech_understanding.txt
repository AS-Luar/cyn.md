# SPEECH UNDERSTANDING

## GLOSSARY

- **NLU (Natural Language Understanding)**: A branch of AI that focuses on machine reading comprehension and understanding human language
- **Intent Recognition**: The process of identifying the purpose or goal behind a user's spoken command
- **Entity Extraction**: Identifying and extracting specific pieces of information from spoken text
- **Slot Filling**: The process of extracting parameters from a user's utterance to complete a command
- **Context Management**: Maintaining conversational context across multiple interactions
- **Dialog Management**: The system that determines the appropriate response based on user input and context
- **Named Entity Recognition (NER)**: Identifying proper nouns and other named entities in text
- **Part-of-Speech (POS) Tagging**: Identifying the grammatical parts of speech in text
- **Regular Expressions (Regex)**: A sequence of characters that define a search pattern
- **Rule-based Systems**: Systems that use manually defined rules for decision making

## CONCEPT INTERACTIONS

- **Building on Speech-to-Text**: We'll take the text output from Vosk and extract meaning from it
- **Building on Preprocessing**: Proper audio preprocessing improves speech recognition, which in turn improves understanding
- **Looking Forward**: The understanding capabilities will be integrated with actions your assistant can perform

## MAIN CONTENT

### Moving from Recognition to Understanding

Speech recognition (converting audio to text) is only the first step in building a voice assistant. To be useful, your assistant needs to understand what the user wants and take appropriate action. This understanding process involves:

1. Detecting user intent (what they want to do)
2. Extracting relevant information (parameters needed to fulfill the intent)
3. Making decisions based on the intent and parameters

For example, when a user says "Turn on the living room lights," your system needs to:
- Recognize the intent is to control a device
- Extract the action ("turn on") and the target device ("living room lights")
- Execute the appropriate function with these parameters

### Intent Recognition Techniques

There are several approaches to intent recognition, ranging from simple to complex:

#### 1. Rule-based/Keyword Matching

The simplest approach is to look for specific keywords or patterns in the recognized text:

```python
def detect_intent(text):
    text = text.lower()
    
    # Intent detection rules
    if "weather" in text:
        return "weather_inquiry"
    elif "time" in text:
        return "time_inquiry"
    elif "turn on" in text or "switch on" in text:
        return "device_control_on"
    elif "turn off" in text or "switch off" in text:
        return "device_control_off"
    elif "play" in text and ("music" in text or "song" in text):
        return "music_playback"
    else:
        return "unknown_intent"
```

**Advantages**:
- Simple to implement
- No training data required
- Predictable behavior

**Disadvantages**:
- Inflexible
- Can't handle variations in phrasing
- Requires maintaining complex rule sets for sophisticated applications

#### 2. Regular Expression Patterns

Regular expressions provide more flexibility than simple keyword matching:

```python
import re

def detect_intent_with_regex(text):
    text = text.lower()
    
    # Weather intent
    weather_pattern = r"(what|how)('s| is) (the )?weather( like)?( in (?P<location>\w+))?"
    weather_match = re.search(weather_pattern, text)
    if weather_match:
        location = weather_match.group("location") if weather_match.group("location") else "current location"
        return "weather_inquiry", {"location": location}
    
    # Time intent
    time_pattern = r"what('s| is) (the )?time( now)?"
    if re.search(time_pattern, text):
        return "time_inquiry", {}
    
    # Device control intent
    device_pattern = r"(turn|switch) (?P<action>on|off) (the )?(?P<device>[\w\s]+)( please)?"
    device_match = re.search(device_pattern, text)
    if device_match:
        return "device_control", {
            "action": device_match.group("action"),
            "device": device_match.group("device").strip()
        }
    
    # Default
    return "unknown_intent", {}
```

**Advantages**:
- More flexible than simple keywords
- Can extract parameters (entities) using capture groups
- Still relatively simple to implement

**Disadvantages**:
- Regular expressions can become complex and hard to maintain
- Limited ability to handle diverse phrasings
- Requires careful crafting to avoid false positives

#### 3. Machine Learning-based Intent Recognition

For more sophisticated applications, machine learning models can classify text into intents:

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Create a simple pipeline with CountVectorizer and Naive Bayes classifier
intent_classifier = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('classifier', MultinomialNB())
])

# Example training data
training_data = [
    ("what's the weather like today", "weather_inquiry"),
    ("how's the weather", "weather_inquiry"),
    ("weather forecast", "weather_inquiry"),
    ("what time is it", "time_inquiry"),
    ("tell me the time", "time_inquiry"),
    ("current time please", "time_inquiry"),
    ("turn on the kitchen lights", "device_control"),
    ("switch off the TV", "device_control"),
    ("turn the heating up", "device_control"),
    # Add more examples...
]

# Split training data into texts and labels
texts, labels = zip(*training_data)

# Train the classifier
intent_classifier.fit(texts, labels)

# Predict intent
def predict_intent(text):
    intent = intent_classifier.predict([text])[0]
    return intent
```

**Advantages**:
- More flexible, can handle variations in phrasing
- Improves with more training data
- Can generalize to unseen inputs

**Disadvantages**:
- Requires training data
- May produce unexpected results
- More complex to implement

### Entity Extraction and Slot Filling

After identifying the intent, you need to extract specific parameters (entities) from the user's speech. This process is often called "slot filling" because you're filling in slots in a command template with specific values.

#### Rule-based Approach

A simple approach uses regular expressions with named capture groups:

```python
def extract_entities(text, intent):
    text = text.lower()
    
    entities = {}
    
    if intent == "weather_inquiry":
        # Extract location if present
        location_match = re.search(r"weather (in|at|for) (?P<location>[\w\s]+)", text)
        if location_match:
            entities["location"] = location_match.group("location").strip()
        else:
            entities["location"] = "current location"
    
    elif intent == "device_control":
        # Extract device and action
        device_match = re.search(r"(turn|switch) (?P<action>on|off) (the )?(?P<device>[\w\s]+)", text)
        if device_match:
            entities["action"] = device_match.group("action")
            entities["device"] = device_match.group("device").strip()
    
    return entities
```

#### Named Entity Recognition

For more complex entity extraction, you can use NER libraries:

```python
import spacy

# Load a pre-trained NER model
nlp = spacy.load("en_core_web_sm")

def extract_entities_with_spacy(text, intent):
    # Process the text with spaCy
    doc = nlp(text)
    
    entities = {}
    
    # Extract entities based on intent
    if intent == "weather_inquiry":
        # Look for location entities
        for ent in doc.ents:
            if ent.label_ in ["GPE", "LOC"]:  # Geopolitical entity or location
                entities["location"] = ent.text
                break
        
        if "location" not in entities:
            entities["location"] = "current location"
    
    elif intent == "device_control":
        # Extract action (on/off)
        if "turn on" in text.lower() or "switch on" in text.lower():
            entities["action"] = "on"
        elif "turn off" in text.lower() or "switch off" in text.lower():
            entities["action"] = "off"
        
        # Try to identify the device
        # This is simplified - in a real system you might have a list of known devices
        if "action" in entities:
            action_phrase = "turn " + entities["action"] or "switch " + entities["action"]
            device_text = text.lower().split(action_phrase)[1].strip()
            if device_text.startswith("the "):
                device_text = device_text[4:]
            entities["device"] = device_text
    
    return entities
```

### Context Management

Real conversations often span multiple interactions. Context management allows your assistant to remember previous interactions and maintain a coherent conversation. Here's a simple context manager:

```python
class ContextManager:
    def __init__(self):
        self.context = {}
        self.last_intent = None
        self.session_start = time.time()
        self.session_timeout = 300  # 5 minutes
    
    def update_context(self, intent, entities):
        """Update the context with new information."""
        # Reset if session has timed out
        if time.time() - self.session_start > self.session_timeout:
            self.reset_context()
        
        # Update session timestamp
        self.session_start = time.time()
        
        # Save the latest intent
        self.last_intent = intent
        
        # Update context with entities
        if entities:
            self.context.update(entities)
    
    def get_context_value(self, key, default=None):
        """Get a value from the context."""
        return self.context.get(key, default)
    
    def reset_context(self):
        """Reset the conversation context."""
        self.context = {}
        self.last_intent = None
    
    def get_state(self):
        """Get the current context state."""
        return {
            "context": self.context,
            "last_intent": self.last_intent,
            "session_age": time.time() - self.session_start
        }
```

Using the context manager allows handling conversations like:

User: "What's the weather like in New York?"
Assistant: "It's sunny and 75°F in New York."
User: "How about tomorrow?"
Assistant: "Tomorrow will be partly cloudy with a high of 78°F in New York."

The second query doesn't specify a location, but the context manager remembers "New York" from the previous interaction.

### Dialog Management

A dialog manager determines how to respond to user intents in the context of the conversation. Here's a simple implementation:

```python
class DialogManager:
    def __init__(self):
        self.context_manager = ContextManager()
    
    def process_input(self, text):
        """Process user input and determine the appropriate response."""
        # Detect intent
        intent, entities = detect_intent_with_regex(text)
        
        # Update context
        self.context_manager.update_context(intent, entities)
        
        # Generate response based on intent and context
        response = self.generate_response(intent, entities)
        
        return response
    
    def generate_response(self, intent, entities):
        """Generate a response based on intent and entities."""
        if intent == "weather_inquiry":
            location = entities.get("location") or self.context_manager.get_context_value("location", "current location")
            return f"The weather in {location} is currently sunny and 72°F."
        
        elif intent == "time_inquiry":
            current_time = time.strftime("%I:%M %p")
            return f"The current time is {current_time}."
        
        elif intent == "device_control":
            device = entities.get("device", "unknown device")
            action = entities.get("action", "unknown action")
            return f"I'll {action} the {device} for you."
        
        else:
            return "I'm not sure how to help with that."
```

### Putting It All Together

Here's how to integrate intent recognition and entity extraction with the speech recognition from the previous module:

```python
from vosk import Model, KaldiRecognizer
import pyaudio
import json
import time
import re

# Initialize speech recognition
model = Model("path/to/model")
recognizer = KaldiRecognizer(model, 16000)

# Initialize dialog manager
dialog_manager = DialogManager()

# Audio setup
p = pyaudio.PyAudio()
stream = p.open(
    format=pyaudio.paInt16,
    channels=1,
    rate=16000,
    input=True,
    frames_per_buffer=8000
)

print("Listening... (Press Ctrl+C to stop)")

try:
    while True:
        data = stream.read(4000, exception_on_overflow=False)
        
        if recognizer.AcceptWaveform(data):
            result = json.loads(recognizer.Result())
            text = result.get("text", "").strip()
            
            if text:
                print(f"Recognized: {text}")
                
                # Process input and get response
                response = dialog_manager.process_input(text)
                print(f"Assistant: {response}")
        
        # Optional: Show partial results
        partial = json.loads(recognizer.PartialResult())
        partial_text = partial.get("partial", "")
        if partial_text:
            print(f"Listening: {partial_text}", end="\r")
        
        time.sleep(0.01)
        
except KeyboardInterrupt:
    print("\nStopping...")

# Clean up
stream.stop_stream()
stream.close()
p.terminate()
```

### Advanced Techniques

For more sophisticated understanding, consider these advanced techniques:

#### 1. Custom NLU with Word Embeddings

Word embeddings represent words as vectors in a semantic space, where similar words are closer together. This allows for more flexible matching:

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Simplified example with word embeddings
# In practice, you would load pre-trained embeddings
word_embeddings = {
    "weather": np.array([0.1, 0.2, 0.3]),
    "forecast": np.array([0.15, 0.25, 0.35]),
    "temperature": np.array([0.12, 0.22, 0.32]),
    "time": np.array([0.5, 0.6, 0.7]),
    "clock": np.array([0.55, 0.65, 0.75]),
    "hour": np.array([0.52, 0.62, 0.72]),
    # etc.
}

# Define intent keywords
weather_keywords = ["weather", "forecast", "temperature"]
time_keywords = ["time", "clock", "hour"]

def get_embedding(text):
    """Get the embedding for a text by averaging word embeddings."""
    words = text.lower().split()
    embeddings = [word_embeddings[word] for word in words if word in word_embeddings]
    if not embeddings:
        return np.zeros(3)  # Same dimension as our embeddings
    return np.mean(embeddings, axis=0)

def semantic_intent_detection(text):
    """Detect intent using semantic similarity."""
    text_embedding = get_embedding(text)
    
    # Calculate similarity with weather keywords
    weather_embeddings = [word_embeddings[word] for word in weather_keywords]
    weather_similarity = max(cosine_similarity([text_embedding], [emb])[0][0] for emb in weather_embeddings)
    
    # Calculate similarity with time keywords
    time_embeddings = [word_embeddings[word] for word in time_keywords]
    time_similarity = max(cosine_similarity([text_embedding], [emb])[0][0] for emb in time_embeddings)
    
    # Return the intent with highest similarity
    if weather_similarity > time_similarity:
        return "weather_inquiry" if weather_similarity > 0.5 else "unknown_intent"
    else:
        return "time_inquiry" if time_similarity > 0.5 else "unknown_intent"
```

#### 2. Intent Classification with Deep Learning

For more complex applications, deep learning models like BERT can provide state-of-the-art intent classification:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load pre-trained model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

# Map indices to intent labels
intent_labels = {
    0: "weather_inquiry",
    1: "time_inquiry",
    2: "device_control"
}

def predict_intent_with_bert(text):
    # Tokenize input
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    
    # Get model outputs
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Get predicted class
    predicted_class = torch.argmax(outputs.logits, dim=1).item()
    
    # Return mapped intent
    return intent_labels[predicted_class]
```

**Note**: This is a simplified example. In practice, you would fine-tune BERT on your specific intents.

### Best Practices

1. **Start Simple**: Begin with rule-based approaches and add complexity as needed
2. **Focus on Common Intents**: Cover the most frequent user requests first
3. **Handle Ambiguity**: Implement confirmation for ambiguous requests
4. **Provide Feedback**: Let users know what the system understood
5. **Learn from Errors**: Collect and analyze failures to improve your system

### Required Libraries

For the techniques in this module, you'll need:

```
pip install spacy scikit-learn numpy transformers torch
python -m spacy download en_core_web_sm
```

## BRIDGE TO PRACTICE

Now that you understand speech understanding concepts, proceed to the practice guide. You'll build a system that can recognize intents, extract entities, and maintain conversation context. This will form the core understanding component of your voice assistant.
