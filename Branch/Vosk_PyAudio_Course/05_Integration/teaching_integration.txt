# INTEGRATION OF SPEECH RECOGNITION AND UNDERSTANDING

## GLOSSARY

- **Modular Architecture**: A design approach that divides a system into separate, interchangeable components
- **API (Application Programming Interface)**: A set of rules that allows different programs to communicate with each other
- **Pipeline**: A sequence of processes where the output of one process becomes the input of the next
- **Asynchronous Processing**: A form of parallel computing where tasks can run independently without waiting for others to complete
- **Callback Function**: A function passed as an argument to another function, to be executed when a specific event occurs
- **Thread**: A lightweight subprocess that can execute concurrently with other threads
- **Queue**: A data structure that follows First-In-First-Out (FIFO) principle for processing elements
- **Event Loop**: A programming construct that waits for and dispatches events in a program's event queue
- **State Machine**: A mathematical model that describes the behavior of a system based on its current state
- **End-to-End System**: A complete solution that handles all aspects of a task without requiring external components

## CONCEPT INTERACTIONS

- **Building on Speech-to-Text**: We'll use the Vosk speech recognition capabilities from Module 3
- **Building on Speech Understanding**: We'll integrate the intent recognition and context management from Module 4
- **Looking Forward**: This integrated system will form the foundation for a complete voice assistant in Module 6

## MAIN CONTENT

### The Integration Challenge

In the previous modules, we've built:

1. **Speech Recognition**: Converting audio to text using Vosk
2. **Speech Understanding**: Converting text to intents and entities

Now we need to combine these components into a cohesive system. This integration presents several challenges:

1. **Real-time processing**: Recognition must happen while the user is speaking
2. **Continuous operation**: The system must listen continuously
3. **Efficient resource usage**: Processing audio and understanding speech require CPU resources
4. **Responsive feedback**: The system should respond quickly to user commands

### Architecture Overview

Our integrated system will follow this high-level architecture:

1. **Audio Input Module**: Captures audio and prepares it for processing
2. **Speech Recognition Module**: Converts audio to text using Vosk
3. **Speech Understanding Module**: Interprets text to extract intents and entities
4. **Response Generation Module**: Creates appropriate responses
5. **Action Execution Module**: Performs actions based on recognized intents

Here's a diagram of the data flow:

```
Audio Input → Speech Recognition → Speech Understanding → Response Generation → Action Execution
     ↑                                                            |
     └────────────────────────────────────────────────────────────┘
                             (Feedback loop)
```

### Designing a Modular Architecture

To create a flexible, maintainable system, we'll design our architecture with clear separation of concerns:

```python
class VoiceAssistant:
    """Main voice assistant class that integrates all components."""
    
    def __init__(self):
        """Initialize the voice assistant with all its components."""
        self.audio_manager = AudioManager()
        self.speech_recognizer = SpeechRecognizer()
        self.understanding_manager = UnderstandingManager()
        self.response_manager = ResponseManager()
        self.action_manager = ActionManager()
    
    def start(self):
        """Start the voice assistant."""
        # Initialize components
        self.audio_manager.initialize()
        self.speech_recognizer.initialize(self.audio_manager.get_sample_rate())
        
        # Start processing loop
        self._processing_loop()
    
    def stop(self):
        """Stop the voice assistant."""
        self.audio_manager.shutdown()
    
    def _processing_loop(self):
        """Main processing loop."""
        while True:
            # Get audio data
            audio_data = self.audio_manager.get_audio_chunk()
            
            # Process with speech recognizer
            text, is_final = self.speech_recognizer.process_audio(audio_data)
            
            # If we have text and it's a final result, process it
            if text and is_final:
                # Understand the text
                intent, entities = self.understanding_manager.process_text(text)
                
                # Generate a response
                response = self.response_manager.generate_response(intent, entities)
                
                # Execute any actions
                self.action_manager.execute_action(intent, entities)
                
                # Provide feedback (e.g., speak response)
                print(f"Assistant: {response}")
```

### Implementing the Audio Manager

The audio manager handles audio input and preprocessing:

```python
import pyaudio
import numpy as np
from scipy import signal

class AudioManager:
    """Handles audio input and preprocessing."""
    
    def __init__(self, sample_rate=16000, chunk_size=2048):
        """Initialize with audio parameters."""
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.p = None
        self.stream = None
        self.is_active = False
    
    def initialize(self):
        """Initialize PyAudio and open an input stream."""
        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size
        )
        self.is_active = True
        print("Audio manager initialized")
    
    def get_sample_rate(self):
        """Get the current sample rate."""
        return self.sample_rate
    
    def get_audio_chunk(self):
        """Get a chunk of audio data."""
        if not self.is_active:
            return None
            
        try:
            data = self.stream.read(self.chunk_size, exception_on_overflow=False)
            return data
        except Exception as e:
            print(f"Error reading audio: {e}")
            return None
    
    def preprocess_audio(self, audio_data):
        """
        Apply preprocessing to audio data.
        Implements techniques from Module 2.
        """
        # Convert to numpy array
        audio = np.frombuffer(audio_data, dtype=np.int16)
        
        # Normalize
        audio = audio / 32768.0
        
        # Apply simple noise reduction (high-pass filter)
        b, a = signal.butter(5, 300/(self.sample_rate/2), 'highpass')
        audio = signal.lfilter(b, a, audio)
        
        # Convert back to bytes
        processed = (audio * 32768).astype(np.int16).tobytes()
        
        return processed
    
    def shutdown(self):
        """Close and clean up audio resources."""
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        
        if self.p:
            self.p.terminate()
        
        self.is_active = False
        print("Audio manager shut down")
```

### Implementing the Speech Recognizer

The speech recognizer wraps Vosk functionality:

```python
from vosk import Model, KaldiRecognizer, SetLogLevel
import json
import os

class SpeechRecognizer:
    """Handles speech recognition using Vosk."""
    
    def __init__(self, model_path="models/vosk-model-small-en-us-0.15"):
        """Initialize with a model path."""
        self.model_path = model_path
        self.model = None
        self.recognizer = None
        self.is_initialized = False
    
    def initialize(self, sample_rate=16000):
        """Initialize the speech recognizer with a specific sample rate."""
        # Reduce Vosk logging
        SetLogLevel(-1)
        
        # Check if model exists
        if not os.path.exists(self.model_path):
            print(f"Model not found at {self.model_path}")
            print("Please download a model from https://alphacephei.com/vosk/models")
            return False
        
        try:
            # Load the model
            print(f"Loading model from {self.model_path}...")
            self.model = Model(self.model_path)
            self.recognizer = KaldiRecognizer(self.model, sample_rate)
            self.recognizer.SetWords(True)  # Enable word timestamps
            
            self.is_initialized = True
            print("Speech recognizer initialized")
            return True
            
        except Exception as e:
            print(f"Error initializing speech recognizer: {e}")
            return False
    
    def process_audio(self, audio_data):
        """
        Process audio data and return recognized text.
        
        Args:
            audio_data: Audio data bytes
            
        Returns:
            Tuple of (text, is_final)
        """
        if not self.is_initialized or not audio_data:
            return "", False
        
        try:
            # Process with Vosk
            if self.recognizer.AcceptWaveform(audio_data):
                # Final result
                result = json.loads(self.recognizer.Result())
                text = result.get("text", "").strip()
                return text, True
            else:
                # Partial result
                partial = json.loads(self.recognizer.PartialResult())
                text = partial.get("partial", "")
                return text, False
                
        except Exception as e:
            print(f"Error processing audio: {e}")
            return "", False
    
    def reset(self):
        """Reset the recognizer state."""
        if self.is_initialized:
            # Create a new recognizer with the same model and sample rate
            sample_rate = self.recognizer.SampleFrequency()
            self.recognizer = KaldiRecognizer(self.model, sample_rate)
            self.recognizer.SetWords(True)
```

### Implementing the Understanding Manager

The understanding manager processes recognized text to extract intents and entities:

```python
class UnderstandingManager:
    """
    Handles speech understanding by identifying intents and extracting entities.
    Implements techniques from Module 4.
    """
    
    def __init__(self):
        """Initialize the understanding manager."""
        # Import components from Module 4
        from speech_understanding import IntentRecognizer, ContextManager
        
        self.intent_recognizer = IntentRecognizer()
        self.context_manager = ContextManager()
        print("Understanding manager initialized")
    
    def process_text(self, text):
        """
        Process text to extract intent and entities.
        
        Args:
            text: The text to analyze
            
        Returns:
            Tuple of (intent, entities)
        """
        # Detect intent and entities
        intent, entities = self.intent_recognizer.detect_intent(text)
        
        # Update context
        self.context_manager.update_context(intent, entities)
        
        # Use context to enhance entities if needed
        for key in list(entities.keys()):
            if entities[key] is None:
                # Try to get from context
                entities[key] = self.context_manager.get_context_value(key)
        
        return intent, entities
```

### Implementing the Response Manager

The response manager generates appropriate responses based on intents and entities:

```python
import random
import time

class ResponseManager:
    """Generates responses based on intents and entities."""
    
    def __init__(self):
        """Initialize the response manager."""
        # Load response templates
        self.response_templates = {
            "greeting": [
                "Hello! How can I help you?",
                "Hi there! What can I do for you?",
                "Hey! How can I assist you today?"
            ],
            "farewell": [
                "Goodbye! Have a great day!",
                "See you later!",
                "Bye for now!"
            ],
            "weather_inquiry": [
                "The weather in {location} is currently {condition} with a temperature of {temperature}°F.",
                "In {location}, it's {condition} and {temperature}°F right now.",
                "It's {condition} in {location} with a temperature of {temperature}°F."
            ],
            "time_inquiry": [
                "The current time is {time}.",
                "It's {time} right now.",
                "The time is {time}."
            ],
            "date_inquiry": [
                "Today is {date}.",
                "It's {date} today.",
                "The date is {date}."
            ],
            "device_control": [
                "I'll {action} the {device} for you.",
                "Turning {action} the {device} now.",
                "The {device} has been turned {action}."
            ],
            "unknown_intent": [
                "I'm not sure what you mean. Could you try again?",
                "I didn't understand that. Can you rephrase it?",
                "I'm sorry, I don't understand what you're asking for."
            ]
        }
        print("Response manager initialized")
    
    def generate_response(self, intent, entities):
        """
        Generate a response based on intent and entities.
        
        Args:
            intent: The detected intent
            entities: Dictionary of entities
            
        Returns:
            Response string
        """
        # Get response templates for the intent
        templates = self.response_templates.get(
            intent, self.response_templates["unknown_intent"])
        
        # Pick a random template
        template = random.choice(templates)
        
        try:
            # For weather_inquiry, generate mock weather data
            if intent == "weather_inquiry" and "{condition}" in template:
                if "location" not in entities:
                    entities["location"] = "your location"
                
                weather_conditions = ["sunny", "cloudy", "rainy", "windy", "clear"]
                entities["condition"] = random.choice(weather_conditions)
                entities["temperature"] = str(random.randint(60, 85))
            
            # For time_inquiry, insert current time
            if intent == "time_inquiry" and "{time}" in template:
                entities["time"] = time.strftime("%I:%M %p")
            
            # For date_inquiry, insert current date
            if intent == "date_inquiry" and "{date}" in template:
                entities["date"] = time.strftime("%A, %B %d, %Y")
            
            # Format the template with entities
            response = template.format(**entities)
            
        except KeyError:
            # If formatting fails, use a generic response
            response = "I understood your request but don't have all the information I need."
        
        return response
```

### Implementing the Action Manager

The action manager executes actions based on recognized intents:

```python
class ActionManager:
    """Executes actions based on intents and entities."""
    
    def __init__(self):
        """Initialize the action manager."""
        # Register action handlers
        self.action_handlers = {
            "device_control": self._handle_device_control,
            "timer_set": self._handle_timer,
            "music_playback": self._handle_music,
            # Add more handlers as needed
        }
        print("Action manager initialized")
    
    def execute_action(self, intent, entities):
        """
        Execute an action based on intent and entities.
        
        Args:
            intent: The detected intent
            entities: Dictionary of entities
            
        Returns:
            Boolean indicating success
        """
        # Check if we have a handler for this intent
        if intent in self.action_handlers:
            handler = self.action_handlers[intent]
            return handler(entities)
        
        # No action needed for most intents
        return True
    
    def _handle_device_control(self, entities):
        """Handle device control actions."""
        device = entities.get("device", "")
        action = entities.get("action", "")
        
        if not device or not action:
            print("Missing device or action for device control")
            return False
        
        # In a real application, you would control actual devices here
        print(f"ACTION: {action.upper()} {device}")
        return True
    
    def _handle_timer(self, entities):
        """Handle timer actions."""
        duration = entities.get("seconds")
        
        if not duration:
            print("Missing duration for timer")
            return False
        
        # In a real application, you would start an actual timer
        minutes = duration // 60
        seconds = duration % 60
        print(f"ACTION: STARTING TIMER for {minutes}m {seconds}s")
        return True
    
    def _handle_music(self, entities):
        """Handle music playback actions."""
        song = entities.get("song", "")
        artist = entities.get("artist", "")
        
        # In a real application, you would play actual music
        if song and artist:
            print(f"ACTION: PLAYING {song} by {artist}")
        elif song:
            print(f"ACTION: PLAYING {song}")
        else:
            print("ACTION: PLAYING MUSIC (no specific song)")
        
        return True
```

### Putting It All Together: Threaded Architecture

To make our system more responsive, we'll use threads with queues to process data asynchronously:

```python
import threading
import queue

class ThreadedVoiceAssistant:
    """Voice assistant with threaded processing architecture."""
    
    def __init__(self):
        """Initialize the voice assistant with threaded components."""
        # Create components
        self.audio_manager = AudioManager()
        self.speech_recognizer = SpeechRecognizer()
        self.understanding_manager = UnderstandingManager()
        self.response_manager = ResponseManager()
        self.action_manager = ActionManager()
        
        # Create queues for inter-thread communication
        self.audio_queue = queue.Queue()
        self.text_queue = queue.Queue()
        self.response_queue = queue.Queue()
        
        # Create control flags
        self.running = threading.Event()
    
    def start(self):
        """Start the voice assistant with all worker threads."""
        print("Starting threaded voice assistant...")
        
        # Initialize components
        self.audio_manager.initialize()
        self.speech_recognizer.initialize(self.audio_manager.get_sample_rate())
        
        # Set running flag
        self.running.set()
        
        # Create and start worker threads
        self.audio_thread = threading.Thread(target=self._audio_worker)
        self.recognition_thread = threading.Thread(target=self._recognition_worker)
        self.understanding_thread = threading.Thread(target=self._understanding_worker)
        
        self.audio_thread.daemon = True
        self.recognition_thread.daemon = True
        self.understanding_thread.daemon = True
        
        self.audio_thread.start()
        self.recognition_thread.start()
        self.understanding_thread.start()
        
        print("Voice assistant started. Press Ctrl+C to stop.")
        
        # Main thread handles output to avoid threading issues with console
        try:
            while self.running.is_set():
                try:
                    # Check for responses
                    response = self.response_queue.get(timeout=0.1)
                    print(f"\nAssistant: {response}")
                    self.response_queue.task_done()
                except queue.Empty:
                    continue
        except KeyboardInterrupt:
            print("\nStopping voice assistant...")
            self.stop()
    
    def stop(self):
        """Stop the voice assistant and all threads."""
        self.running.clear()
        
        # Wait for threads to finish
        if hasattr(self, 'audio_thread') and self.audio_thread.is_alive():
            self.audio_thread.join(timeout=1.0)
            
        if hasattr(self, 'recognition_thread') and self.recognition_thread.is_alive():
            self.recognition_thread.join(timeout=1.0)
            
        if hasattr(self, 'understanding_thread') and self.understanding_thread.is_alive():
            self.understanding_thread.join(timeout=1.0)
        
        # Shut down audio
        self.audio_manager.shutdown()
        
        print("Voice assistant stopped")
    
    def _audio_worker(self):
        """Audio capture worker thread."""
        print("Audio worker started")
        
        while self.running.is_set():
            # Get audio data
            audio_data = self.audio_manager.get_audio_chunk()
            
            if audio_data:
                # Optional preprocessing
                processed_audio = self.audio_manager.preprocess_audio(audio_data)
                
                # Add to queue for recognition
                self.audio_queue.put(processed_audio)
            
            # Small pause to reduce CPU usage
            time.sleep(0.001)
    
    def _recognition_worker(self):
        """Speech recognition worker thread."""
        print("Recognition worker started")
        
        while self.running.is_set():
            try:
                # Get audio data from queue with timeout
                audio_data = self.audio_queue.get(timeout=0.1)
                
                # Process with recognizer
                text, is_final = self.speech_recognizer.process_audio(audio_data)
                
                # If we have final text, add to text queue
                if text and is_final:
                    self.text_queue.put(text)
                    print(f"\nRecognized: \"{text}\"")
                elif text:  # Partial result
                    print(f"Listening: {text}", end="\r", flush=True)
                
                # Mark task as done
                self.audio_queue.task_done()
                
            except queue.Empty:
                # Queue timeout - just continue
                pass
    
    def _understanding_worker(self):
        """Speech understanding worker thread."""
        print("Understanding worker started")
        
        while self.running.is_set():
            try:
                # Get text from queue with timeout
                text = self.text_queue.get(timeout=0.1)
                
                # Process with understanding manager
                intent, entities = self.understanding_manager.process_text(text)
                
                print(f"Intent: {intent}, Entities: {entities}")
                
                # Generate response
                response = self.response_manager.generate_response(intent, entities)
                
                # Add to response queue
                self.response_queue.put(response)
                
                # Execute any actions
                self.action_manager.execute_action(intent, entities)
                
                # Mark task as done
                self.text_queue.task_done()
                
            except queue.Empty:
                # Queue timeout - just continue
                pass
```

### Testing the Integrated System

Here's how to test the integrated system:

```python
def main():
    """Main entry point for the application."""
    try:
        # Create and start the voice assistant
        assistant = ThreadedVoiceAssistant()
        assistant.start()
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == "__main__":
    main()
```

### Advanced Integration Topics

#### 1. Handling Cross-Component Communication

In a more complex system, you might need to handle events across components. An event bus pattern can help:

```python
class EventBus:
    """Simple event bus for cross-component communication."""
    
    def __init__(self):
        """Initialize with empty subscribers dict."""
        self.subscribers = {}
    
    def subscribe(self, event_name, callback):
        """Subscribe to an event with a callback."""
        if event_name not in self.subscribers:
            self.subscribers[event_name] = []
        self.subscribers[event_name].append(callback)
    
    def publish(self, event_name, **kwargs):
        """Publish an event with optional data."""
        if event_name in self.subscribers:
            for callback in self.subscribers[event_name]:
                try:
                    callback(**kwargs)
                except Exception as e:
                    print(f"Error in event handler: {e}")
```

Components can then interact through the event bus:

```python
# Create an event bus
event_bus = EventBus()

# Subscribe to events
event_bus.subscribe("intent_detected", lambda intent, entities: 
    print(f"Intent detected: {intent}")
)

# Publish events
event_bus.publish("intent_detected", intent="weather_inquiry", 
    entities={"location": "New York"})
```

#### 2. Dynamically Loading Components

For a more extensible architecture, you can dynamically load components:

```python
import importlib

def load_component(component_type, component_name):
    """
    Dynamically load a component.
    
    Args:
        component_type: Type of component (e.g., 'recognizer', 'action')
        component_name: Name of component module to load
        
    Returns:
        Instantiated component
    """
    try:
        # Import the module
        module = importlib.import_module(f"components.{component_type}.{component_name}")
        
        # Get the component class (assumed to be named ComponentClass)
        component_class = getattr(module, "ComponentClass")
        
        # Instantiate and return
        return component_class()
    except Exception as e:
        print(f"Error loading component {component_name}: {e}")
        return None

# Usage
speech_recognizer = load_component("recognizer", "vosk_recognizer")
```

#### 3. Configuration Management

For a flexible system, implement configuration management:

```python
import json
import os

class ConfigManager:
    """Manages configuration for the voice assistant."""
    
    def __init__(self, config_path="config.json"):
        """Initialize with a configuration file path."""
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self):
        """Load configuration from file."""
        if not os.path.exists(self.config_path):
            # Create default config
            config = {
                "audio": {
                    "sample_rate": 16000,
                    "chunk_size": 2048
                },
                "speech_recognition": {
                    "model_path": "models/vosk-model-small-en-us-0.15"
                },
                "actions": {
                    "enabled": ["device_control", "timer_set"]
                }
            }
            self._save_config(config)
            return config
            
        try:
            with open(self.config_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading config: {e}")
            return {}
    
    def _save_config(self, config):
        """Save configuration to file."""
        try:
            with open(self.config_path, 'w') as f:
                json.dump(config, f, indent=4)
        except Exception as e:
            print(f"Error saving config: {e}")
    
    def get(self, section, key, default=None):
        """Get a configuration value."""
        if section in self.config and key in self.config[section]:
            return self.config[section][key]
        return default
    
    def set(self, section, key, value):
        """Set a configuration value."""
        if section not in self.config:
            self.config[section] = {}
        self.config[section][key] = value
        self._save_config(self.config)
```

### Required Libraries

For this module, you'll need:

```
pip install vosk pyaudio numpy scipy threading queue
```

## BRIDGE TO PRACTICE

Now that you understand the concepts of integrating speech recognition and understanding, head to the practice guide. You'll build a complete voice assistant system that combines the components from previous modules into a cohesive application. The practice guide will walk you through each step of creating a threaded architecture that provides real-time speech processing and response generation.
