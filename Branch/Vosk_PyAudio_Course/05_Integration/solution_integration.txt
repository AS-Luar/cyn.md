# SOLUTION: INTEGRATION OF SPEECH RECOGNITION AND UNDERSTANDING

## OVERVIEW

This solution implements a complete integration of the speech recognition and understanding components developed in previous modules. It demonstrates a modular, threaded architecture for processing audio input, recognizing speech, and responding to user intents.

## FILE 1: threaded_audio_processor.py

```python
import threading
import queue
import pyaudio
import time
import numpy as np

class ThreadedAudioProcessor:
    def __init__(self, sample_rate=16000, frames_per_buffer=8000, channels=1):
        # Audio parameters
        self.sample_rate = sample_rate
        self.frames_per_buffer = frames_per_buffer
        self.channels = channels
        
        # Initialize PyAudio
        self.audio = pyaudio.PyAudio()
        
        # Create audio frames queue
        self.audio_queue = queue.Queue()
        
        # Thread control
        self.is_running = False
        self.thread = None
        
    def start_processing(self):
        """Start the audio processing in a background thread"""
        if self.is_running:
            print("Audio processor is already running")
            return
            
        self.is_running = True
        self.thread = threading.Thread(target=self.process_audio_thread)
        self.thread.daemon = True
        self.thread.start()
        print("Audio processing started")
        
    def process_audio_thread(self):
        """Main audio processing loop that runs in a background thread"""
        try:
            # Open audio stream
            stream = self.audio.open(
                format=pyaudio.paInt16,
                channels=self.channels,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.frames_per_buffer
            )
            
            print("Audio stream opened")
            
            # Process audio while running
            while self.is_running:
                try:
                    # Read audio frame
                    data = stream.read(self.frames_per_buffer, exception_on_overflow=False)
                    
                    # Optional: apply preprocessing here (noise reduction, normalization, etc.)
                    
                    # Add frame to queue
                    self.audio_queue.put(data)
                except Exception as e:
                    print(f"Error capturing audio: {e}")
                    time.sleep(0.1)  # Prevent tight loop if errors occur
            
            # Clean up
            stream.stop_stream()
            stream.close()
            print("Audio stream closed")
            
        except Exception as e:
            print(f"Error in audio processing thread: {e}")
            self.is_running = False
    
    def stop_processing(self):
        """Stop the audio processing thread"""
        if not self.is_running:
            print("Audio processor is not running")
            return
            
        self.is_running = False
        if self.thread:
            self.thread.join(timeout=2.0)  # Wait for thread to finish
        
        # Clear the queue
        while not self.audio_queue.empty():
            try:
                self.audio_queue.get_nowait()
            except queue.Empty:
                break
                
        print("Audio processing stopped")
    
    def get_audio_queue(self):
        """Return the audio queue for other components to use"""
        return self.audio_queue
    
    def __del__(self):
        """Cleanup when object is destroyed"""
        self.stop_processing()
        if self.audio:
            self.audio.terminate()
```

## FILE 2: threaded_recognition.py

```python
import threading
import queue
import json
import time
from vosk import Model, KaldiRecognizer

class ThreadedRecognition:
    def __init__(self, audio_queue, model_path, sample_rate=16000):
        # Store the audio queue
        self.audio_queue = audio_queue
        
        # Create a text output queue
        self.text_queue = queue.Queue()
        
        # Audio parameters
        self.sample_rate = sample_rate
        
        # Load Vosk model
        try:
            self.model = Model(model_path)
            self.recognizer = KaldiRecognizer(self.model, self.sample_rate)
            self.recognizer.SetWords(True)  # Get word timings
            print(f"Loaded Vosk model from {model_path}")
        except Exception as e:
            print(f"Error loading Vosk model: {e}")
            raise
        
        # Thread control
        self.is_running = False
        self.thread = None
    
    def start_recognition(self):
        """Start the recognition in a background thread"""
        if self.is_running:
            print("Recognition is already running")
            return
            
        self.is_running = True
        self.thread = threading.Thread(target=self.recognition_thread)
        self.thread.daemon = True
        self.thread.start()
        print("Speech recognition started")
    
    def recognition_thread(self):
        """Main recognition loop that runs in a background thread"""
        try:
            while self.is_running:
                try:
                    # Get audio data from queue with timeout
                    audio_data = self.audio_queue.get(timeout=1.0)
                    
                    # Process with Vosk
                    if self.recognizer.AcceptWaveform(audio_data):
                        # Get final result
                        result = json.loads(self.recognizer.Result())
                        if 'text' in result and result['text'].strip():
                            self.text_queue.put(('final', result['text']))
                    else:
                        # Get partial result
                        result = json.loads(self.recognizer.PartialResult())
                        if 'partial' in result and result['partial'].strip():
                            self.text_queue.put(('partial', result['partial']))
                
                except queue.Empty:
                    continue  # No audio data, continue waiting
                    
                except Exception as e:
                    print(f"Error in recognition: {e}")
                    time.sleep(0.1)  # Prevent tight loop if errors occur
        
        except Exception as e:
            print(f"Error in recognition thread: {e}")
            self.is_running = False
    
    def stop_recognition(self):
        """Stop the recognition thread"""
        if not self.is_running:
            print("Recognition is not running")
            return
            
        self.is_running = False
        if self.thread:
            self.thread.join(timeout=2.0)  # Wait for thread to finish
        
        # Process any final results
        final_result = json.loads(self.recognizer.FinalResult())
        if 'text' in final_result and final_result['text'].strip():
            self.text_queue.put(('final', final_result['text']))
        
        print("Speech recognition stopped")
    
    def get_text_queue(self):
        """Return the text queue for other components to use"""
        return self.text_queue
```

## FILE 3: intent_processor.py

```python
import threading
import queue
import re
import json
import time

class IntentProcessor:
    def __init__(self, text_queue):
        # Store the text queue
        self.text_queue = text_queue
        
        # Create a results queue for processed intents
        self.results_queue = queue.Queue()
        
        # Define intent patterns
        self.intents = {
            'greeting': r'\b(hello|hi|hey|greetings)\b',
            'weather': r'\b(weather|temperature|forecast)\b.*?\b(in|for|at)?\s+([a-zA-Z ]+)',
            'time': r'\bwhat\s+(?:(?:time|hour)\s+is\s+it|is\s+the\s+time)\b',
            'search': r'\b(?:search|look|google)\s+(?:for)?\s+(.+)',
            'play': r'\b(?:play|start)\s+(?:the\s+)?(?:song|music|video)?\s*(.+)',
            'stop': r'\b(stop|exit|quit|end)\b'
        }
        
        # Context management
        self.context = {
            'current_intent': None,
            'last_entities': {},
            'conversation_history': []
        }
        
        # Thread control
        self.is_running = False
        self.thread = None
    
    def start_processing(self):
        """Start the intent processing in a background thread"""
        if self.is_running:
            print("Intent processor is already running")
            return
            
        self.is_running = True
        self.thread = threading.Thread(target=self.processing_thread)
        self.thread.daemon = True
        self.thread.start()
        print("Intent processing started")
    
    def processing_thread(self):
        """Main processing loop that runs in a background thread"""
        try:
            while self.is_running:
                try:
                    # Get text data from queue with timeout
                    result_type, text = self.text_queue.get(timeout=1.0)
                    
                    # Only process final results for intents
                    if result_type == 'final':
                        intent, entities = self.identify_intent(text)
                        
                        # Update context
                        self.context['current_intent'] = intent
                        if entities:
                            self.context['last_entities'].update(entities)
                        self.context['conversation_history'].append((text, intent, entities))
                        
                        # Generate response
                        response = self.generate_response(intent, entities)
                        
                        # Put result in output queue
                        result = {
                            'text': text,
                            'intent': intent,
                            'entities': entities,
                            'response': response
                        }
                        self.results_queue.put(result)
                
                except queue.Empty:
                    continue  # No text data, continue waiting
                    
                except Exception as e:
                    print(f"Error in intent processing: {e}")
                    time.sleep(0.1)  # Prevent tight loop if errors occur
        
        except Exception as e:
            print(f"Error in intent processing thread: {e}")
            self.is_running = False
    
    def identify_intent(self, text):
        """Identify intent and entities in the text"""
        text = text.lower()
        intent = None
        entities = {}
        
        # Check for each intent pattern
        for intent_name, pattern in self.intents.items():
            match = re.search(pattern, text)
            if match:
                intent = intent_name
                
                # Extract entities based on intent
                if intent == 'weather' and len(match.groups()) >= 1:
                    location = match.group(3) if len(match.groups()) >= 3 else None
                    if location:
                        entities['location'] = location.strip()
                
                elif intent == 'search' and len(match.groups()) >= 1:
                    query = match.group(1)
                    entities['query'] = query.strip()
                
                elif intent == 'play' and len(match.groups()) >= 1:
                    media = match.group(1)
                    entities['media'] = media.strip()
                
                break
        
        # Default to 'unknown' if no intent matched
        if not intent:
            intent = 'unknown'
        
        return intent, entities
    
    def generate_response(self, intent, entities):
        """Generate a response based on the intent and entities"""
        if intent == 'greeting':
            return "Hello! How can I help you today?"
        
        elif intent == 'weather':
            location = entities.get('location', 'your area')
            return f"I'll check the weather in {location} for you."
        
        elif intent == 'time':
            from datetime import datetime
            current_time = datetime.now().strftime('%H:%M')
            return f"The current time is {current_time}."
        
        elif intent == 'search':
            query = entities.get('query', '')
            return f"I'll search for '{query}' for you."
        
        elif intent == 'play':
            media = entities.get('media', '')
            return f"Playing {media} for you now."
        
        elif intent == 'stop':
            return "Stopping now. Goodbye!"
        
        else:
            return "I'm not sure what you want me to do."
    
    def stop_processing(self):
        """Stop the intent processing thread"""
        if not self.is_running:
            print("Intent processor is not running")
            return
            
        self.is_running = False
        if self.thread:
            self.thread.join(timeout=2.0)  # Wait for thread to finish
        
        print("Intent processing stopped")
    
    def get_results_queue(self):
        """Return the results queue for other components to use"""
        return self.results_queue
```

## FILE 4: integrated_assistant.py

```python
from threaded_audio_processor import ThreadedAudioProcessor
from threaded_recognition import ThreadedRecognition
from intent_processor import IntentProcessor
import queue
import time
import os
import signal

class IntegratedAssistant:
    def __init__(self, model_path):
        """Initialize the integrated voice assistant with all components"""
        # Check if model exists
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Vosk model not found at {model_path}")
            
        # Initialize the audio processor
        self.audio_processor = ThreadedAudioProcessor()
        self.audio_queue = self.audio_processor.get_audio_queue()
        
        # Initialize the recognition component
        self.recognition = ThreadedRecognition(self.audio_queue, model_path)
        self.text_queue = self.recognition.get_text_queue()
        
        # Initialize the intent processor
        self.intent_processor = IntentProcessor(self.text_queue)
        self.results_queue = self.intent_processor.get_results_queue()
        
        # System control
        self.is_running = False
    
    def start(self):
        """Start all components of the assistant"""
        if self.is_running:
            print("Assistant is already running")
            return
            
        print("Starting integrated assistant...")
        
        # Start components from bottom up
        self.audio_processor.start_processing()
        time.sleep(0.5)  # Small delay to ensure audio processor is ready
        
        self.recognition.start_recognition()
        time.sleep(0.5)  # Small delay to ensure recognition is ready
        
        self.intent_processor.start_processing()
        
        self.is_running = True
        print("Integrated assistant is now running")
    
    def stop(self):
        """Stop all components of the assistant"""
        if not self.is_running:
            print("Assistant is not running")
            return
            
        print("Stopping integrated assistant...")
        
        # Stop components from top down
        self.intent_processor.stop_processing()
        self.recognition.stop_recognition()
        self.audio_processor.stop_processing()
        
        self.is_running = False
        print("Integrated assistant has been stopped")
    
    def run_interactive(self):
        """Run the assistant interactively with console output"""
        # Register signal handler for clean shutdown
        signal.signal(signal.SIGINT, lambda sig, frame: self.handle_interrupt())
        
        self.start()
        
        print("\nAssistant is listening... (Press Ctrl+C to exit)")
        try:
            while self.is_running:
                try:
                    # Process results with a timeout
                    result = self.results_queue.get(timeout=0.5)
                    
                    # Print the result
                    print("\n" + "="*50)
                    print(f"You said: {result['text']}")
                    print(f"Intent: {result['intent']}")
                    print(f"Entities: {result['entities']}")
                    print(f"Response: {result['response']}")
                    print("="*50 + "\n")
                    
                    # Check for stop intent
                    if result['intent'] == 'stop':
                        print("Stop command detected. Shutting down...")
                        self.stop()
                        break
                        
                except queue.Empty:
                    continue  # No results, continue waiting
        
        except Exception as e:
            print(f"Error in interactive mode: {e}")
        
        finally:
            self.stop()
    
    def handle_interrupt(self):
        """Handle keyboard interrupt (Ctrl+C)"""
        print("\nInterrupt received. Shutting down...")
        self.stop()
```

## FILE 5: main.py

```python
from integrated_assistant import IntegratedAssistant
import os
import sys

def main():
    """Main entry point for the integrated voice assistant"""
    # Default model path - adjust as needed
    model_path = os.path.expanduser("~/.cache/vosk/vosk-model-small-en-us-0.15")
    
    # Check command line arguments for a custom model path
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
    
    print(f"Using Vosk model at: {model_path}")
    
    try:
        # Create and start the assistant
        assistant = IntegratedAssistant(model_path)
        assistant.run_interactive()
        
    except KeyboardInterrupt:
        print("\nExiting...")
        
    except Exception as e:
        print(f"Error: {e}")
        return 1
        
    return 0

if __name__ == "__main__":
    sys.exit(main())
```

## USAGE GUIDE

1. Ensure you have installed all necessary dependencies:
   ```
   pip install vosk pyaudio numpy
   ```

2. Download a Vosk model (if you haven't already):
   ```
   mkdir -p ~/.cache/vosk
   wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
   unzip vosk-model-small-en-us-0.15.zip -d ~/.cache/vosk/
   ```

3. Run the integrated assistant:
   ```
   python main.py
   ```

4. Speak commands like:
   - "Hello"
   - "What time is it"
   - "What's the weather in New York"
   - "Search for Python tutorials"
   - "Play some jazz music"
   - "Stop" (to exit)

## SYSTEM ARCHITECTURE

The implementation follows a modular, threaded architecture:

1. **ThreadedAudioProcessor**: Captures audio continuously in a background thread and puts frames in a queue.
2. **ThreadedRecognition**: Takes audio frames from the queue, processes them with Vosk, and puts recognized text in another queue.
3. **IntentProcessor**: Takes recognized text, identifies intents and entities, and generates appropriate responses.
4. **IntegratedAssistant**: Coordinates all components and provides the main interface for the user.

This architecture provides several advantages:
- Decoupled components that can be developed and tested independently
- Efficient resource usage through background threading
- Responsive user experience
- Clean separation of concerns

## FUTURE IMPROVEMENTS

1. Add audio output for spoken responses
2. Implement more sophisticated intent recognition using machine learning
3. Add a wake word detection system
4. Implement a more advanced context management system
5. Create a graphical user interface
6. Add plugin support for extensibility
