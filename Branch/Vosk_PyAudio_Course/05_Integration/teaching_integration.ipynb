{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782ee7e7",
   "metadata": {},
   "source": [
    "# INTEGRATION OF SPEECH RECOGNITION AND UNDERSTANDING\n",
    "\n",
    "## GLOSSARY\n",
    "\n",
    "- **Modular Architecture**: A design approach that divides a system into separate, interchangeable components\n",
    "- **API (Application Programming Interface)**: A set of rules that allows different programs to communicate with each other\n",
    "- **Pipeline**: A sequence of processes where the output of one process becomes the input of the next\n",
    "- **Asynchronous Processing**: A form of parallel computing where tasks can run independently without waiting for others to complete\n",
    "- **Callback Function**: A function passed as an argument to another function, to be executed when a specific event occurs\n",
    "- **Thread**: A lightweight subprocess that can execute concurrently with other threads\n",
    "- **Queue**: A data structure that follows First-In-First-Out (FIFO) principle for processing elements\n",
    "- **Event Loop**: A programming construct that waits for and dispatches events in a program's event queue\n",
    "- **State Machine**: A mathematical model that describes the behavior of a system based on its current state\n",
    "- **End-to-End System**: A complete solution that handles all aspects of a task without requiring external components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e46641",
   "metadata": {},
   "source": [
    "## CONCEPT INTERACTIONS\n",
    "\n",
    "- **Building on Speech-to-Text**: We'll use the Vosk speech recognition capabilities from Module 3\n",
    "- **Building on Speech Understanding**: We'll integrate the intent recognition and context management from Module 4\n",
    "- **Looking Forward**: This integrated system will form the foundation for a complete voice assistant in Module 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952e558",
   "metadata": {},
   "source": [
    "## MAIN CONTENT\n",
    "\n",
    "### The Integration Challenge\n",
    "\n",
    "In the previous modules, we've built:\n",
    "\n",
    "1. **Speech Recognition**: Converting audio to text using Vosk\n",
    "2. **Speech Understanding**: Converting text to intents and entities\n",
    "\n",
    "Now we need to combine these components into a cohesive system. This integration presents several challenges:\n",
    "\n",
    "1. **Real-time processing**: Recognition must happen while the user is speaking\n",
    "2. **Continuous operation**: The system must listen continuously\n",
    "3. **Efficient resource usage**: Processing audio and understanding speech require CPU resources\n",
    "4. **Responsive feedback**: The system should respond quickly to user commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089a56a",
   "metadata": {},
   "source": [
    "### Architecture Overview\n",
    "\n",
    "Our integrated system will follow this high-level architecture:\n",
    "\n",
    "1. **Audio Input Module**: Captures audio and prepares it for processing\n",
    "2. **Speech Recognition Module**: Converts audio to text using Vosk\n",
    "3. **Speech Understanding Module**: Interprets text to extract intents and entities\n",
    "4. **Response Generation Module**: Creates appropriate responses\n",
    "5. **Action Execution Module**: Performs actions based on recognized intents\n",
    "\n",
    "Here's a diagram of the data flow:\n",
    "\n",
    "```\n",
    "Audio Input → Speech Recognition → Speech Understanding → Response Generation → Action Execution\n",
    "     ↑                                                            |\n",
    "     └────────────────────────────────────────────────────────────┘\n",
    "                             (Feedback loop)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60599381",
   "metadata": {},
   "source": [
    "### Designing a Modular Architecture\n",
    "\n",
    "To create a flexible, maintainable system, we'll design our architecture with clear separation of concerns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6612717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceAssistant:\n",
    "    \"\"\"Main voice assistant class that integrates all components.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the voice assistant with all its components.\"\"\"\n",
    "        self.audio_manager = AudioManager()\n",
    "        self.speech_recognizer = SpeechRecognizer()\n",
    "        self.understanding_manager = UnderstandingManager()\n",
    "        self.response_manager = ResponseManager()\n",
    "        self.action_manager = ActionManager()\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start the voice assistant.\"\"\"\n",
    "        # Initialize components\n",
    "        self.audio_manager.initialize()\n",
    "        self.speech_recognizer.initialize(self.audio_manager.get_sample_rate())\n",
    "        \n",
    "        # Start processing loop\n",
    "        self._processing_loop()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the voice assistant.\"\"\"\n",
    "        self.audio_manager.shutdown()\n",
    "    \n",
    "    def _processing_loop(self):\n",
    "        \"\"\"Main processing loop.\"\"\"\n",
    "        while True:\n",
    "            # Get audio data\n",
    "            audio_data = self.audio_manager.get_audio_chunk()\n",
    "            \n",
    "            # Process with speech recognizer\n",
    "            text, is_final = self.speech_recognizer.process_audio(audio_data)\n",
    "            \n",
    "            # If we have text and it's a final result, process it\n",
    "            if text and is_final:\n",
    "                # Understand the text\n",
    "                intent, entities = self.understanding_manager.process_text(text)\n",
    "                \n",
    "                # Generate a response\n",
    "                response = self.response_manager.generate_response(intent, entities)\n",
    "                \n",
    "                # Execute any actions\n",
    "                self.action_manager.execute_action(intent, entities)\n",
    "                \n",
    "                # Output the response (text-to-speech would go here in a full implementation)\n",
    "                print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65adc5",
   "metadata": {},
   "source": [
    "Let's define each of our component classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import os\n",
    "\n",
    "class AudioManager:\n",
    "    \"\"\"Manages audio input and output.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=1024, format=pyaudio.paInt16, \n",
    "                 channels=1, rate=16000, timeout=2):\n",
    "        \"\"\"Initialize audio parameters.\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.format = format\n",
    "        self.channels = channels\n",
    "        self.rate = rate\n",
    "        self.timeout = timeout\n",
    "        self.p = None\n",
    "        self.stream = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize PyAudio and open input stream.\"\"\"\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(\n",
    "            format=self.format,\n",
    "            channels=self.channels,\n",
    "            rate=self.rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk_size\n",
    "        )\n",
    "        \n",
    "    def get_sample_rate(self):\n",
    "        \"\"\"Get the sample rate.\"\"\"\n",
    "        return self.rate\n",
    "        \n",
    "    def get_audio_chunk(self):\n",
    "        \"\"\"Get a chunk of audio data.\"\"\"\n",
    "        if self.stream:\n",
    "            return self.stream.read(self.chunk_size, exception_on_overflow=False)\n",
    "        return None\n",
    "        \n",
    "    def shutdown(self):\n",
    "        \"\"\"Close the audio stream and PyAudio.\"\"\"\n",
    "        if self.stream:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "        if self.p:\n",
    "            self.p.terminate()\n",
    "\n",
    "\n",
    "class SpeechRecognizer:\n",
    "    \"\"\"Speech recognition using Vosk.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"path/to/model\"):\n",
    "        \"\"\"Initialize with model path.\"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.recognizer = None\n",
    "        self.partial_result = \"\"\n",
    "        \n",
    "    def initialize(self, sample_rate):\n",
    "        \"\"\"Initialize the Vosk model and recognizer.\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            raise ValueError(f\"Model path {self.model_path} does not exist\")\n",
    "            \n",
    "        self.model = Model(self.model_path)\n",
    "        self.recognizer = KaldiRecognizer(self.model, sample_rate)\n",
    "        \n",
    "    def process_audio(self, audio_data):\n",
    "        \"\"\"\n",
    "        Process an audio chunk.\n",
    "        \n",
    "        Args:\n",
    "            audio_data: Audio data to process\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (text, is_final) where is_final indicates if this is a complete utterance\n",
    "        \"\"\"\n",
    "        if self.recognizer.AcceptWaveform(audio_data):\n",
    "            # This is a final result\n",
    "            result = json.loads(self.recognizer.Result())\n",
    "            text = result.get(\"text\", \"\").strip()\n",
    "            self.partial_result = \"\"\n",
    "            return text, True\n",
    "        else:\n",
    "            # This is a partial result\n",
    "            result = json.loads(self.recognizer.PartialResult())\n",
    "            partial = result.get(\"partial\", \"\").strip()\n",
    "            if partial != self.partial_result:\n",
    "                self.partial_result = partial\n",
    "                return partial, False\n",
    "                \n",
    "        return \"\", False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128dc87",
   "metadata": {},
   "source": [
    "Now, let's add our understanding and response components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnderstandingManager:\n",
    "    \"\"\"Manages natural language understanding.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with intent patterns.\"\"\"\n",
    "        self.intent_patterns = {\n",
    "            \"greeting\": [\n",
    "                r\"(hello|hi|hey|greetings)( there| assistant| voice assistant)?\",\n",
    "                r\"good (morning|afternoon|evening)\"\n",
    "            ],\n",
    "            \"farewell\": [\n",
    "                r\"(goodbye|bye|see you( later)?)\",\n",
    "                r\"(exit|quit|stop)( assistant| program)?\"\n",
    "            ],\n",
    "            \"weather_inquiry\": [\n",
    "                r\"(what|how)('s| is) (the )?weather( like)?( in (?P<location>\\w+))?\",\n",
    "                r\"(weather|forecast)( in| for) (?P<location>[\\w\\s]+)\"\n",
    "            ],\n",
    "            \"time_inquiry\": [\n",
    "                r\"what('s| is) (the )?time( now)?\",\n",
    "                r\"(tell|give) me the (current |)time\"\n",
    "            ],\n",
    "            \"device_control\": [\n",
    "                r\"(turn|switch) (?P<action>on|off) (the )?(?P<device>[\\w\\s]+)( please)?\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Context stores information across turns\n",
    "        self.context = {\n",
    "            \"last_intent\": None,\n",
    "            \"entities\": {}\n",
    "        }\n",
    "        \n",
    "    def process_text(self, text):\n",
    "        \"\"\"\n",
    "        Process text to extract intent and entities.\n",
    "        \n",
    "        Args:\n",
    "            text: The text to process\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (intent, entities)\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Convert to lowercase for easier matching\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Check each intent pattern\n",
    "        for intent, patterns in self.intent_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, text)\n",
    "                if match:\n",
    "                    # Extract entities from named groups\n",
    "                    entities = {name: value for name, value \n",
    "                              in match.groupdict().items() if value}\n",
    "                    \n",
    "                    # Update context\n",
    "                    self.context[\"last_intent\"] = intent\n",
    "                    self.context[\"entities\"].update(entities)\n",
    "                    \n",
    "                    return intent, entities\n",
    "        \n",
    "        # Handle context for follow-up questions\n",
    "        if self.context[\"last_intent\"] == \"weather_inquiry\" and \"tomorrow\" in text:\n",
    "            # Handle follow-up like \"how about tomorrow?\"\n",
    "            entities = dict(self.context[\"entities\"])\n",
    "            entities[\"time\"] = \"tomorrow\"\n",
    "            return \"weather_inquiry\", entities\n",
    "            \n",
    "        # No intent recognized\n",
    "        return \"unknown\", {}\n",
    "\n",
    "\n",
    "class ResponseManager:\n",
    "    \"\"\"Generates responses based on intents and entities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize response templates.\"\"\"\n",
    "        import random\n",
    "        self.random = random\n",
    "        \n",
    "        self.response_templates = {\n",
    "            \"greeting\": [\n",
    "                \"Hello! How can I help you today?\",\n",
    "                \"Hi there! What can I do for you?\",\n",
    "                \"Greetings! How may I assist you?\"\n",
    "            ],\n",
    "            \"farewell\": [\n",
    "                \"Goodbye! Have a great day!\",\n",
    "                \"See you later!\",\n",
    "                \"Bye for now!\"\n",
    "            ],\n",
    "            \"weather_inquiry\": [\n",
    "                \"The weather in {location} is {condition} with a temperature of {temp}°F.\",\n",
    "                \"In {location}, it's {condition} and {temp}°F.\",\n",
    "                \"The forecast for {location} shows {condition} conditions and {temp}°F.\"\n",
    "            ],\n",
    "            \"time_inquiry\": [\n",
    "                \"The current time is {time}.\",\n",
    "                \"It's {time} right now.\",\n",
    "                \"The time is {time}.\"\n",
    "            ],\n",
    "            \"device_control\": [\n",
    "                \"I've turned {action} the {device}.\",\n",
    "                \"The {device} is now {action}.\",\n",
    "                \"{device} turned {action}.\"\n",
    "            ],\n",
    "            \"unknown\": [\n",
    "                \"I'm not sure I understand. Can you rephrase that?\",\n",
    "                \"I don't know how to help with that yet.\",\n",
    "                \"I didn't quite catch that. What would you like me to do?\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def generate_response(self, intent, entities):\n",
    "        \"\"\"\n",
    "        Generate a response based on intent and entities.\n",
    "        \n",
    "        Args:\n",
    "            intent: The recognized intent\n",
    "            entities: Dictionary of entities\n",
    "            \n",
    "        Returns:\n",
    "            A response string\n",
    "        \"\"\"\n",
    "        # If we don't have templates for this intent, use unknown\n",
    "        if intent not in self.response_templates:\n",
    "            intent = \"unknown\"\n",
    "            \n",
    "        # Get a random template for this intent\n",
    "        templates = self.response_templates[intent]\n",
    "        template = self.random.choice(templates)\n",
    "        \n",
    "        # For specific intents, add mock data\n",
    "        if intent == \"weather_inquiry\":\n",
    "            # Mock weather data\n",
    "            entities[\"location\"] = entities.get(\"location\", \"current location\")\n",
    "            entities[\"condition\"] = self.random.choice([\"sunny\", \"cloudy\", \"rainy\", \"clear\"])\n",
    "            entities[\"temp\"] = self.random.randint(65, 85)\n",
    "            \n",
    "        elif intent == \"time_inquiry\":\n",
    "            # Real time\n",
    "            entities[\"time\"] = time.strftime(\"%I:%M %p\")\n",
    "            \n",
    "        # Format the template with entities\n",
    "        try:\n",
    "            return template.format(**entities)\n",
    "        except KeyError:\n",
    "            # If we're missing required entities, return a fallback\n",
    "            return \"I need more information to help with that.\"\n",
    "\n",
    "\n",
    "class ActionManager:\n",
    "    \"\"\"Executes actions based on intents and entities.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize action handlers.\"\"\"\n",
    "        self.action_handlers = {\n",
    "            \"device_control\": self._handle_device_control,\n",
    "            \"weather_inquiry\": self._handle_weather_inquiry,\n",
    "            \"time_inquiry\": self._handle_time_inquiry\n",
    "        }\n",
    "        \n",
    "    def execute_action(self, intent, entities):\n",
    "        \"\"\"\n",
    "        Execute an action based on intent and entities.\n",
    "        \n",
    "        Args:\n",
    "            intent: The recognized intent\n",
    "            entities: Dictionary of entities\n",
    "            \n",
    "        Returns:\n",
    "            True if an action was executed, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if we have a handler for this intent\n",
    "        if intent in self.action_handlers:\n",
    "            # Call the handler\n",
    "            return self.action_handlers[intent](entities)\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def _handle_device_control(self, entities):\n",
    "        \"\"\"Handle device control actions.\"\"\"\n",
    "        device = entities.get(\"device\", \"unknown device\")\n",
    "        action = entities.get(\"action\", \"unknown action\")\n",
    "        \n",
    "        # In a real implementation, this would control actual devices\n",
    "        print(f\"[Action] Turning {action} {device}\")\n",
    "        return True\n",
    "    \n",
    "    def _handle_weather_inquiry(self, entities):\n",
    "        \"\"\"Handle weather inquiries.\"\"\"\n",
    "        location = entities.get(\"location\", \"current location\")\n",
    "        \n",
    "        # In a real implementation, this would call a weather API\n",
    "        print(f\"[Action] Looking up weather for {location}\")\n",
    "        return True\n",
    "    \n",
    "    def _handle_time_inquiry(self, entities):\n",
    "        \"\"\"Handle time inquiries.\"\"\"\n",
    "        # No action needed for time inquiries in this example\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce00b3",
   "metadata": {},
   "source": [
    "### Asynchronous Processing\n",
    "\n",
    "The simple design above works, but it has limitations. For real-time responsiveness, we should use asynchronous processing:\n",
    "\n",
    "1. **Speech recognition** should run in a background thread\n",
    "2. **Audio processing** should occur continuously\n",
    "3. **Intent handling** should not block audio capture\n",
    "\n",
    "Let's implement an asynchronous architecture using threads and queues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncVoiceAssistant:\n",
    "    \"\"\"Asynchronous voice assistant using threads and queues.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"path/to/model\"):\n",
    "        \"\"\"Initialize components and queues.\"\"\"\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Create component instances\n",
    "        self.audio_manager = AsyncAudioManager()\n",
    "        self.speech_recognizer = AsyncSpeechRecognizer(model_path)\n",
    "        self.understanding_manager = UnderstandingManager()\n",
    "        self.response_manager = ResponseManager()\n",
    "        self.action_manager = ActionManager()\n",
    "        \n",
    "        # Create communication queues\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.text_queue = queue.Queue()\n",
    "        self.intent_queue = queue.Queue()\n",
    "        self.response_queue = queue.Queue()\n",
    "        \n",
    "        # Control flags\n",
    "        self.running = False\n",
    "        self.threads = []\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start the voice assistant.\"\"\"\n",
    "        self.running = True\n",
    "        \n",
    "        # Create and start threads\n",
    "        self.threads = [\n",
    "            threading.Thread(target=self._audio_thread),\n",
    "            threading.Thread(target=self._recognition_thread),\n",
    "            threading.Thread(target=self._understanding_thread),\n",
    "            threading.Thread(target=self._response_thread)\n",
    "        ]\n",
    "        \n",
    "        for thread in self.threads:\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "        \n",
    "        print(\"Voice assistant started. Press Ctrl+C to stop.\")\n",
    "        \n",
    "        try:\n",
    "            # Keep main thread alive\n",
    "            while self.running:\n",
    "                time.sleep(0.1)\n",
    "        except KeyboardInterrupt:\n",
    "            self.stop()\n",
    "            \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the voice assistant.\"\"\"\n",
    "        print(\"Stopping voice assistant...\")\n",
    "        self.running = False\n",
    "        \n",
    "        # Wait for threads to finish\n",
    "        for thread in self.threads:\n",
    "            if thread.is_alive():\n",
    "                thread.join(timeout=1.0)\n",
    "                \n",
    "        # Clean up resources\n",
    "        self.audio_manager.shutdown()\n",
    "        print(\"Voice assistant stopped.\")\n",
    "            \n",
    "    def _audio_thread(self):\n",
    "        \"\"\"Thread that captures audio and puts it in the queue.\"\"\"\n",
    "        self.audio_manager.initialize()\n",
    "        \n",
    "        while self.running:\n",
    "            # Get audio chunk\n",
    "            audio_data = self.audio_manager.get_audio_chunk()\n",
    "            if audio_data:\n",
    "                # Put in queue for recognition\n",
    "                self.audio_queue.put(audio_data)\n",
    "                \n",
    "    def _recognition_thread(self):\n",
    "        \"\"\"Thread that processes audio and recognizes speech.\"\"\"\n",
    "        self.speech_recognizer.initialize(self.audio_manager.get_sample_rate())\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                # Get audio from queue with timeout\n",
    "                audio_data = self.audio_queue.get(timeout=0.5)\n",
    "                \n",
    "                # Process audio to get text\n",
    "                text, is_final = self.speech_recognizer.process_audio(audio_data)\n",
    "                \n",
    "                if text:\n",
    "                    if is_final:\n",
    "                        # Final result, send for understanding\n",
    "                        print(f\"Recognized: {text}\")\n",
    "                        self.text_queue.put(text)\n",
    "                    else:\n",
    "                        # Partial result, just display\n",
    "                        print(f\"Partial: {text}\", end=\"\\r\")\n",
    "                \n",
    "                # Mark task as done\n",
    "                self.audio_queue.task_done()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                # No audio data available, just continue\n",
    "                pass\n",
    "                \n",
    "    def _understanding_thread(self):\n",
    "        \"\"\"Thread that understands text and extracts intents.\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                # Get text from queue with timeout\n",
    "                text = self.text_queue.get(timeout=0.5)\n",
    "                \n",
    "                # Process text to get intent and entities\n",
    "                intent, entities = self.understanding_manager.process_text(text)\n",
    "                \n",
    "                # Put in queue for response generation\n",
    "                self.intent_queue.put((intent, entities))\n",
    "                \n",
    "                # Mark task as done\n",
    "                self.text_queue.task_done()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                # No text available, just continue\n",
    "                pass\n",
    "                \n",
    "    def _response_thread(self):\n",
    "        \"\"\"Thread that generates responses and executes actions.\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                # Get intent and entities from queue with timeout\n",
    "                intent, entities = self.intent_queue.get(timeout=0.5)\n",
    "                \n",
    "                # Generate response\n",
    "                response = self.response_manager.generate_response(intent, entities)\n",
    "                \n",
    "                # Execute action\n",
    "                self.action_manager.execute_action(intent, entities)\n",
    "                \n",
    "                # Output response\n",
    "                print(f\"\\nAssistant: {response}\")\n",
    "                \n",
    "                # Mark task as done\n",
    "                self.intent_queue.task_done()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                # No intent available, just continue\n",
    "                pass\n",
    "\n",
    "\n",
    "class AsyncAudioManager(AudioManager):\n",
    "    \"\"\"Asynchronous version of AudioManager.\"\"\"\n",
    "    \n",
    "    def get_audio_chunk(self):\n",
    "        \"\"\"Get a chunk of audio data, non-blocking.\"\"\"\n",
    "        if self.stream and self.stream.is_active():\n",
    "            return self.stream.read(self.chunk_size, exception_on_overflow=False)\n",
    "        return None\n",
    "\n",
    "\n",
    "class AsyncSpeechRecognizer(SpeechRecognizer):\n",
    "    \"\"\"Asynchronous version of SpeechRecognizer.\"\"\"\n",
    "    \n",
    "    def initialize(self, sample_rate):\n",
    "        \"\"\"Initialize with thread safety.\"\"\"\n",
    "        super().initialize(sample_rate)\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def process_audio(self, audio_data):\n",
    "        \"\"\"Process audio with thread safety.\"\"\"\n",
    "        with self.lock:\n",
    "            return super().process_audio(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82d15b",
   "metadata": {},
   "source": [
    "### Complete Implementation Example\n",
    "\n",
    "Let's create a complete implementation that combines all the concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_voice_assistant():\n",
    "    \"\"\"Run the complete voice assistant.\"\"\"\n",
    "    # Set the path to your Vosk model\n",
    "    model_path = \"/home/luar/AI/voice_assistant/vosk-model-small-en-us-0.15\"\n",
    "    \n",
    "    # Create and start the voice assistant\n",
    "    assistant = AsyncVoiceAssistant(model_path=model_path)\n",
    "    assistant.start()\n",
    "\n",
    "\n",
    "# If you want to run the assistant directly\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting voice assistant...\")\n",
    "    run_voice_assistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71be9b",
   "metadata": {},
   "source": [
    "### Common Integration Challenges\n",
    "\n",
    "1. **Resource Management**: Speech recognition can be resource-intensive. Solutions include:\n",
    "   - Using smaller models for devices with limited resources\n",
    "   - Implementing wake word detection to only process speech when needed\n",
    "   - Processing audio in smaller chunks\n",
    "\n",
    "2. **Error Handling**: Things will go wrong. Robust systems need:\n",
    "   - Timeout handling for unresponsive components\n",
    "   - Graceful recovery from recognition errors\n",
    "   - Fallback responses for unknown intents\n",
    "\n",
    "3. **Performance Optimization**: For better responsiveness:\n",
    "   - Preload models during initialization\n",
    "   - Use buffering to prevent audio data loss\n",
    "   - Consider using VAD (Voice Activity Detection) to process only when speech is present\n",
    "\n",
    "4. **User Experience**: A good assistant should:\n",
    "   - Provide feedback during processing (e.g., \"thinking...\" indicators)\n",
    "   - Handle interruptions gracefully\n",
    "   - Confirm actions for critical commands\n",
    "\n",
    "### Next Steps and Advanced Integration\n",
    "\n",
    "As you continue developing your voice assistant, consider these advanced integration techniques:\n",
    "\n",
    "1. **Wake Word Detection**: Add a lightweight model that listens for a trigger phrase before activating the full assistant\n",
    "2. **Voice Activity Detection (VAD)**: Process audio only when speech is detected\n",
    "3. **Speaker Identification**: Recognize different users and personalize responses\n",
    "4. **Multi-modal Integration**: Combine speech with other inputs like text or gestures\n",
    "5. **Distributed Architecture**: Split processing across multiple devices for better performance\n",
    "\n",
    "The next module will build on this integrated system to create a complete voice assistant project."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
